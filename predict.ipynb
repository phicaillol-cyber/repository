{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
        "outputId": "ab164713-aa78-46ae-dec3-3a148f3228df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  138 of 138 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variation shape : (1000, 139)\n",
            "Volume shape    : (1001, 139)\n",
            "NaN volume (top 10):\n",
            "Ticker\n",
            "^CIISCSEP    1000\n",
            "000001.SS     335\n",
            "^MOVE         321\n",
            "FISV          317\n",
            "^STOXX50E     317\n",
            "ADBE          316\n",
            "ABBV          316\n",
            "ABT           316\n",
            "ACN           316\n",
            "AAPL          316\n",
            "dtype: int64\n",
            "Creation terminÃ©e : 2026-02-12 07:04:15.991318\n"
          ]
        }
      ],
      "source": [
        "### Creation des datasets de VARIATION et de VOLUME des tickers sur 1000 jours\n",
        "### (100 % des tickers conservÃ©s, commentaires inclus)\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ======================================================\n",
        "# PERIODE\n",
        "# ======================================================\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# ======================================================\n",
        "# CAC 40\n",
        "# ======================================================\n",
        "cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # CrÃ©dit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault SystÃ¨mes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # HermÃ¨s\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'OrÃ©al\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # SociÃ©tÃ© GÃ©nÃ©rale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# NASDAQ\n",
        "# ======================================================\n",
        "nasdaq_tickers = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # MondelÄ“z\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\",  # ASML Holding\n",
        "    \"MRNA\",  # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# NYSE\n",
        "# ======================================================\n",
        "nyse_tickers = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\"     # Altria\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# INDICES & MACRO\n",
        "# ======================================================\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold)\n",
        "    \"BZ=F\",        # PÃ©trole Brent\n",
        "    \"^NDX\",        # NASDAQ 100\n",
        "    \"^DJI\",        # Dow Jones\n",
        "    \"^SP500-20\",   # S&P 500 Industrials\n",
        "    \"^SP500-15\",   # S&P 500 Materials\n",
        "    \"^STOXX50E\",   # Euro STOXX 50\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY)\n",
        "    \"EUR=X\",       # EUR/USD\n",
        "    \"CHF=X\",       # USD/CHF\n",
        "    \"^VIX\",        # VolatilitÃ©\n",
        "    \"^IRX\",        # Taux US 13 semaines\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans\n",
        "    \"^TYX\",        # Taux US 30 ans\n",
        "    \"^CIISCSEP\",   # Citi Economic Surprise Index\n",
        "    \"^MOVE\",       # Stress crÃ©dit\n",
        "    \"BTC-USD\",     # Bitcoin\n",
        "    \"TIP\",         # ETF inflation-linked\n",
        "    \"HYG\",         # CrÃ©dit high yield\n",
        "    \"LQD\",         # CrÃ©dit investment grade\n",
        "    \"000001.SS\",   # Shanghai Composite\n",
        "    \"FXI\",         # ETF China\n",
        "    \"HG=F\"         # Cuivre\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# COMBINAISON TOTALE\n",
        "# ======================================================\n",
        "all_tickers = (\n",
        "    cac40_tickers\n",
        "    + nasdaq_tickers\n",
        "    + nyse_tickers\n",
        "    + macro_tickers\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# DOWNLOAD\n",
        "# ======================================================\n",
        "print(\"Downloading data...\")\n",
        "data = yf.download(\n",
        "    all_tickers,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    auto_adjust=True\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# VARIATIONS (%)\n",
        "# ======================================================\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "variation_df = variation.reset_index().sort_values(\"date\")\n",
        "\n",
        "# ======================================================\n",
        "# VOLUMES\n",
        "# ======================================================\n",
        "if \"Volume\" in data.columns:\n",
        "    volume = data[\"Volume\"]\n",
        "else:\n",
        "    volume = pd.DataFrame(index=variation.index)\n",
        "\n",
        "volume.index = pd.to_datetime(volume.index)\n",
        "volume.index.name = \"date\"\n",
        "volume_df = volume.reset_index().sort_values(\"date\")\n",
        "\n",
        "# ======================================================\n",
        "# EXPORT CSV\n",
        "# ======================================================\n",
        "variation_df.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "volume_df.to_csv(\"DATASET_1000DAYS_VOLUME.csv\", index=False)\n",
        "\n",
        "# ======================================================\n",
        "# LOGS\n",
        "# ======================================================\n",
        "print(\"Variation shape :\", variation_df.shape)\n",
        "print(\"Volume shape    :\", volume_df.shape)\n",
        "print(\"NaN volume (top 10):\")\n",
        "print(volume.isna().sum().sort_values(ascending=False).head(10))\n",
        "print(\"Creation terminÃ©e :\", datetime.now())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag 1-3 jours"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "# ======================================================\n",
        "# 1. CHARGEMENT DES DONNÃ‰ES\n",
        "# ======================================================\n",
        "FILE_VAR = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "FILE_VOL = \"DATASET_1000DAYS_VOLUME.csv\"\n",
        "\n",
        "df_var = pd.read_csv(FILE_VAR)\n",
        "df_vol = pd.read_csv(FILE_VOL)\n",
        "\n",
        "df_var[\"date\"] = pd.to_datetime(df_var[\"date\"])\n",
        "df_vol[\"date\"] = pd.to_datetime(df_vol[\"date\"])\n",
        "\n",
        "df_var = df_var.set_index(\"date\").sort_index()\n",
        "df_vol = df_vol.set_index(\"date\").sort_index()\n",
        "\n",
        "common_cols = sorted(set(df_var.columns) & set(df_vol.columns))\n",
        "df_var = df_var[common_cols]\n",
        "df_vol = df_vol[common_cols]\n",
        "\n",
        "# ======================================================\n",
        "# 2. DISCRÃ‰TISATION PRIX\n",
        "# ======================================================\n",
        "def discretize_price(x, thr=1.0):\n",
        "    if x > thr:\n",
        "        return \"UP\"\n",
        "    elif x < -thr:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc_price = df_var.map(discretize_price)\n",
        "\n",
        "# ======================================================\n",
        "# 3. DISCRÃ‰TISATION VOLUME (Z-SCORE)\n",
        "# ======================================================\n",
        "VOL_WINDOW = 20\n",
        "\n",
        "vol_ma = df_vol.rolling(VOL_WINDOW).mean()\n",
        "vol_std = df_vol.rolling(VOL_WINDOW).std()\n",
        "vol_z = (df_vol - vol_ma) / vol_std\n",
        "\n",
        "def discretize_volume_z(z, thr=0.8):  # seuil lÃ©gÃ¨rement assoupli\n",
        "    if pd.isna(z):\n",
        "        return \"NEUTRAL\"\n",
        "    elif z > thr:\n",
        "        return \"VOL_HIGH\"\n",
        "    elif z < -thr:\n",
        "        return \"VOL_LOW\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc_vol = vol_z.map(discretize_volume_z)\n",
        "\n",
        "# ======================================================\n",
        "# 4. PARAMÃˆTRES AJUSTÃ‰S\n",
        "# ======================================================\n",
        "LEADER_PRICE_STATES = [\"UP\", \"DOWN\"]\n",
        "LEADER_VOL_STATES   = [\"VOL_HIGH\"]\n",
        "TARGET_STATES       = [\"UP\", \"DOWN\"]\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "MIN_OCCURRENCES = 15\n",
        "\n",
        "MIN_SUPPORT = 0.02\n",
        "MIN_CONFIDENCE = 0.55\n",
        "MIN_LIFT = 1.20\n",
        "MAX_CORR = 0.90\n",
        "\n",
        "# ======================================================\n",
        "# 5. SPLIT TRAIN / TEST\n",
        "# ======================================================\n",
        "split_idx = int(len(disc_price) * TRAIN_RATIO)\n",
        "\n",
        "train_price = disc_price.iloc[:split_idx]\n",
        "test_price  = disc_price.iloc[split_idx:]\n",
        "\n",
        "train_vol = disc_vol.iloc[:split_idx]\n",
        "test_vol  = disc_vol.iloc[split_idx:]\n",
        "\n",
        "train_var = df_var.iloc[:split_idx]\n",
        "\n",
        "# ======================================================\n",
        "# 6. MATRICE CORRÃ‰LATION\n",
        "# ======================================================\n",
        "corr_matrix = train_var.corr().abs()\n",
        "\n",
        "# ======================================================\n",
        "# 7. MINING\n",
        "# ======================================================\n",
        "def mine_lead_rules(disc_price, disc_vol):\n",
        "\n",
        "    rules = []\n",
        "    targets = list(disc_price.columns)\n",
        "    n_targets = len(targets)\n",
        "    max_lag = 3\n",
        "\n",
        "    total_iters = n_targets * max_lag\n",
        "    iter_count = 0\n",
        "\n",
        "    for t_idx, target in enumerate(targets, start=1):\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            iter_count += 1\n",
        "            progress = 100 * iter_count / total_iters\n",
        "            now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "\n",
        "            print(\n",
        "                f\"[{now}] \"\n",
        "                f\"[{progress:6.2f}%] \"\n",
        "                f\"Target {t_idx}/{n_targets}: {target} | \"\n",
        "                f\"Lag {lag}/{max_lag}\"\n",
        "            )\n",
        "\n",
        "            future = disc_price[target]\n",
        "            leaders_p = disc_price.shift(lag)\n",
        "            leaders_v = disc_vol.shift(lag)\n",
        "\n",
        "            future = future.iloc[lag:]\n",
        "            leaders_p = leaders_p.iloc[lag:]\n",
        "            leaders_v = leaders_v.iloc[lag:]\n",
        "\n",
        "            leaders = [c for c in targets if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                if corr_matrix.loc[l1, l2] > MAX_CORR:\n",
        "                    continue\n",
        "\n",
        "                for s1 in LEADER_PRICE_STATES:\n",
        "                    for s2 in LEADER_PRICE_STATES:\n",
        "                        for v1 in LEADER_VOL_STATES:\n",
        "                            for v2 in LEADER_VOL_STATES:\n",
        "\n",
        "                                mask = (\n",
        "                                    (leaders_p[l1] == s1) &\n",
        "                                    (leaders_p[l2] == s2) &\n",
        "                                    (leaders_v[l1] == v1) &\n",
        "                                    (leaders_v[l2] == v2)\n",
        "                                )\n",
        "\n",
        "                                occurrences = mask.sum()\n",
        "                                if occurrences < MIN_OCCURRENCES:\n",
        "                                    continue\n",
        "\n",
        "                                support = mask.mean()\n",
        "                                if support < MIN_SUPPORT:\n",
        "                                    continue\n",
        "\n",
        "                                for target_state in TARGET_STATES:\n",
        "\n",
        "                                    base_rate = (future == target_state).mean()\n",
        "                                    if base_rate == 0:\n",
        "                                        continue\n",
        "\n",
        "                                    confidence = (future[mask] == target_state).mean()\n",
        "                                    if confidence < MIN_CONFIDENCE:\n",
        "                                        continue\n",
        "\n",
        "                                    lift = confidence / base_rate\n",
        "                                    if lift < MIN_LIFT:\n",
        "                                        continue\n",
        "\n",
        "                                    rules.append({\n",
        "                                        \"target\": target,\n",
        "                                        \"target_direction\": target_state,\n",
        "                                        \"leader_1\": l1,\n",
        "                                        \"leader_2\": l2,\n",
        "                                        \"lag_days\": lag,\n",
        "                                        \"support\": round(support, 3),\n",
        "                                        \"confidence\": round(confidence, 3),\n",
        "                                        \"lift\": round(lift, 2),\n",
        "                                        \"occurrences\": int(occurrences)\n",
        "                                    })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# ======================================================\n",
        "# 8. MINING TRAIN\n",
        "# ======================================================\n",
        "rules_train = mine_lead_rules(train_price, train_vol)\n",
        "\n",
        "print(\"\\nNombre rÃ¨gles train :\", len(rules_train))\n",
        "\n",
        "# ======================================================\n",
        "# 9. VALIDATION TEST\n",
        "# ======================================================\n",
        "def validate_rules(rules_df):\n",
        "\n",
        "    if rules_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _, row in rules_df.iterrows():\n",
        "\n",
        "        lag = row[\"lag_days\"]\n",
        "        target = row[\"target\"]\n",
        "        target_state = row[\"target_direction\"]\n",
        "\n",
        "        future = test_price[target]\n",
        "        leaders_p = test_price.shift(lag)\n",
        "        leaders_v = test_vol.shift(lag)\n",
        "\n",
        "        future = future.iloc[lag:]\n",
        "        leaders_p = leaders_p.iloc[lag:]\n",
        "        leaders_v = leaders_v.iloc[lag:]\n",
        "\n",
        "        mask = (\n",
        "            (leaders_p[row[\"leader_1\"]] == \"UP\") &\n",
        "            (leaders_p[row[\"leader_2\"]] == \"UP\") &\n",
        "            (leaders_v[row[\"leader_1\"]] == \"VOL_HIGH\") &\n",
        "            (leaders_v[row[\"leader_2\"]] == \"VOL_HIGH\")\n",
        "        )\n",
        "\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        confidence_test = (future[mask] == target_state).mean()\n",
        "        base_rate = (future == target_state).mean()\n",
        "        lift_test = confidence_test / base_rate if base_rate > 0 else 0\n",
        "\n",
        "        row_result = row.to_dict()\n",
        "        row_result[\"test_confidence\"] = round(confidence_test, 3)\n",
        "        row_result[\"test_lift\"] = round(lift_test, 2)\n",
        "\n",
        "        results.append(row_result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "rules_final = validate_rules(rules_train)\n",
        "\n",
        "# ======================================================\n",
        "# 10. TRI & EXPORT\n",
        "# ======================================================\n",
        "if rules_final.empty:\n",
        "    print(\"\\nâš ï¸ Aucune rÃ¨gle valide trouvÃ©e.\")\n",
        "else:\n",
        "    rules_final = rules_final.sort_values(\n",
        "        by=[\"test_lift\", \"lift\"],\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    rules_final.to_csv(\n",
        "        \"lead_lag_rules_more_rules.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(\"\\nTop 20 rÃ¨gles validÃ©es :\\n\")\n",
        "    print(rules_final.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulxQ3yK6AiaM",
        "outputId": "1bf1cb85-591d-4a30-c246-d3e8a50bb07c"
      },
      "id": "ulxQ3yK6AiaM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:05:06] [  0.24%] Target 1/138: 000001.SS | Lag 1/3\n",
            "[07:06:35] [  0.48%] Target 1/138: 000001.SS | Lag 2/3\n",
            "[07:08:03] [  0.72%] Target 1/138: 000001.SS | Lag 3/3\n",
            "[07:09:31] [  0.97%] Target 2/138: AAPL | Lag 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script hybride v3"
      ],
      "metadata": {
        "id": "gunL1Xi-r_XF"
      },
      "id": "gunL1Xi-r_XF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "# ======================================================\n",
        "# 1. CHARGEMENT\n",
        "# ======================================================\n",
        "FILE_VAR = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "FILE_VOL = \"DATASET_1000DAYS_VOLUME.csv\"\n",
        "\n",
        "df_var = pd.read_csv(FILE_VAR)\n",
        "df_vol = pd.read_csv(FILE_VOL)\n",
        "\n",
        "df_var[\"date\"] = pd.to_datetime(df_var[\"date\"])\n",
        "df_vol[\"date\"] = pd.to_datetime(df_vol[\"date\"])\n",
        "\n",
        "df_var = df_var.set_index(\"date\").sort_index()\n",
        "df_vol = df_vol.set_index(\"date\").sort_index()\n",
        "\n",
        "common_cols = sorted(set(df_var.columns) & set(df_vol.columns))\n",
        "df_var = df_var[common_cols]\n",
        "df_vol = df_vol[common_cols]\n",
        "\n",
        "# ======================================================\n",
        "# 2. DISCRÃ‰TISATION PRIX\n",
        "# ======================================================\n",
        "def discretize_price(x, thr=1.0):\n",
        "    if x > thr:\n",
        "        return \"UP\"\n",
        "    elif x < -thr:\n",
        "        return \"DOWN\"\n",
        "    return \"NEUTRAL\"\n",
        "\n",
        "disc_price = df_var.map(discretize_price)\n",
        "\n",
        "# ======================================================\n",
        "# 3. DISCRÃ‰TISATION VOLUME (Z-score)\n",
        "# ======================================================\n",
        "VOL_WINDOW = 20\n",
        "vol_ma = df_vol.rolling(VOL_WINDOW).mean()\n",
        "vol_std = df_vol.rolling(VOL_WINDOW).std()\n",
        "vol_z = (df_vol - vol_ma) / vol_std\n",
        "\n",
        "def discretize_volume_z(z, thr=0.9):\n",
        "    if pd.isna(z):\n",
        "        return \"NEUTRAL\"\n",
        "    elif z > thr:\n",
        "        return \"VOL_HIGH\"\n",
        "    elif z < -thr:\n",
        "        return \"VOL_LOW\"\n",
        "    return \"NEUTRAL\"\n",
        "\n",
        "disc_vol = vol_z.map(discretize_volume_z)\n",
        "\n",
        "# ======================================================\n",
        "# 4. PARAMÃˆTRES HYBRIDES\n",
        "# ======================================================\n",
        "TRAIN_RATIO = 0.7\n",
        "MIN_OCCURRENCES = 20\n",
        "MIN_SUPPORT = 0.025\n",
        "MIN_CONFIDENCE = 0.57\n",
        "MIN_LIFT = 1.25\n",
        "MAX_CORR = 0.88\n",
        "Z_THRESHOLD = 2.0\n",
        "\n",
        "LEADER_PRICE_STATES = [\"UP\", \"DOWN\"]\n",
        "LEADER_VOL_STATES = [\"VOL_HIGH\"]\n",
        "TARGET_STATES = [\"UP\", \"DOWN\"]\n",
        "\n",
        "# ======================================================\n",
        "# 5. SPLIT TRAIN / TEST\n",
        "# ======================================================\n",
        "split_idx = int(len(disc_price) * TRAIN_RATIO)\n",
        "\n",
        "train_price = disc_price.iloc[:split_idx]\n",
        "test_price  = disc_price.iloc[split_idx:]\n",
        "\n",
        "train_vol = disc_vol.iloc[:split_idx]\n",
        "test_vol  = disc_vol.iloc[split_idx:]\n",
        "\n",
        "train_var = df_var.iloc[:split_idx]\n",
        "corr_matrix = train_var.corr().abs()\n",
        "\n",
        "# ======================================================\n",
        "# 6. MINING V3\n",
        "# ======================================================\n",
        "def mine_rules():\n",
        "\n",
        "    rules = []\n",
        "    targets = list(train_price.columns)\n",
        "    n_targets = len(targets)\n",
        "    max_lag = 3\n",
        "    total_iters = n_targets * max_lag\n",
        "    iter_count = 0\n",
        "\n",
        "    for t_idx, target in enumerate(targets, 1):\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            iter_count += 1\n",
        "            progress = 100 * iter_count / total_iters\n",
        "            now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "\n",
        "            print(\n",
        "                f\"[{now}] \"\n",
        "                f\"[{progress:6.2f}%] \"\n",
        "                f\"Target {t_idx}/{n_targets}: {target} | \"\n",
        "                f\"Lag {lag}/{max_lag}\"\n",
        "            )\n",
        "\n",
        "            future = train_price[target]\n",
        "            leaders_p = train_price.shift(lag)\n",
        "            leaders_v = train_vol.shift(lag)\n",
        "\n",
        "            future = future.iloc[lag:]\n",
        "            leaders_p = leaders_p.iloc[lag:]\n",
        "            leaders_v = leaders_v.iloc[lag:]\n",
        "\n",
        "            leaders = [c for c in targets if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                if corr_matrix.loc[l1, l2] > MAX_CORR:\n",
        "                    continue\n",
        "\n",
        "                for s1 in LEADER_PRICE_STATES:\n",
        "                    for s2 in LEADER_PRICE_STATES:\n",
        "                        for v1 in LEADER_VOL_STATES:\n",
        "                            for v2 in LEADER_VOL_STATES:\n",
        "\n",
        "                                mask = (\n",
        "                                    (leaders_p[l1] == s1) &\n",
        "                                    (leaders_p[l2] == s2) &\n",
        "                                    (leaders_v[l1] == v1) &\n",
        "                                    (leaders_v[l2] == v2)\n",
        "                                )\n",
        "\n",
        "                                n = mask.sum()\n",
        "                                if n < MIN_OCCURRENCES:\n",
        "                                    continue\n",
        "\n",
        "                                support = mask.mean()\n",
        "                                if support < MIN_SUPPORT:\n",
        "                                    continue\n",
        "\n",
        "                                for target_state in TARGET_STATES:\n",
        "\n",
        "                                    base_rate = (future == target_state).mean()\n",
        "                                    if base_rate == 0:\n",
        "                                        continue\n",
        "\n",
        "                                    confidence = (future[mask] == target_state).mean()\n",
        "                                    lift = confidence / base_rate\n",
        "\n",
        "                                    if confidence < MIN_CONFIDENCE:\n",
        "                                        continue\n",
        "                                    if lift < MIN_LIFT:\n",
        "                                        continue\n",
        "\n",
        "                                    # ðŸ”¬ Z-score binomial\n",
        "                                    std = np.sqrt(base_rate * (1 - base_rate) / n)\n",
        "                                    if std == 0:\n",
        "                                        continue\n",
        "\n",
        "                                    z_score = (confidence - base_rate) / std\n",
        "\n",
        "                                    if z_score < Z_THRESHOLD:\n",
        "                                        continue\n",
        "\n",
        "                                    rules.append({\n",
        "                                        \"target\": target,\n",
        "                                        \"leader_1\": l1,\n",
        "                                        \"leader_2\": l2,\n",
        "                                        \"lag\": lag,\n",
        "                                        \"support\": round(support,3),\n",
        "                                        \"confidence\": round(confidence,3),\n",
        "                                        \"lift\": round(lift,2),\n",
        "                                        \"z_score\": round(z_score,2),\n",
        "                                        \"occurrences\": int(n)\n",
        "                                    })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# ======================================================\n",
        "# 7. MINING\n",
        "# ======================================================\n",
        "rules_train = mine_rules()\n",
        "\n",
        "if rules_train.empty:\n",
        "    print(\"\\nâš ï¸ Aucune rÃ¨gle trouvÃ©e en TRAIN.\")\n",
        "else:\n",
        "    print(\"\\nRÃ¨gles train :\", len(rules_train))\n",
        "\n",
        "# ======================================================\n",
        "# 8. VALIDATION OOS\n",
        "# ======================================================\n",
        "def validate_rules(rules_df):\n",
        "\n",
        "    if rules_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _, row in rules_df.iterrows():\n",
        "\n",
        "        lag = row[\"lag\"]\n",
        "        target = row[\"target\"]\n",
        "\n",
        "        future = test_price[target]\n",
        "        leaders_p = test_price.shift(lag)\n",
        "\n",
        "        future = future.iloc[lag:]\n",
        "        leaders_p = leaders_p.iloc[lag:]\n",
        "\n",
        "        mask = (\n",
        "            (leaders_p[row[\"leader_1\"]] == \"UP\") &\n",
        "            (leaders_p[row[\"leader_2\"]] == \"UP\")\n",
        "        )\n",
        "\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        confidence_test = (future[mask] == \"UP\").mean()\n",
        "        base_rate = (future == \"UP\").mean()\n",
        "        lift_test = confidence_test / base_rate if base_rate > 0 else 0\n",
        "\n",
        "        r = row.to_dict()\n",
        "        r[\"test_lift\"] = round(lift_test,2)\n",
        "        r[\"test_confidence\"] = round(confidence_test,3)\n",
        "\n",
        "        results.append(r)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "rules_final = validate_rules(rules_train)\n",
        "\n",
        "# ======================================================\n",
        "# 9. TRI & EXPORT SÃ‰CURISÃ‰\n",
        "# ======================================================\n",
        "if rules_final.empty:\n",
        "    print(\"\\nâš ï¸ Aucune rÃ¨gle valide en OOS.\")\n",
        "else:\n",
        "    rules_final = rules_final.sort_values(\n",
        "        by=[\"test_lift\",\"z_score\"],\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    rules_final.to_csv(\"lead_lag_rules_V3_hybrid.csv\", index=False)\n",
        "\n",
        "    print(\"\\nTop 20 rÃ¨gles V3 :\\n\")\n",
        "    print(rules_final.head(20))\n"
      ],
      "metadata": {
        "id": "xnT4xOXUsD8v"
      },
      "id": "xnT4xOXUsD8v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GÃ©nÃ©ration des features Ã  partir des rÃ¨gles"
      ],
      "metadata": {
        "id": "OHG0YnIhPrtO"
      },
      "id": "OHG0YnIhPrtO"
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# AUTO-GENERATE ONE FEATURES CSV PER TARGET TICKER\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# CLEAN OUTPUT\n",
        "# ======================================================\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
        "pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "\n",
        "# ======================================================\n",
        "# FILES\n",
        "# ======================================================\n",
        "RULES_FILE = \"lead_lag_rules_exhaustive_2leaders.csv\"\n",
        "DATA_FILE  = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "OUTPUT_DIR = \"features_by_target\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_csv(filename):\n",
        "    if os.path.exists(filename):\n",
        "        return pd.read_csv(filename)\n",
        "    if os.path.exists(f\"/mnt/data/{filename}\"):\n",
        "        return pd.read_csv(f\"/mnt/data/{filename}\")\n",
        "    raise FileNotFoundError(filename)\n",
        "\n",
        "# ======================================================\n",
        "# LOAD DATA\n",
        "# ======================================================\n",
        "rules = load_csv(RULES_FILE)\n",
        "df    = load_csv(DATA_FILE)\n",
        "\n",
        "# ======================================================\n",
        "# DATA PREP\n",
        "# ======================================================\n",
        "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
        "df = df.set_index(df.columns[0]).sort_index()\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# ======================================================\n",
        "# HELPERS\n",
        "# ======================================================\n",
        "def state_condition(series: pd.Series, state: str) -> pd.Series:\n",
        "    if state == \"UP\":\n",
        "        return series > 0\n",
        "    if state == \"DOWN\":\n",
        "        return series < 0\n",
        "    raise ValueError(state)\n",
        "\n",
        "# ======================================================\n",
        "# FEATURE ENGINEERING (ONCE)\n",
        "# ======================================================\n",
        "feature_dict = {}\n",
        "\n",
        "for idx, rule in rules.iterrows():\n",
        "\n",
        "    try:\n",
        "        lag = int(rule[\"lag_days\"])\n",
        "        direction = 1 if rule[\"target_direction\"] == \"UP\" else -1\n",
        "\n",
        "        leaders = [\n",
        "            (rule[\"leader_1\"], rule[\"leader_1_state\"]),\n",
        "            (rule[\"leader_2\"], rule[\"leader_2_state\"]),\n",
        "        ]\n",
        "\n",
        "        support    = float(rule[\"support\"])\n",
        "        confidence = float(rule[\"confidence\"])\n",
        "        lift       = float(rule[\"lift\"])\n",
        "\n",
        "        fname = f\"rule_{idx}_{leaders[0][0]}_{leaders[1][0]}_lag{lag}\"\n",
        "\n",
        "        conds = [\n",
        "            state_condition(df[leader], state)\n",
        "            for leader, state in leaders\n",
        "        ]\n",
        "\n",
        "        rule_active = conds[0] & conds[1]\n",
        "        rule_active = (\n",
        "            rule_active\n",
        "            .shift(lag)\n",
        "            .astype(\"boolean\")\n",
        "            .fillna(False)\n",
        "        )\n",
        "\n",
        "        bin_feat = rule_active.astype(\"int8\")\n",
        "\n",
        "        feature_dict[fname] = bin_feat\n",
        "        feature_dict[f\"{fname}_w\"] = (\n",
        "            bin_feat * confidence * np.log1p(support) * lift\n",
        "        ).astype(\"float32\")\n",
        "        feature_dict[f\"{fname}_dir\"] = (\n",
        "            bin_feat * direction\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "# ======================================================\n",
        "# BUILD BASE FEATURES DF (SHARED)\n",
        "# ======================================================\n",
        "base_features = pd.concat(\n",
        "    list(feature_dict.values()),\n",
        "    axis=1\n",
        ")\n",
        "base_features.columns = list(feature_dict.keys())\n",
        "base_features = base_features.copy()\n",
        "base_features = base_features.fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# EXPORT ONE CSV PER TARGET\n",
        "# ======================================================\n",
        "HORIZON = 1\n",
        "targets = sorted(rules[\"target\"].unique())\n",
        "\n",
        "print(f\"ðŸ“Š Targets found: {targets}\")\n",
        "\n",
        "for target in targets:\n",
        "\n",
        "    if target not in df.columns:\n",
        "        print(f\"âš ï¸ Target {target} not in dataset â€” skipped\")\n",
        "        continue\n",
        "\n",
        "    features = base_features.copy()\n",
        "\n",
        "    features[\"target\"] = (\n",
        "        df[target]\n",
        "        .shift(-HORIZON)\n",
        "        .astype(\"float32\")\n",
        "    )\n",
        "\n",
        "    features[\"target_ticker\"] = target\n",
        "\n",
        "    # Rule-based signal (optional)\n",
        "    features[\"signal_score\"] = (\n",
        "        features.filter(like=\"_dir\").sum(axis=1)\n",
        "    ).astype(\"int16\")\n",
        "\n",
        "    features[\"signal\"] = np.sign(features[\"signal_score\"]).astype(\"int8\")\n",
        "\n",
        "    # Reorder columns (human-friendly)\n",
        "    ordered_cols = (\n",
        "        [\"target_ticker\"]\n",
        "        + [c for c in features.columns if c != \"target_ticker\"]\n",
        "    )\n",
        "    features = features[ordered_cols]\n",
        "\n",
        "    # Export\n",
        "    out_path = os.path.join(\n",
        "        OUTPUT_DIR,\n",
        "        f\"features_{target}.csv\"\n",
        "    )\n",
        "\n",
        "    features.to_csv(out_path, index_label=\"date\")\n",
        "\n",
        "    print(f\"âœ… Exported: {out_path} | shape={features.shape}\")\n",
        "\n",
        "print(\"ðŸŽ¯ All targets exported.\")\n"
      ],
      "metadata": {
        "id": "4_Tj7NPgZwOh"
      },
      "id": "4_Tj7NPgZwOh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}