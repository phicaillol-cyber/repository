{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e"
      },
      "outputs": [],
      "source": [
        "### Creation des datasets de VARIATION et de VOLUME des tickers sur 1000 jours\n",
        "### (100 % des tickers conservÃ©s, commentaires inclus)\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ======================================================\n",
        "# PERIODE\n",
        "# ======================================================\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# ======================================================\n",
        "# CAC 40\n",
        "# ======================================================\n",
        "cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # CrÃ©dit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault SystÃ¨mes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # HermÃ¨s\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'OrÃ©al\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # SociÃ©tÃ© GÃ©nÃ©rale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# NASDAQ\n",
        "# ======================================================\n",
        "nasdaq_tickers = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # MondelÄ“z\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\",  # ASML Holding\n",
        "    \"MRNA\",  # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# NYSE\n",
        "# ======================================================\n",
        "nyse_tickers = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\"     # Altria\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# INDICES & MACRO\n",
        "# ======================================================\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold)\n",
        "    \"BZ=F\",        # PÃ©trole Brent\n",
        "    \"^NDX\",        # NASDAQ 100\n",
        "    \"^DJI\",        # Dow Jones\n",
        "    \"^SP500-20\",   # S&P 500 Industrials\n",
        "    \"^SP500-15\",   # S&P 500 Materials\n",
        "    \"^STOXX50E\",   # Euro STOXX 50\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY)\n",
        "    \"EUR=X\",       # EUR/USD\n",
        "    \"CHF=X\",       # USD/CHF\n",
        "    \"^VIX\",        # VolatilitÃ©\n",
        "    \"^IRX\",        # Taux US 13 semaines\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans\n",
        "    \"^TYX\",        # Taux US 30 ans\n",
        "    \"^CIISCSEP\",   # Citi Economic Surprise Index\n",
        "    \"^MOVE\",       # Stress crÃ©dit\n",
        "    \"BTC-USD\",     # Bitcoin\n",
        "    \"TIP\",         # ETF inflation-linked\n",
        "    \"HYG\",         # CrÃ©dit high yield\n",
        "    \"LQD\",         # CrÃ©dit investment grade\n",
        "    \"000001.SS\",   # Shanghai Composite\n",
        "    \"FXI\",         # ETF China\n",
        "    \"HG=F\"         # Cuivre\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# COMBINAISON TOTALE\n",
        "# ======================================================\n",
        "all_tickers = (\n",
        "    cac40_tickers\n",
        "    + nasdaq_tickers\n",
        "    + nyse_tickers\n",
        "    + macro_tickers\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# DOWNLOAD\n",
        "# ======================================================\n",
        "print(\"Downloading data...\")\n",
        "data = yf.download(\n",
        "    all_tickers,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    auto_adjust=True\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# VARIATIONS (%)\n",
        "# ======================================================\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "variation_df = variation.reset_index().sort_values(\"date\")\n",
        "\n",
        "# ======================================================\n",
        "# VOLUMES\n",
        "# ======================================================\n",
        "if \"Volume\" in data.columns:\n",
        "    volume = data[\"Volume\"]\n",
        "else:\n",
        "    volume = pd.DataFrame(index=variation.index)\n",
        "\n",
        "volume.index = pd.to_datetime(volume.index)\n",
        "volume.index.name = \"date\"\n",
        "volume_df = volume.reset_index().sort_values(\"date\")\n",
        "\n",
        "# ======================================================\n",
        "# EXPORT CSV\n",
        "# ======================================================\n",
        "variation_df.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "volume_df.to_csv(\"DATASET_1000DAYS_VOLUME.csv\", index=False)\n",
        "\n",
        "# ======================================================\n",
        "# LOGS\n",
        "# ======================================================\n",
        "print(\"Variation shape :\", variation_df.shape)\n",
        "print(\"Volume shape    :\", volume_df.shape)\n",
        "print(\"NaN volume (top 10):\")\n",
        "print(volume.isna().sum().sort_values(ascending=False).head(10))\n",
        "print(\"Creation terminÃ©e :\", datetime.now())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag 1-3 jours"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# ======================================================\n",
        "# 1. CHARGEMENT DES DONNÃ‰ES\n",
        "# ======================================================\n",
        "FILE_VAR = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "FILE_VOL = \"DATASET_1000DAYS_VOLUME.csv\"\n",
        "\n",
        "df_var = pd.read_csv(FILE_VAR)\n",
        "df_vol = pd.read_csv(FILE_VOL)\n",
        "\n",
        "df_var[\"date\"] = pd.to_datetime(df_var[\"date\"])\n",
        "df_vol[\"date\"] = pd.to_datetime(df_vol[\"date\"])\n",
        "\n",
        "df_var = df_var.set_index(\"date\").sort_index()\n",
        "df_vol = df_vol.set_index(\"date\").sort_index()\n",
        "\n",
        "# colonnes communes uniquement\n",
        "common_cols = sorted(set(df_var.columns) & set(df_vol.columns))\n",
        "df_var = df_var[common_cols]\n",
        "df_vol = df_vol[common_cols]\n",
        "\n",
        "# ======================================================\n",
        "# 2. DISCRÃ‰TISATION PRIX\n",
        "# ======================================================\n",
        "def discretize_price(x, thr=1.0):\n",
        "    if x > thr:\n",
        "        return \"UP\"\n",
        "    elif x < -thr:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc_price = pd.DataFrame(\n",
        "    {\n",
        "        col: df_var[col].map(discretize_price)\n",
        "        for col in df_var.columns\n",
        "    },\n",
        "    index=df_var.index\n",
        ")\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. DISCRÃ‰TISATION VOLUME\n",
        "# ======================================================\n",
        "VOL_WINDOW = 20\n",
        "vol_ma = df_vol.rolling(VOL_WINDOW).mean()\n",
        "\n",
        "def discretize_volume(v, ma):\n",
        "    if pd.isna(v) or pd.isna(ma):\n",
        "        return \"NEUTRAL\"\n",
        "    elif v > ma:\n",
        "        return \"VOL_HIGH\"\n",
        "    else:\n",
        "        return \"VOL_LOW\"\n",
        "\n",
        "disc_vol = pd.DataFrame(index=df_vol.index, columns=df_vol.columns)\n",
        "\n",
        "for col in df_vol.columns:\n",
        "    disc_vol[col] = [\n",
        "        discretize_volume(v, m)\n",
        "        for v, m in zip(df_vol[col], vol_ma[col])\n",
        "    ]\n",
        "\n",
        "# ======================================================\n",
        "# 4. PARAMÃˆTRES DE PATTERNS\n",
        "# ======================================================\n",
        "LEADER_PRICE_STATES = [\"UP\", \"DOWN\"]\n",
        "LEADER_VOL_STATES   = [\"VOL_HIGH\"]  # volontairement restrictif\n",
        "TARGET_STATES       = [\"UP\", \"DOWN\"]\n",
        "\n",
        "# ======================================================\n",
        "# 5. MINING DES RÃˆGLES\n",
        "# ======================================================\n",
        "def mine_lead_rules_with_volume(\n",
        "    disc_price,\n",
        "    disc_vol,\n",
        "    df_var,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    max_corr=0.85\n",
        "):\n",
        "    rules = []\n",
        "    targets = disc_price.columns.tolist()\n",
        "    total_targets = len(targets)\n",
        "\n",
        "    for i, target in enumerate(targets, 1):\n",
        "        print(f\"\\nðŸŽ¯ Target {i}/{total_targets} : {target}\")\n",
        "\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            future = disc_price[target]\n",
        "            leaders_p = disc_price.shift(lag)\n",
        "            leaders_v = disc_vol.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_p.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_p = leaders_p.loc[valid_idx]\n",
        "            leaders_v = leaders_v.loc[valid_idx]\n",
        "\n",
        "            leaders = [c for c in targets if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                corr = abs(df_var[l1].corr(df_var[l2]))\n",
        "                if corr > max_corr:\n",
        "                    continue\n",
        "\n",
        "                for s1 in LEADER_PRICE_STATES:\n",
        "                    for s2 in LEADER_PRICE_STATES:\n",
        "                        for v1 in LEADER_VOL_STATES:\n",
        "                            for v2 in LEADER_VOL_STATES:\n",
        "\n",
        "                                mask = (\n",
        "                                    (leaders_p[l1] == s1) &\n",
        "                                    (leaders_p[l2] == s2) &\n",
        "                                    (leaders_v[l1] == v1) &\n",
        "                                    (leaders_v[l2] == v2)\n",
        "                                )\n",
        "\n",
        "                                support = mask.mean()\n",
        "                                if support < min_support:\n",
        "                                    continue\n",
        "\n",
        "                                for target_state in TARGET_STATES:\n",
        "\n",
        "                                    base_rate = (future == target_state).mean()\n",
        "                                    if base_rate == 0:\n",
        "                                        continue\n",
        "\n",
        "                                    confidence = (future[mask] == target_state).mean()\n",
        "                                    if confidence < min_confidence:\n",
        "                                        continue\n",
        "\n",
        "                                    lift = confidence / base_rate\n",
        "                                    if lift < min_lift:\n",
        "                                        continue\n",
        "\n",
        "                                    rules.append({\n",
        "                                        \"target\": target,\n",
        "                                        \"target_direction\": target_state,\n",
        "                                        \"leader_1\": l1,\n",
        "                                        \"leader_2\": l2,\n",
        "                                        \"leader_1_state\": s1,\n",
        "                                        \"leader_2_state\": s2,\n",
        "                                        \"leader_1_vol\": v1,\n",
        "                                        \"leader_2_vol\": v2,\n",
        "                                        \"lag_days\": lag,\n",
        "                                        \"support\": round(support, 3),\n",
        "                                        \"confidence\": round(confidence, 3),\n",
        "                                        \"lift\": round(lift, 2),\n",
        "                                        \"corr_leaders\": round(corr, 2),\n",
        "                                        \"occurrences\": int(mask.sum())\n",
        "                                    })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# ======================================================\n",
        "# 6. EXECUTION\n",
        "# ======================================================\n",
        "rules = mine_lead_rules_with_volume(\n",
        "    disc_price,\n",
        "    disc_vol,\n",
        "    df_var,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\", \"support\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "rules.to_csv(\n",
        "    \"lead_lag_rules_exhaustive_2leaders_price_volume.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“Š Top 20 rÃ¨gles prix + volume :\\n\")\n",
        "print(rules.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulxQ3yK6AiaM",
        "outputId": "29bdea8b-898e-418b-a5ad-032a6b3c8972"
      },
      "id": "ulxQ3yK6AiaM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Target 1/138 : 000001.SS\n",
            "\n",
            "ðŸŽ¯ Target 2/138 : AAPL\n",
            "\n",
            "ðŸŽ¯ Target 3/138 : ABBV\n",
            "\n",
            "ðŸŽ¯ Target 4/138 : ABT\n",
            "\n",
            "ðŸŽ¯ Target 5/138 : ACA.PA\n",
            "\n",
            "ðŸŽ¯ Target 6/138 : ACN\n",
            "\n",
            "ðŸŽ¯ Target 7/138 : ADBE\n",
            "\n",
            "ðŸŽ¯ Target 8/138 : ADI\n",
            "\n",
            "ðŸŽ¯ Target 9/138 : AI.PA\n",
            "\n",
            "ðŸŽ¯ Target 10/138 : AIR.PA\n",
            "\n",
            "ðŸŽ¯ Target 11/138 : ALO.PA\n",
            "\n",
            "ðŸŽ¯ Target 12/138 : AMAT\n",
            "\n",
            "ðŸŽ¯ Target 13/138 : AMD\n",
            "\n",
            "ðŸŽ¯ Target 14/138 : AMGN\n",
            "\n",
            "ðŸŽ¯ Target 15/138 : AMZN\n",
            "\n",
            "ðŸŽ¯ Target 16/138 : ASML\n",
            "\n",
            "ðŸŽ¯ Target 17/138 : AVGO\n",
            "\n",
            "ðŸŽ¯ Target 18/138 : BAC\n",
            "\n",
            "ðŸŽ¯ Target 19/138 : BKNG\n",
            "\n",
            "ðŸŽ¯ Target 20/138 : BN.PA\n",
            "\n",
            "ðŸŽ¯ Target 21/138 : BNP.PA\n",
            "\n",
            "ðŸŽ¯ Target 22/138 : BTC-USD\n",
            "\n",
            "ðŸŽ¯ Target 23/138 : BZ=F\n",
            "\n",
            "ðŸŽ¯ Target 24/138 : C\n",
            "\n",
            "ðŸŽ¯ Target 25/138 : CA.PA\n",
            "\n",
            "ðŸŽ¯ Target 26/138 : CAP.PA\n",
            "\n",
            "ðŸŽ¯ Target 27/138 : CAT\n",
            "\n",
            "ðŸŽ¯ Target 28/138 : CHF=X\n",
            "\n",
            "ðŸŽ¯ Target 29/138 : CMCSA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GÃ©nÃ©ration des features Ã  partir des rÃ¨gles"
      ],
      "metadata": {
        "id": "OHG0YnIhPrtO"
      },
      "id": "OHG0YnIhPrtO"
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# AUTO-GENERATE ONE FEATURES CSV PER TARGET TICKER\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# CLEAN OUTPUT\n",
        "# ======================================================\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
        "pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "\n",
        "# ======================================================\n",
        "# FILES\n",
        "# ======================================================\n",
        "RULES_FILE = \"lead_lag_rules_exhaustive_2leaders.csv\"\n",
        "DATA_FILE  = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "OUTPUT_DIR = \"features_by_target\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_csv(filename):\n",
        "    if os.path.exists(filename):\n",
        "        return pd.read_csv(filename)\n",
        "    if os.path.exists(f\"/mnt/data/{filename}\"):\n",
        "        return pd.read_csv(f\"/mnt/data/{filename}\")\n",
        "    raise FileNotFoundError(filename)\n",
        "\n",
        "# ======================================================\n",
        "# LOAD DATA\n",
        "# ======================================================\n",
        "rules = load_csv(RULES_FILE)\n",
        "df    = load_csv(DATA_FILE)\n",
        "\n",
        "# ======================================================\n",
        "# DATA PREP\n",
        "# ======================================================\n",
        "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
        "df = df.set_index(df.columns[0]).sort_index()\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# ======================================================\n",
        "# HELPERS\n",
        "# ======================================================\n",
        "def state_condition(series: pd.Series, state: str) -> pd.Series:\n",
        "    if state == \"UP\":\n",
        "        return series > 0\n",
        "    if state == \"DOWN\":\n",
        "        return series < 0\n",
        "    raise ValueError(state)\n",
        "\n",
        "# ======================================================\n",
        "# FEATURE ENGINEERING (ONCE)\n",
        "# ======================================================\n",
        "feature_dict = {}\n",
        "\n",
        "for idx, rule in rules.iterrows():\n",
        "\n",
        "    try:\n",
        "        lag = int(rule[\"lag_days\"])\n",
        "        direction = 1 if rule[\"target_direction\"] == \"UP\" else -1\n",
        "\n",
        "        leaders = [\n",
        "            (rule[\"leader_1\"], rule[\"leader_1_state\"]),\n",
        "            (rule[\"leader_2\"], rule[\"leader_2_state\"]),\n",
        "        ]\n",
        "\n",
        "        support    = float(rule[\"support\"])\n",
        "        confidence = float(rule[\"confidence\"])\n",
        "        lift       = float(rule[\"lift\"])\n",
        "\n",
        "        fname = f\"rule_{idx}_{leaders[0][0]}_{leaders[1][0]}_lag{lag}\"\n",
        "\n",
        "        conds = [\n",
        "            state_condition(df[leader], state)\n",
        "            for leader, state in leaders\n",
        "        ]\n",
        "\n",
        "        rule_active = conds[0] & conds[1]\n",
        "        rule_active = (\n",
        "            rule_active\n",
        "            .shift(lag)\n",
        "            .astype(\"boolean\")\n",
        "            .fillna(False)\n",
        "        )\n",
        "\n",
        "        bin_feat = rule_active.astype(\"int8\")\n",
        "\n",
        "        feature_dict[fname] = bin_feat\n",
        "        feature_dict[f\"{fname}_w\"] = (\n",
        "            bin_feat * confidence * np.log1p(support) * lift\n",
        "        ).astype(\"float32\")\n",
        "        feature_dict[f\"{fname}_dir\"] = (\n",
        "            bin_feat * direction\n",
        "        ).astype(\"int8\")\n",
        "\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "# ======================================================\n",
        "# BUILD BASE FEATURES DF (SHARED)\n",
        "# ======================================================\n",
        "base_features = pd.concat(\n",
        "    list(feature_dict.values()),\n",
        "    axis=1\n",
        ")\n",
        "base_features.columns = list(feature_dict.keys())\n",
        "base_features = base_features.copy()\n",
        "base_features = base_features.fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# EXPORT ONE CSV PER TARGET\n",
        "# ======================================================\n",
        "HORIZON = 1\n",
        "targets = sorted(rules[\"target\"].unique())\n",
        "\n",
        "print(f\"ðŸ“Š Targets found: {targets}\")\n",
        "\n",
        "for target in targets:\n",
        "\n",
        "    if target not in df.columns:\n",
        "        print(f\"âš ï¸ Target {target} not in dataset â€” skipped\")\n",
        "        continue\n",
        "\n",
        "    features = base_features.copy()\n",
        "\n",
        "    features[\"target\"] = (\n",
        "        df[target]\n",
        "        .shift(-HORIZON)\n",
        "        .astype(\"float32\")\n",
        "    )\n",
        "\n",
        "    features[\"target_ticker\"] = target\n",
        "\n",
        "    # Rule-based signal (optional)\n",
        "    features[\"signal_score\"] = (\n",
        "        features.filter(like=\"_dir\").sum(axis=1)\n",
        "    ).astype(\"int16\")\n",
        "\n",
        "    features[\"signal\"] = np.sign(features[\"signal_score\"]).astype(\"int8\")\n",
        "\n",
        "    # Reorder columns (human-friendly)\n",
        "    ordered_cols = (\n",
        "        [\"target_ticker\"]\n",
        "        + [c for c in features.columns if c != \"target_ticker\"]\n",
        "    )\n",
        "    features = features[ordered_cols]\n",
        "\n",
        "    # Export\n",
        "    out_path = os.path.join(\n",
        "        OUTPUT_DIR,\n",
        "        f\"features_{target}.csv\"\n",
        "    )\n",
        "\n",
        "    features.to_csv(out_path, index_label=\"date\")\n",
        "\n",
        "    print(f\"âœ… Exported: {out_path} | shape={features.shape}\")\n",
        "\n",
        "print(\"ðŸŽ¯ All targets exported.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4_Tj7NPgZwOh",
        "outputId": "acdb7c7b-d0e4-4c33-bbb6-4951488bfe43"
      },
      "id": "4_Tj7NPgZwOh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "lead_lag_rules_exhaustive_2leaders.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1314009372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# LOAD DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# ======================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mrules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRULES_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdf\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1314009372.py\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/mnt/data/{filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/mnt/data/{filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# ======================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: lead_lag_rules_exhaustive_2leaders.csv"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}