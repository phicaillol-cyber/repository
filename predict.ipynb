{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e"
      },
      "outputs": [],
      "source": [
        "### Creation du dataset de variation des tickers des 1000 derniers jours\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Définir la période de 1000 jours\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# Liste des tickers du CAC 40 (exemple : 'OR.PA' pour L'Oréal)\n",
        "cac40_tickers = cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # Crédit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault Systèmes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # Hermès\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'Oréal\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # Société Générale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# Liste des tickers du NASDAQ (exemple : 'AAPL' pour Apple)\n",
        "nasdaq_tickers = nasdaq_top_40 = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices (AMD)\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # Mondelēz\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\", # ASML Holding\n",
        "    \"MRNA\", # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "nyse_tickers = nyse_top_40 = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"CSCO\",  # Cisco (aussi coté sur NYSE)\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\",    # Altria\n",
        "]\n",
        "\n",
        "# Inclure également les indices macro (VIX, Dollar Index, Brent, Or)\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold Futures COMEX)\n",
        "    \"BZ=F\",        # Pétrole Brent (Brent Crude Oil Futures)\n",
        "    \"^NDX\",        # NASDAQ 100 (technologie US, croissance)\n",
        "    \"^DJI\",        # Dow Jones Industrial Average (blue chips US)\n",
        "    \"^SP500-20\",   # S&P 500 Industrials (secteur industriel)\n",
        "    \"^SP500-15\",   # S&P 500 Materials (matières premières)\n",
        "    \"^STOXX50E\",   # Euro STOXX 50 (grandes capitalisations zone euro)\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY – force du dollar US)\n",
        "    \"EUR=X\",       # Taux de change EUR/USD\n",
        "    \"CHF=X\",       # Taux de change USD/CHF (valeur refuge)\n",
        "    \"^VIX\",        # Indice de volatilité (peur / stress de marché)\n",
        "    \"^IRX\",        # Taux US 13 semaines (T-Bills court terme)\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans (benchmark macro mondial)\n",
        "    \"^TYX\",        # Taux US 30 ans (long terme)\n",
        "    \"^CIISCSEP\",   # Indice de surprises économiques Citi (US)\n",
        "    \"CDX\"          # Indice de crédit (Credit Default Swaps – stress crédit)\n",
        "    \"BTC-USD\"      # Bitcoin\n",
        "]\n",
        "\n",
        "\n",
        "# Combiner toutes les listes de tickers\n",
        "all_tickers = cac40_tickers + nasdaq_tickers + nyse_tickers + macro_tickers\n",
        "\n",
        "# Télécharger les données boursières\n",
        "data = yf.download(all_tickers, start=start_date, end=end_date,auto_adjust=True)\n",
        "\n",
        "# Calculer la variation en pourcentage par rapport à la clôture précédente\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "\n",
        "dataset_ts = variation.reset_index()\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "print(\"Shape:\", dataset_ts.shape)\n",
        "print(dataset_ts.head())\n",
        "print(dataset_ts.tail())\n",
        "print(\"NaN par colonne :\")\n",
        "print(variation.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "pd.DataFrame(cac40_tickers, columns=[\"Ticker\"]).to_csv(\"CAC40_TICKERS.csv\", index=False)\n",
        "dataset_ts.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminée: \")\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection t-1"
      ],
      "metadata": {
        "id": "AAME67UIp6Oc"
      },
      "id": "AAME67UIp6Oc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "# Adapter le chemin si nécessaire\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Détection des règles\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules(\n",
        "    disc,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        future = disc[target].shift(-1)\n",
        "        base_rate = (future == \"UP\").mean()\n",
        "\n",
        "        # On ignore les cibles trop rares\n",
        "        if base_rate < 0.05:\n",
        "            continue\n",
        "\n",
        "        leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "        for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "            # Condition à t\n",
        "            mask = (\n",
        "                (disc[l1] == \"UP\") &\n",
        "                (disc[l2] == \"DOWN\")\n",
        "            )\n",
        "\n",
        "            support = mask.mean()\n",
        "            if support < min_support:\n",
        "                continue\n",
        "\n",
        "            confidence = (future[mask] == \"UP\").mean()\n",
        "            if confidence < min_confidence:\n",
        "                continue\n",
        "\n",
        "            lift = confidence / base_rate\n",
        "            if lift < min_lift:\n",
        "                continue\n",
        "\n",
        "            rules.append({\n",
        "                \"target\": target,\n",
        "                \"leaders\": f\"{l1} ↑ AND {l2} ↓\",\n",
        "                \"support\": round(support, 3),\n",
        "                \"confidence\": round(confidence, 3),\n",
        "                \"lift\": round(lift, 2)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. Exécution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules(disc)\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Résultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 règles détectées :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "# Optionnel : sauvegarde\n",
        "rules.to_csv(\"lead_lag_rules_results1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "tTulpFIuqE4F"
      },
      "id": "tTulpFIuqE4F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Détection des règles avec lag\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_base_rate=0.05\n",
        "):\n",
        "    rules = []\n",
        "    n = len(disc)\n",
        "\n",
        "    for target in disc.columns:\n",
        "\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            future = disc[target]\n",
        "            leaders_disc = disc.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_disc.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_disc = leaders_disc.loc[valid_idx]\n",
        "\n",
        "            base_rate = (future == \"UP\").mean()\n",
        "            if base_rate < min_base_rate:\n",
        "                continue\n",
        "\n",
        "            leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                mask = (\n",
        "                    (leaders_disc[l1] == \"UP\") &\n",
        "                    (leaders_disc[l2] == \"DOWN\")\n",
        "                )\n",
        "\n",
        "                support = mask.mean()\n",
        "                if support < min_support:\n",
        "                    continue\n",
        "\n",
        "                confidence = (future[mask] == \"UP\").mean()\n",
        "                if confidence < min_confidence:\n",
        "                    continue\n",
        "\n",
        "                lift = confidence / base_rate\n",
        "                if lift < min_lift:\n",
        "                    continue\n",
        "\n",
        "                rules.append({\n",
        "                    \"target\": target,\n",
        "                    \"leaders\": f\"{l1} ↑ AND {l2} ↓\",\n",
        "                    \"lag_days\": lag,\n",
        "                    \"support\": round(support, 3),\n",
        "                    \"confidence\": round(confidence, 3),\n",
        "                    \"lift\": round(lift, 2),\n",
        "                    \"occurrences\": int(mask.sum())\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. Exécution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\", \"support\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Résultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 règles détectées :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "rules.to_csv(\"lead_lag_rules_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "vf5ybat9VqvF"
      },
      "id": "vf5ybat9VqvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode cross correlation Lead–Lag multivarié (Lasso)"
      ],
      "metadata": {
        "id": "BdSnwSI0IB7W"
      },
      "id": "BdSnwSI0IB7W"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LEAD–LAG MULTIVARIÉ PAR LASSO (SCRIPT COMPLET)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# ----------------------------\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# gestion de la date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2. NETTOYAGE & SÉCURITÉS\n",
        "# ----------------------------\n",
        "\n",
        "# garder uniquement les colonnes numériques\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "if df.shape[1] == 0:\n",
        "    raise ValueError(\"Aucune colonne numérique exploitable\")\n",
        "\n",
        "# conserver les séries suffisamment complètes (au moins 80% des lignes présentes)\n",
        "threshold = int(len(df) * 0.8)\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# supprimer les lignes avec NaN restantes\n",
        "df = df.dropna()\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset vide après nettoyage. Essayez de réduire le threshold ou de vérifier la source des données.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. STANDARDISATION\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df.values),\n",
        "    index=df.index,\n",
        "    columns=df.columns\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4. CONSTRUCTION DES LAGS\n",
        "# ----------------------------\n",
        "def build_lagged_matrix(df, max_lag):\n",
        "    X = []\n",
        "    feature_names = []\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        shifted = df.shift(lag)\n",
        "        X.append(shifted.values)\n",
        "\n",
        "        for col in df.columns:\n",
        "            feature_names.append(f\"{col}_lag{lag}\")\n",
        "\n",
        "    X = np.hstack(X)\n",
        "    return X, feature_names\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5. LASSO MULTIVARIÉ LEAD–LAG\n",
        "# ----------------------------\n",
        "def multivariate_lead_lag_lasso(df, target, max_lag=5, min_coef=1e-4):\n",
        "    X, feature_names = build_lagged_matrix(df, max_lag)\n",
        "    y = df[target].values\n",
        "\n",
        "    # alignement temporel\n",
        "    X = X[max_lag:]\n",
        "    y = y[max_lag:]\n",
        "\n",
        "    model = LassoCV(\n",
        "        cv=5,\n",
        "        max_iter=5000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for coef, fname in zip(model.coef_, feature_names):\n",
        "        if abs(coef) >= min_coef:\n",
        "            col, lag = fname.rsplit(\"_lag\", 1)\n",
        "            results.append({\n",
        "                \"target\": target,\n",
        "                \"leader\": col,\n",
        "                \"lag_days\": int(lag),\n",
        "                \"coefficient\": coef\n",
        "            })\n",
        "\n",
        "    # Gestion du cas où aucun coefficient n'est retenu (évite KeyError)\n",
        "    if not results:\n",
        "        return pd.DataFrame(columns=[\"target\", \"leader\", \"lag_days\", \"coefficient\"])\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6. LANCEMENT GLOBAL\n",
        "# ----------------------------\n",
        "MAX_LAG = 5\n",
        "all_results = []\n",
        "\n",
        "for target in df_scaled.columns:\n",
        "    res = multivariate_lead_lag_lasso(\n",
        "        df_scaled,\n",
        "        target=target,\n",
        "        max_lag=MAX_LAG\n",
        "    )\n",
        "\n",
        "    if not res.empty:\n",
        "        all_results.append(res)\n",
        "\n",
        "if not all_results:\n",
        "    print(\"Aucun lead-lag significatif trouvé.\")\n",
        "    best_results = pd.DataFrame()\n",
        "else:\n",
        "    final_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 7. RÉSULTATS FINAUX\n",
        "    # ----------------------------\n",
        "    best_results = final_results.sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    print(best_results.head(20))\n",
        "\n",
        "    # ----------------------------\n",
        "    # 8. EXPORT OPTIONNEL\n",
        "    # ----------------------------\n",
        "    best_results.to_csv(\n",
        "        \"multivariate_lead_lag_lasso_results.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# FIN DU SCRIPT\n",
        "# ============================================================"
      ],
      "metadata": {
        "id": "j6SeFQr3H8PV"
      },
      "id": "j6SeFQr3H8PV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Règles vers features ML Section"
      ],
      "metadata": {
        "id": "5AcBjr6xB4TW"
      },
      "id": "5AcBjr6xB4TW"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "DATA_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "RULES_PATH = \"lead_lag_rules_results.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "dates = df[\"date\"] if \"date\" in df.columns else None\n",
        "df = df.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "rules_df = pd.read_csv(RULES_PATH)\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation (identique à la détection)\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Parsing des règles\n",
        "# =========================\n",
        "\n",
        "def parse_leaders(leaders_str):\n",
        "    \"\"\"\n",
        "    Ex: 'TEP.PAS ↑ AND ^NDX ↓'\n",
        "    \"\"\"\n",
        "    l1, l2 = leaders_str.split(\" AND \")\n",
        "\n",
        "    def parse_one(s):\n",
        "        name, arrow = s.split(\" \")\n",
        "        direction = \"UP\" if \"↑\" in arrow else \"DOWN\"\n",
        "        return name, direction\n",
        "\n",
        "    return parse_one(l1), parse_one(l2)\n",
        "\n",
        "# =========================\n",
        "# 4. Génération des features binaires\n",
        "# =========================\n",
        "\n",
        "feature_cols = {}\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "\n",
        "    (l1_name, l1_dir), (l2_name, l2_dir) = parse_leaders(rule[\"leaders\"])\n",
        "    lag = int(rule[\"lag_days\"])\n",
        "\n",
        "    cond = (\n",
        "        (disc[l1_name].shift(lag) == l1_dir) &\n",
        "        (disc[l2_name].shift(lag) == l2_dir)\n",
        "    )\n",
        "\n",
        "    feature_cols[f\"rule_{idx}\"] = cond.astype(int)\n",
        "\n",
        "# Construction finale en UNE fois\n",
        "features = pd.DataFrame(feature_cols, index=disc.index)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. Pondération des règles\n",
        "# =========================\n",
        "\n",
        "weighted_features = features.copy()\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "    weight = rule[\"lift\"] * rule[\"confidence\"]\n",
        "    weighted_features[f\"rule_{idx}\"] *= weight\n",
        "\n",
        "# =========================\n",
        "# 6. Agrégation (features robustes)\n",
        "# =========================\n",
        "\n",
        "agg_features = pd.DataFrame(index=features.index)\n",
        "\n",
        "agg_features[\"rules_count\"] = (features > 0).sum(axis=1)\n",
        "agg_features[\"rules_strength\"] = weighted_features.sum(axis=1)\n",
        "\n",
        "# =========================\n",
        "# 7. Dataset ML final\n",
        "# =========================\n",
        "\n",
        "X = pd.concat(\n",
        "    [\n",
        "        agg_features,\n",
        "        weighted_features\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Nettoyage des NaN dus aux lags\n",
        "X = X.dropna()\n",
        "\n",
        "# =========================\n",
        "# 8. Exemple de target ML\n",
        "# =========================\n",
        "\n",
        "TARGET = \"NVDA\"  # à adapter\n",
        "\n",
        "y = (df[TARGET] > 1.0).astype(int)\n",
        "y = y.loc[X.index]\n",
        "\n",
        "# =========================\n",
        "# 9. Export final\n",
        "# =========================\n",
        "\n",
        "if dates is not None:\n",
        "    X.insert(0, \"date\", dates.loc[X.index])\n",
        "\n",
        "X.to_csv(\"ml_features_from_rules.csv\", index=False)\n",
        "\n",
        "print(\"✅ Dataset ML généré\")\n",
        "print(\"Shape X :\", X.shape)\n",
        "print(\"Target positives :\", y.mean())\n"
      ],
      "metadata": {
        "id": "lwy4zRekB20J"
      },
      "id": "lwy4zRekB20J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lead Lag search"
      ],
      "metadata": {
        "id": "Z1jerT1XM-pP"
      },
      "id": "Z1jerT1XM-pP"
    },
    {
      "cell_type": "code",
      "source": [
        "# The following line caused a ModuleNotFoundError because 'toto' is not a valid or installed module.\n",
        "# import toto\n",
        "\n",
        "# To fix this, replace 'toto' with the actual library you intended to use.\n",
        "# If you need to install a package, use: !pip install <package_name>"
      ],
      "metadata": {
        "id": "owfOt5FAWCgN"
      },
      "id": "owfOt5FAWCgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7",
      "metadata": {
        "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7"
      },
      "outputs": [],
      "source": [
        "### Recherche des leads/lags sur les actions du CAC40\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger le CSV\n",
        "dataset_ts = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]   # très important\n",
        ")\n",
        "\n",
        "# Trier par date (sécurité)\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "# Optionnel : remettre la date en index\n",
        "dataset_ts = dataset_ts.set_index(\"date\")\n",
        "\n",
        "def scan_lead_lag_cac40(\n",
        "    df,\n",
        "    cac40_cols,\n",
        "    other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40,\n",
        "    method=\"pearson\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Scan exhaustif lead/lag entre :\n",
        "    - other_cols (X candidates)\n",
        "    - cac40_cols (Y targets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for y in cac40_cols:\n",
        "        for x in other_cols:\n",
        "\n",
        "            if x == y:\n",
        "                continue\n",
        "\n",
        "            best_row = None\n",
        "\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "\n",
        "                if lag < 0:\n",
        "                    x_shifted = df[x].shift(-lag)\n",
        "                    y_shifted = df[y]\n",
        "                else:\n",
        "                    x_shifted = df[x]\n",
        "                    y_shifted = df[y].shift(lag)\n",
        "\n",
        "                pair = pd.concat([x_shifted, y_shifted], axis=1).dropna()\n",
        "\n",
        "                if len(pair) < min_obs:\n",
        "                    continue\n",
        "\n",
        "                corr = pair.iloc[:, 0].corr(pair.iloc[:, 1], method=method)\n",
        "\n",
        "                if pd.isna(corr):\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"x\": x,\n",
        "                    \"y\": y,\n",
        "                    \"lag\": lag,\n",
        "                    \"correlation\": corr,\n",
        "                    \"abs_corr\": abs(corr),\n",
        "                    \"direction\": \"inverse\" if corr < 0 else \"same\",\n",
        "                    \"n_obs\": len(pair)\n",
        "                }\n",
        "\n",
        "                if best_row is None or row[\"abs_corr\"] > best_row[\"abs_corr\"]:\n",
        "                    best_row = row\n",
        "\n",
        "            if best_row is not None:\n",
        "                results.append(best_row)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df = dataset_ts\n",
        "cac40_cols = pd.read_csv(\"CAC40_TICKERS.csv\")['Ticker'].tolist()\n",
        "other_cols = [c for c in df.columns if c not in cac40_cols]\n",
        "\n",
        "results = scan_lead_lag_cac40(\n",
        "    df=df,\n",
        "    cac40_cols=cac40_cols,\n",
        "    other_cols=other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40\n",
        ")\n",
        "\n",
        "results.sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "results[\n",
        "    results[\"direction\"] == \"inverse\"\n",
        "].sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "\n",
        "results[results[\"y\"] == \"MC.PA\"] \\\n",
        "    .sort_values(\"abs_corr\", ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(\"Shape:\", results.shape)\n",
        "results.to_csv(\"LEAD_LAG_SCAN_CAC40.csv\", index=False)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des leads/lags sur les actions du CAC40 terminée: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candidates selection"
      ],
      "metadata": {
        "id": "5rIc4-G4NG_T"
      },
      "id": "5rIc4-G4NG_T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
      },
      "outputs": [],
      "source": [
        "### Recherche des variables candidates qui ont des leads/lags significatifs sur le CAC40\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU CSV\n",
        "# =========================\n",
        "\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "print(\"Dataset initial :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 2. FILTRE : LAG ≠ 0\n",
        "# =========================\n",
        "\n",
        "leadlag = leadlag[leadlag[\"lag\"] != 0]\n",
        "\n",
        "print(\"Après suppression lag = 0 :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 3. CLASSIFICATION DU RISQUE\n",
        "# =========================\n",
        "\n",
        "def classify_risk(abs_corr, n_obs):\n",
        "    if abs_corr >= 0.55 and n_obs >= 80:\n",
        "        return \"very_low_risk\"\n",
        "    elif abs_corr >= 0.45 and n_obs >= 60:\n",
        "        return \"low_risk\"\n",
        "    elif abs_corr >= 0.40 and n_obs >= 50:\n",
        "        return \"medium_risk\"\n",
        "    elif abs_corr >= 0.30 and n_obs >= 40:\n",
        "        return \"high_risk\"\n",
        "    else:\n",
        "        return \"very_high_risk\"\n",
        "\n",
        "leadlag[\"risk_level\"] = leadlag.apply(\n",
        "    lambda r: classify_risk(r[\"abs_corr\"], r[\"n_obs\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. TRI FINAL (DU + SÛR AU + RISQUÉ)\n",
        "# =========================\n",
        "\n",
        "risk_order = [\n",
        "    \"very_low_risk\",\n",
        "    \"low_risk\",\n",
        "    \"medium_risk\",\n",
        "    \"high_risk\",\n",
        "    \"very_high_risk\"\n",
        "]\n",
        "\n",
        "leadlag[\"risk_level\"] = pd.Categorical(\n",
        "    leadlag[\"risk_level\"],\n",
        "    categories=risk_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "leadlag_sorted = leadlag.sort_values(\n",
        "    [\"risk_level\", \"abs_corr\"],\n",
        "    ascending=[True, False]\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. CONTRÔLE RAPIDE\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 relations les plus sûres (lag ≠ 0) :\")\n",
        "print(leadlag_sorted.head(20))\n",
        "\n",
        "# =========================\n",
        "# 6. SAUVEGARDE\n",
        "# =========================\n",
        "\n",
        "leadlag_sorted.to_csv(\n",
        "    \"LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nCSV sauvegardé : LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\")\n",
        "\n",
        "# Charger le scan lead/lag\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "# Filtre SAFE\n",
        "filtered = leadlag[\n",
        "    (leadlag[\"lag\"] != 0) &\n",
        "    (leadlag[\"abs_corr\"] >= 0.25) &\n",
        "    (leadlag[\"n_obs\"] >= 40)\n",
        "]\n",
        "\n",
        "# Extraire les candidats (variables X)\n",
        "candidatesX = (\n",
        "    filtered[\"x\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesX.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate variables\")\n",
        "print(candidatesX)\n",
        "pd.DataFrame(candidatesX, columns=[\"variable\"]).to_csv(\"CANDIDATES_VARIABLES.csv\", index=False)\n",
        "\n",
        "# Extraire les cibles (variables y)\n",
        "candidatesY = (\n",
        "    filtered[\"y\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesY.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate taget\")\n",
        "print(candidatesY)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des variables candidates CANDIDATES_VARIABLES.csv terminée: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ],
      "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "Uij5aRPaK47h"
      },
      "id": "Uij5aRPaK47h"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"^NDX\", \"LIN\", \"ABBV\", \"CS.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"^NDX\", \"LIN\", \"ABBV\"]]\n",
        "y = df[\"CS.PA\"].shift(-1)  # URW.PA à J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"^NDX\", \"LIN\", \"ABBV\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraîner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"^NDX\", \"LIN\", \"ABBV\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"^NDX\", \"LIN\", \"ABBV\"]]],\n",
        "        columns=[\"^NDX\", \"LIN\", \"ABBV\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÉDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"CS.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"CS.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÉSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Création du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"CS.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"CS.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Vérification\n",
        "print(final_dataset[[\n",
        "    \"CS.PA\",\n",
        "    \"CS.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"CS_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "HqrQ5wv0WwYg"
      },
      "id": "HqrQ5wv0WwYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# URW.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "zyY4sc6rMDr0"
      },
      "id": "zyY4sc6rMDr0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"GC=F\", \"^NDX\", \"AAPL\", \"URW.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"GC=F\", \"^NDX\", \"AAPL\"]]\n",
        "y = df[\"URW.PA\"].shift(-1)  # URW.PA à J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"GC=F\", \"^NDX\", \"AAPL\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraîner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"GC=F\", \"^NDX\", \"AAPL\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"GC=F\", \"^NDX\", \"AAPL\"]]],\n",
        "        columns=[\"GC=F\", \"^NDX\", \"AAPL\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÉDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"URW.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"URW.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÉSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Création du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"URW.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"URW.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Vérification\n",
        "print(final_dataset[[\n",
        "    \"URW.PA\",\n",
        "    \"URW.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"URW_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "khd_HR51LwOG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "khd_HR51LwOG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect saxo bank"
      ],
      "metadata": {
        "id": "R_gn7zKPNkJh"
      },
      "id": "R_gn7zKPNkJh"
    },
    {
      "cell_type": "code",
      "source": [
        "# test conncetion Saxo bank simulation\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "\n",
        "ACCESS_TOKEN = \"eyJhbGciOiJFUzI1NiIsIng1dCI6IjY3NEM0MjFEMzZEMUE1OUNFNjFBRTIzMjMyOTVFRTAyRTc3MDMzNTkifQ.eyJvYWEiOiI3Nzc3NSIsImlzcyI6Im9hIiwiYWlkIjoiMTA5IiwidWlkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiY2lkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiaXNhIjoiRmFsc2UiLCJ0aWQiOiIyMDAyIiwic2lkIjoiMzYwNGFlY2NmYTc3NGNlNmEzNGZmNDIyNGUyZDBmNWEiLCJkZ2kiOiI4NCIsImV4cCI6IjE3Njg4MTEzMjgiLCJvYWwiOiIxRiIsImlpZCI6IjFkNzAzNjg4NzM4MTQzNjMwNGE1MDhkZTRjNTUxOTUwIn0.NuSPihbRVQ_YHoWnW-sQHESQ9QNBExUgGpEM-GaD7gWOQfhr2OU7D4qmgGjhLEW7FNDKsdkL0bRfwf5xuOzftg\"\n",
        "BASE_URL = \"https://gateway.saxobank.com/sim/openapi\"  # ou /sim/openapi\n",
        "SYMBOL = {\"URW.PA\" : \"19099381\"}\n",
        "EXCHANGE = \"Euronext Paris\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "    \"Accept\" : \"*/*\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def get_AccountKeys():\n",
        "    url = f\"{BASE_URL}/port/v1/accounts/me\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"Data\"]\n",
        "\n",
        "def get_Balances(AccountKey):\n",
        "    url = f\"{BASE_URL}/port/v1/balances?AccountKey=\"+AccountKey+\"&ClientKey=\"+AccountKey\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def get_TickerPosition(Uic):\n",
        "    url = f\"{BASE_URL}/trade/v1/infoprices/list?AccountKey=\"+AccountKey+\"&Uics=\"+Uic+\"&AssetType=Stock&Amount=100000&FieldGroups=DisplayAndFormat,Quote\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "AccountKeys = get_AccountKeys()\n",
        "AccountKey = AccountKeys[0][\"AccountKey\"]\n",
        "Balances = get_Balances(AccountKey)\n",
        "URW = get_TickerPosition(SYMBOL[\"URW.PA\"])\n",
        "print(URW)\n",
        "\n",
        "print(AccountKey)\n",
        "print(Balances)"
      ],
      "metadata": {
        "id": "SWGNJbYASMC1"
      },
      "id": "SWGNJbYASMC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today prediction"
      ],
      "metadata": {
        "id": "HDaUtak-Lj_h"
      },
      "id": "HDaUtak-Lj_h"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CS.PA – SIGNAL QUANT +\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# PARAMÈTRES GÉNÉRAUX\n",
        "# =========================\n",
        "\n",
        "CSV_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "FEATURES = [\"^NDX\", \"LIN\",\"ABBV\"]\n",
        "TARGET   = \"CS.PA\"\n",
        "\n",
        "BUY_THRESHOLD  = 0.5\n",
        "SELL_THRESHOLD = -0.5\n",
        "\n",
        "MIN_TRAIN_SIZE = 60\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGER DATASET HISTORIQUE\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "df = df[FEATURES + [TARGET]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. ENTRAÎNEMENT DU MODÈLE (lag = 1)\n",
        "# =========================\n",
        "\n",
        "X = df[FEATURES]\n",
        "y = df[TARGET].shift(-1)\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = FEATURES + [\"CS_TARGET\"]\n",
        "\n",
        "X_train = sm.add_constant(data[FEATURES], has_constant=\"add\")\n",
        "y_train = data[\"CS_TARGET\"]\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# =========================\n",
        "# 3. DONNÉES LIVE (clôture US)\n",
        "# =========================\n",
        "\n",
        "tickers = FEATURES + [TARGET]\n",
        "\n",
        "prices = yf.download(\n",
        "    tickers,\n",
        "    period=\"2d\",\n",
        "    interval=\"1d\",\n",
        "    auto_adjust=True,\n",
        "    progress=False\n",
        ")[\"Close\"]\n",
        "\n",
        "returns_today = prices.pct_change(fill_method=None).iloc[-1] * 100\n",
        "\n",
        "missing = returns_today[FEATURES].isna()\n",
        "\n",
        "if missing.any():\n",
        "    print(\"⚠️ Marchés non clôturés :\", missing[missing].index.tolist())\n",
        "    predicted_return = 0.0\n",
        "    signal = \"KEEP\"\n",
        "\n",
        "else:\n",
        "    X_today = pd.DataFrame(\n",
        "        [[\n",
        "            returns_today[\"^NDX\"],\n",
        "            returns_today[\"LIN\"],\n",
        "            returns_today[\"ABBV\"]\n",
        "        ]],\n",
        "        columns=FEATURES\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    predicted_return = model.predict(X_today).iloc[0]\n",
        "\n",
        "    if predicted_return > BUY_THRESHOLD:\n",
        "        signal = \"BUY\"\n",
        "    elif predicted_return < SELL_THRESHOLD:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"KEEP\"\n",
        "\n",
        "# =========================\n",
        "# 6. SORTIE\n",
        "# =========================\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"   SIGNAL URW.PA (J+1)\")\n",
        "print(\"==============================\")\n",
        "print(\"Date :\", datetime.now().strftime(\"%Y-%m-%d %H:%M CET\"))\n",
        "print(\"\\nVariations aujourd'hui (%)\")\n",
        "print(returns_today[FEATURES])\n",
        "print(f\"\\nPrévision URW.PA J+1 : {predicted_return:.3f} %\")\n",
        "print(\"==============================\\n\")"
      ],
      "metadata": {
        "id": "9RikQhCXSRQb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9RikQhCXSRQb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}