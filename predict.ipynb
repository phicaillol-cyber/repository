{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
        "outputId": "2ef9d1e8-7add-45cb-ed85-b07c37c367f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[**********************95%*********************  ]  124 of 131 completedERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: CDXBTC-USD\"}}}\n",
            "[*********************100%***********************]  131 of 131 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['CDXBTC-USD']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (710, 132)\n",
            "Ticker       date      AAPL      ABBV       ABT    ACA.PA       ACN      ADBE  \\\n",
            "0      2023-05-17  0.360300  0.041878 -0.521075 -0.289895  1.948484  3.338072   \n",
            "1      2023-05-18  1.366601  0.062762 -0.321628  0.991954  1.001301  1.065527   \n",
            "2      2023-05-19  0.062835  1.164281  0.424073  0.508052  0.845287  3.001972   \n",
            "3      2023-05-22 -0.548053 -0.144718 -0.624254  0.454918  0.358734  0.215485   \n",
            "4      2023-05-23 -1.515508 -1.642526 -2.318720  0.318687 -0.979572 -0.438106   \n",
            "\n",
            "Ticker       ADI     AI.PA    AIR.PA  ...      ^DJI      ^FVX      ^IRX  \\\n",
            "0       2.828393  0.037619  1.552598  ...  1.237814  1.987505  0.593115   \n",
            "1       2.287703  1.065162  0.468026  ...  0.344518  2.951001  0.727205   \n",
            "2      -0.899810  1.004331  0.341618  ... -0.325863  1.352081 -0.780487   \n",
            "3       0.409387 -0.282354 -0.804715  ... -0.418980  0.506938  0.196652   \n",
            "4      -1.771984 -1.514206 -1.497658  ... -0.694173 -0.504382  1.138373   \n",
            "\n",
            "Ticker      ^NDX  ^SP500-15  ^SP500-20  ^STOXX50E      ^TNX      ^TYX  \\\n",
            "0       1.215923   0.666612   1.700486   0.178895  0.901664  0.180831   \n",
            "1       1.805546   0.548096   0.667387        NaN  1.870983  0.593089   \n",
            "2      -0.225014   0.259382  -0.268740        NaN  1.206138  1.204818   \n",
            "3       0.335060  -0.547745   0.003565  -0.220006  0.731316  0.557246   \n",
            "4      -1.279448  -1.536429  -1.232121  -0.986175 -0.564672 -0.478591   \n",
            "\n",
            "Ticker      ^VIX  \n",
            "0      -6.225675  \n",
            "1      -4.860709  \n",
            "2       4.735204  \n",
            "3       2.379534  \n",
            "4       7.669969  \n",
            "\n",
            "[5 rows x 132 columns]\n",
            "Ticker       date      AAPL      ABBV       ABT    ACA.PA       ACN      ADBE  \\\n",
            "705    2026-02-03 -0.196289  0.008866 -0.356464  1.433990 -9.588066 -7.311341   \n",
            "706    2026-02-04  2.601298 -3.788887 -0.752155 -3.040819  0.182408  2.861030   \n",
            "707    2026-02-05 -0.209768  0.879740  0.813313 -2.503438 -3.339537 -3.689527   \n",
            "708    2026-02-06  0.800983  2.013509  1.604327  0.818287  3.013954 -0.374925   \n",
            "709    2026-02-09       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "Ticker       ADI     AI.PA    AIR.PA  ...      ^DJI      ^FVX      ^IRX  \\\n",
            "705    -1.757867  0.025084 -1.836944  ... -0.337340  0.052147  0.335379   \n",
            "706     2.939379  5.706049 -1.040796  ...  0.528650 -0.104247  0.139279   \n",
            "707     0.524277 -0.462688  0.669292  ... -1.197104 -2.165404 -0.472878   \n",
            "708    -0.518435 -0.035755  0.939214  ...  2.467767  0.133336  0.335379   \n",
            "709          NaN       NaN       NaN  ...       NaN -0.053271 -0.139279   \n",
            "\n",
            "Ticker      ^NDX  ^SP500-15  ^SP500-20  ^STOXX50E      ^TNX      ^TYX  \\\n",
            "705    -1.554048   2.001594   0.893708  -0.202408 -0.023390 -0.061108   \n",
            "706    -1.765601   1.804768   0.239105  -0.414986  0.023396  0.183445   \n",
            "707    -1.376190  -2.751861  -0.607487  -0.749858 -1.520469 -1.078331   \n",
            "708     2.147080   1.770942   2.844084   1.226854 -0.095016 -0.143973   \n",
            "709          NaN        NaN        NaN        NaN  0.237761  0.370754   \n",
            "\n",
            "Ticker       ^VIX  \n",
            "705     10.159118  \n",
            "706      3.555552  \n",
            "707     16.791852  \n",
            "708     -6.430866  \n",
            "709     -8.394703  \n",
            "\n",
            "[5 rows x 132 columns]\n",
            "NaN par colonne :\n",
            "Ticker\n",
            "CDXBTC-USD    710\n",
            "^CIISCSEP     710\n",
            "FISV           53\n",
            "ABT            51\n",
            "ABBV           51\n",
            "AAPL           51\n",
            "ADI            51\n",
            "ASML           51\n",
            "AMZN           51\n",
            "AMGN           51\n",
            "ACN            51\n",
            "AVGO           51\n",
            "BAC            51\n",
            "BKNG           51\n",
            "CSCO           51\n",
            "dtype: int64\n",
            "Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminÃ©e: \n",
            "2026-02-09 14:28:34.830470\n"
          ]
        }
      ],
      "source": [
        "### Creation du dataset de variation des tickers des 1000 derniers jours\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# DÃ©finir la pÃ©riode de 1000 jours\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# Liste des tickers du CAC 40 (exemple : 'OR.PA' pour L'OrÃ©al)\n",
        "cac40_tickers = cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # CrÃ©dit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault SystÃ¨mes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # HermÃ¨s\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'OrÃ©al\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # SociÃ©tÃ© GÃ©nÃ©rale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# Liste des tickers du NASDAQ (exemple : 'AAPL' pour Apple)\n",
        "nasdaq_tickers = nasdaq_top_40 = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices (AMD)\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # MondelÄ“z\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\", # ASML Holding\n",
        "    \"MRNA\", # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "nyse_tickers = nyse_top_40 = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"CSCO\",  # Cisco (aussi cotÃ© sur NYSE)\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\",    # Altria\n",
        "]\n",
        "\n",
        "# Inclure Ã©galement les indices macro (VIX, Dollar Index, Brent, Or)\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold Futures COMEX)\n",
        "    \"BZ=F\",        # PÃ©trole Brent (Brent Crude Oil Futures)\n",
        "    \"^NDX\",        # NASDAQ 100 (technologie US, croissance)\n",
        "    \"^DJI\",        # Dow Jones Industrial Average (blue chips US)\n",
        "    \"^SP500-20\",   # S&P 500 Industrials (secteur industriel)\n",
        "    \"^SP500-15\",   # S&P 500 Materials (matiÃ¨res premiÃ¨res)\n",
        "    \"^STOXX50E\",   # Euro STOXX 50 (grandes capitalisations zone euro)\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY â€“ force du dollar US)\n",
        "    \"EUR=X\",       # Taux de change EUR/USD\n",
        "    \"CHF=X\",       # Taux de change USD/CHF (valeur refuge)\n",
        "    \"^VIX\",        # Indice de volatilitÃ© (peur / stress de marchÃ©)\n",
        "    \"^IRX\",        # Taux US 13 semaines (T-Bills court terme)\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans (benchmark macro mondial)\n",
        "    \"^TYX\",        # Taux US 30 ans (long terme)\n",
        "    \"^CIISCSEP\",   # Indice de surprises Ã©conomiques Citi (US)\n",
        "    \"^MOVE\",       # Indice de crÃ©dit (Credit Default Swaps â€“ stress crÃ©dit)\n",
        "    \"BTC-USD\",     # Bitcoin\n",
        "    \"^T5YIE\",      # Breakeven inflation 5 ans\n",
        "    \"HYG\",         # CrÃ©dit high yield (risk-on/off)\n",
        "    \"LQD\",         # CrÃ©dit investment grade\n",
        "    \"000001.SS\",   # Shanghai Composite\n",
        "    \"FXI\",         # ETF China large caps\n",
        "    \"HG=F\"         # Cuivre (Dr Copper)\n",
        "]\n",
        "\n",
        "\n",
        "# Combiner toutes les listes de tickers\n",
        "all_tickers = cac40_tickers + nasdaq_tickers + nyse_tickers + macro_tickers\n",
        "\n",
        "# TÃ©lÃ©charger les donnÃ©es boursiÃ¨res\n",
        "data = yf.download(all_tickers, start=start_date, end=end_date,auto_adjust=True)\n",
        "\n",
        "# Calculer la variation en pourcentage par rapport Ã  la clÃ´ture prÃ©cÃ©dente\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "\n",
        "dataset_ts = variation.reset_index()\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "print(\"Shape:\", dataset_ts.shape)\n",
        "print(dataset_ts.head())\n",
        "print(dataset_ts.tail())\n",
        "print(\"NaN par colonne :\")\n",
        "print(variation.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "pd.DataFrame(cac40_tickers, columns=[\"Ticker\"]).to_csv(\"CAC40_TICKERS.csv\", index=False)\n",
        "dataset_ts.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminÃ©e: \")\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag 1-3 jours"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donnÃ©es\n",
        "# =========================\n",
        "\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# 2. DiscrÃ©tisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x, thr=1.0):\n",
        "    if x > thr:\n",
        "        return \"UP\"\n",
        "    elif x < -thr:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. DÃ©finition des patterns\n",
        "# =========================\n",
        "\n",
        "LEADER_PATTERNS = [\n",
        "    (\"UP\", \"UP\"),\n",
        "    (\"UP\", \"DOWN\"),\n",
        "    (\"DOWN\", \"UP\"),\n",
        "    (\"DOWN\", \"DOWN\"),\n",
        "]\n",
        "\n",
        "TARGET_STATES = [\"UP\", \"DOWN\"]\n",
        "\n",
        "# =========================\n",
        "# 4. Lift conditionnel (pattern-aware)\n",
        "# =========================\n",
        "\n",
        "def conditional_lift(disc, target, l1, l2, s1, s2, lag, target_state):\n",
        "    shifted = disc.shift(lag)\n",
        "    valid = shifted[[l1, l2, target]].dropna()\n",
        "\n",
        "    base_rate = (valid[target] == target_state).mean()\n",
        "    if base_rate == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Lift avec l1 seul\n",
        "    mask_l1 = valid[l1] == s1\n",
        "    if mask_l1.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    lift_l1 = (\n",
        "        (valid.loc[mask_l1, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    # Lift avec l1 + l2\n",
        "    mask_l1_l2 = (valid[l1] == s1) & (valid[l2] == s2)\n",
        "    if mask_l1_l2.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    lift_l1_l2 = (\n",
        "        (valid.loc[mask_l1_l2, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    return lift_l1_l2 - lift_l1\n",
        "\n",
        "# =========================\n",
        "# 5. Mining des rÃ¨gles exhaustives\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules_exhaustive(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05,\n",
        "    min_base_rate=0.05,\n",
        "    max_corr=0.85\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            print(f\"\\nTarget: {target} | lag: {lag}\")\n",
        "\n",
        "            future = disc[target]\n",
        "            leaders_disc = disc.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_disc.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_disc = leaders_disc.loc[valid_idx]\n",
        "\n",
        "            leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                # ---- filtre corrÃ©lation brute\n",
        "                corr = abs(df[l1].corr(df[l2]))\n",
        "                if corr > max_corr:\n",
        "                    continue\n",
        "\n",
        "                for s1, s2 in LEADER_PATTERNS:\n",
        "\n",
        "                    mask = (\n",
        "                        (leaders_disc[l1] == s1) &\n",
        "                        (leaders_disc[l2] == s2)\n",
        "                    )\n",
        "\n",
        "                    support = mask.mean()\n",
        "                    if support < min_support:\n",
        "                        continue\n",
        "\n",
        "                    for target_state in TARGET_STATES:\n",
        "\n",
        "                        base_rate = (future == target_state).mean()\n",
        "                        if base_rate < min_base_rate:\n",
        "                            continue\n",
        "\n",
        "                        confidence = (future[mask] == target_state).mean()\n",
        "                        if confidence < min_confidence:\n",
        "                            continue\n",
        "\n",
        "                        lift = confidence / base_rate\n",
        "                        if lift < min_lift:\n",
        "                            continue\n",
        "\n",
        "                        delta_lift = conditional_lift(\n",
        "                            disc, target, l1, l2, s1, s2, lag, target_state\n",
        "                        )\n",
        "                        if delta_lift < min_conditional_lift:\n",
        "                            continue\n",
        "\n",
        "                        rules.append({\n",
        "                            \"target\": target,\n",
        "                            \"target_direction\": target_state,\n",
        "                            \"leader_1\": l1,\n",
        "                            \"leader_2\": l2,\n",
        "                            \"leader_1_state\": s1,\n",
        "                            \"leader_2_state\": s2,\n",
        "                            \"lag_days\": lag,\n",
        "                            \"support\": round(support, 3),\n",
        "                            \"confidence\": round(confidence, 3),\n",
        "                            \"lift\": round(lift, 2),\n",
        "                            \"delta_lift\": round(delta_lift, 2),\n",
        "                            \"corr_leaders\": round(corr, 2),\n",
        "                            \"occurrences\": int(mask.sum())\n",
        "                        })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 6. ExÃ©cution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules_exhaustive(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"delta_lift\", \"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "print(\"\\nTop 20 rÃ¨gles exhaustives (2 leaders, tous les sens) :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "rules.to_csv(\"lead_lag_rules_exhaustive_2leaders.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulxQ3yK6AiaM",
        "outputId": "baea5c1f-c8c6-4112-efc8-7132a22b499e"
      },
      "id": "ulxQ3yK6AiaM",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 rÃ¨gles exhaustives (2 leaders, tous les sens) :\n",
            "\n",
            "     target target_direction leader_1 leader_2 leader_1_state leader_2_state  \\\n",
            "71     AMAT             DOWN       GS     LRCX             UP           DOWN   \n",
            "207    ASML             DOWN       GS     LRCX             UP           DOWN   \n",
            "924    NVDA               UP      MRK     ^NDX             UP             UP   \n",
            "2374   ^VIX               UP      PEP     ^NDX             UP           DOWN   \n",
            "921    NVDA               UP      MDT     ^NDX             UP             UP   \n",
            "441    INTC             DOWN      PEP     ^NDX             UP           DOWN   \n",
            "926    NVDA               UP       PM     ^NDX             UP             UP   \n",
            "1261   TSLA             DOWN     BZ=F     ^NDX             UP           DOWN   \n",
            "574     MMM             DOWN       KO     ORCL           DOWN           DOWN   \n",
            "977    PYPL             DOWN      PEP     ^NDX             UP           DOWN   \n",
            "2329   ^VIX               UP       KO     ^NDX             UP           DOWN   \n",
            "1220   TSLA             DOWN      KDP     ^NDX             UP           DOWN   \n",
            "2280   ^VIX               UP   ERF.PA     ^NDX           DOWN           DOWN   \n",
            "953    ORCL             DOWN   DSY.PA     LRCX             UP           DOWN   \n",
            "430    INTC             DOWN    HO.PA     ^NDX           DOWN           DOWN   \n",
            "778      MU             DOWN   ALO.PA     AVGO             UP           DOWN   \n",
            "920    NVDA               UP     MDLZ     MSFT             UP             UP   \n",
            "1344   TSLA             DOWN   PUB.PA     ^NDX           DOWN           DOWN   \n",
            "569    META             DOWN     ABBV     CSCO           DOWN           DOWN   \n",
            "428    INTC             DOWN    HO.PA     LRCX           DOWN           DOWN   \n",
            "\n",
            "      lag_days  support  confidence  lift  delta_lift  corr_leaders  \\\n",
            "71           2    0.032       0.609  2.46        2.48          0.45   \n",
            "207          2    0.032       0.609  2.37        1.98          0.45   \n",
            "924          2    0.035       0.600  1.73        1.64          0.09   \n",
            "2374         2    0.035       0.600  1.76        1.60          0.02   \n",
            "921          2    0.032       0.696  2.00        1.59          0.20   \n",
            "441          2    0.035       0.600  1.92        1.59          0.02   \n",
            "926          2    0.035       0.720  2.07        1.55          0.01   \n",
            "1261         2    0.035       0.600  1.72        1.53          0.09   \n",
            "574          2    0.031       0.636  3.24        1.48          0.08   \n",
            "977          1    0.035       0.680  2.62        1.47          0.02   \n",
            "2329         2    0.034       0.625  1.83        1.46          0.03   \n",
            "1220         1    0.035       0.640  1.84        1.41          0.06   \n",
            "2280         2    0.042       0.600  1.76        1.41          0.07   \n",
            "953          1    0.039       0.607  2.55        1.40          0.08   \n",
            "430          2    0.035       0.680  2.18        1.39          0.01   \n",
            "778          2    0.064       0.600  2.06        1.39          0.05   \n",
            "920          2    0.034       0.625  1.80        1.38          0.01   \n",
            "1344         2    0.041       0.621  1.78        1.38          0.19   \n",
            "569          2    0.032       0.609  2.61        1.34          0.17   \n",
            "428          2    0.052       0.676  2.16        1.34          0.01   \n",
            "\n",
            "      occurrences  \n",
            "71             23  \n",
            "207            23  \n",
            "924            25  \n",
            "2374           25  \n",
            "921            23  \n",
            "441            25  \n",
            "926            25  \n",
            "1261           25  \n",
            "574            22  \n",
            "977            25  \n",
            "2329           24  \n",
            "1220           25  \n",
            "2280           30  \n",
            "953            28  \n",
            "430            25  \n",
            "778            45  \n",
            "920            24  \n",
            "1344           29  \n",
            "569            23  \n",
            "428            37  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Annex*"
      ],
      "metadata": {
        "id": "AGqlLzokPs_r"
      },
      "id": "AGqlLzokPs_r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RÃ¨gles vers features ML Section"
      ],
      "metadata": {
        "id": "5AcBjr6xB4TW"
      },
      "id": "5AcBjr6xB4TW"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donnÃ©es\n",
        "# =========================\n",
        "\n",
        "DATA_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "RULES_PATH = \"lead_lag_rules_results.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "dates = df[\"date\"] if \"date\" in df.columns else None\n",
        "df = df.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "rules_df = pd.read_csv(RULES_PATH)\n",
        "\n",
        "# =========================\n",
        "# 2. DiscrÃ©tisation (identique Ã  la dÃ©tection)\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Parsing des rÃ¨gles\n",
        "# =========================\n",
        "\n",
        "def parse_leaders(leaders_str):\n",
        "    \"\"\"\n",
        "    Ex: 'TEP.PAS â†‘ AND ^NDX â†“'\n",
        "    \"\"\"\n",
        "    l1, l2 = leaders_str.split(\" AND \")\n",
        "\n",
        "    def parse_one(s):\n",
        "        name, arrow = s.split(\" \")\n",
        "        direction = \"UP\" if \"â†‘\" in arrow else \"DOWN\"\n",
        "        return name, direction\n",
        "\n",
        "    return parse_one(l1), parse_one(l2)\n",
        "\n",
        "# =========================\n",
        "# 4. GÃ©nÃ©ration des features binaires\n",
        "# =========================\n",
        "\n",
        "feature_cols = {}\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "\n",
        "    (l1_name, l1_dir), (l2_name, l2_dir) = parse_leaders(rule[\"leaders\"])\n",
        "    lag = int(rule[\"lag_days\"])\n",
        "\n",
        "    cond = (\n",
        "        (disc[l1_name].shift(lag) == l1_dir) &\n",
        "        (disc[l2_name].shift(lag) == l2_dir)\n",
        "    )\n",
        "\n",
        "    feature_cols[f\"rule_{idx}\"] = cond.astype(int)\n",
        "\n",
        "# Construction finale en UNE fois\n",
        "features = pd.DataFrame(feature_cols, index=disc.index)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. PondÃ©ration des rÃ¨gles\n",
        "# =========================\n",
        "\n",
        "weighted_features = features.copy()\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "    weight = rule[\"lift\"] * rule[\"confidence\"]\n",
        "    weighted_features[f\"rule_{idx}\"] *= weight\n",
        "\n",
        "# =========================\n",
        "# 6. AgrÃ©gation (features robustes)\n",
        "# =========================\n",
        "\n",
        "agg_features = pd.DataFrame(index=features.index)\n",
        "\n",
        "agg_features[\"rules_count\"] = (features > 0).sum(axis=1)\n",
        "agg_features[\"rules_strength\"] = weighted_features.sum(axis=1)\n",
        "\n",
        "# =========================\n",
        "# 7. Dataset ML final\n",
        "# =========================\n",
        "\n",
        "X = pd.concat(\n",
        "    [\n",
        "        agg_features,\n",
        "        weighted_features\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Nettoyage des NaN dus aux lags\n",
        "X = X.dropna()\n",
        "\n",
        "# =========================\n",
        "# 8. Exemple de target ML\n",
        "# =========================\n",
        "\n",
        "TARGET = \"NVDA\"  # Ã  adapter\n",
        "\n",
        "y = (df[TARGET] > 1.0).astype(int)\n",
        "y = y.loc[X.index]\n",
        "\n",
        "# =========================\n",
        "# 9. Export final\n",
        "# =========================\n",
        "\n",
        "if dates is not None:\n",
        "    X.insert(0, \"date\", dates.loc[X.index])\n",
        "\n",
        "X.to_csv(\"ml_features_from_rules.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Dataset ML gÃ©nÃ©rÃ©\")\n",
        "print(\"Shape X :\", X.shape)\n",
        "print(\"Target positives :\", y.mean())\n"
      ],
      "metadata": {
        "id": "lwy4zRekB20J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0b4059e7-0d5c-49ab-9933-204782009d49"
      },
      "id": "lwy4zRekB20J",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'lead_lag_rules_results.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3090656290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mrules_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRULES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lead_lag_rules_results.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature selection par variable"
      ],
      "metadata": {
        "id": "OHG0YnIhPrtO"
      },
      "id": "OHG0YnIhPrtO"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# =========================================================\n",
        "# 1. LOAD DATA\n",
        "# =========================================================\n",
        "\n",
        "DATA_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "RULES_PATH = \"ml_features_from_rules.csv\"\n",
        "\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "rules = pd.read_csv(RULES_PATH)\n",
        "\n",
        "# Remove overlapping columns (date, etc.)\n",
        "overlap_cols = set(data.columns).intersection(set(rules.columns))\n",
        "rules = rules.drop(columns=list(overlap_cols))\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([data, rules], axis=1)\n",
        "\n",
        "# Identify raw variables (targets)\n",
        "raw_variables = [c for c in data.columns if c.lower() != \"date\"]\n",
        "\n",
        "# =========================================================\n",
        "# 2. FEATURE SELECTION FUNCTION (PER VARIABLE)\n",
        "# =========================================================\n",
        "\n",
        "def feature_selection_for_variable(\n",
        "    df,\n",
        "    target,\n",
        "    raw_variables,\n",
        "    n_splits=3,\n",
        "    n_repeats=10,\n",
        "    min_importance=0.002\n",
        "):\n",
        "\n",
        "    # Target direction at T+1\n",
        "    y = (df[target].shift(-1) > 0).astype(int).dropna()\n",
        "\n",
        "    # ðŸš¨ Guard: only one class â†’ no model possible\n",
        "    if y.nunique() < 2:\n",
        "        print(f\"âš ï¸ Skipped {target}: only one class in target\")\n",
        "        return pd.Series(dtype=float), [], np.nan\n",
        "\n",
        "    # Features at T (rules only, numeric only)\n",
        "    X = df.loc[y.index].drop(columns=raw_variables, errors=\"ignore\")\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "    # âœ… CRITICAL FIX: NaN = rule inactive\n",
        "    X = X.fillna(0)\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    importances_all = []\n",
        "    acc_scores = []\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Accuracy\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "        # Permutation importance\n",
        "        perm = permutation_importance(\n",
        "            model,\n",
        "            X_test,\n",
        "            y_test,\n",
        "            n_repeats=n_repeats,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        importances_all.append(perm.importances_mean)\n",
        "\n",
        "    mean_importance = np.mean(importances_all, axis=0)\n",
        "\n",
        "    importance_series = pd.Series(\n",
        "        mean_importance,\n",
        "        index=X.columns\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "    selected_features = importance_series[\n",
        "        importance_series > min_importance\n",
        "    ].index.tolist()\n",
        "\n",
        "    return importance_series, selected_features, round(np.mean(acc_scores) * 100, 2)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. RUN FEATURE SELECTION FOR ALL VARIABLES\n",
        "# =========================================================\n",
        "\n",
        "all_results = {}\n",
        "selected_features_dict = {}\n",
        "accuracy_summary = {}\n",
        "\n",
        "# Limit for demonstration to avoid long execution, or run for all\n",
        "for target in raw_variables:\n",
        "    print(f\"\\n===== Feature selection for: {target} =====\")\n",
        "\n",
        "    importance, selected, acc = feature_selection_for_variable(\n",
        "        df,\n",
        "        target,\n",
        "        raw_variables\n",
        "    )\n",
        "\n",
        "    all_results[target] = importance\n",
        "    selected_features_dict[target] = selected\n",
        "    accuracy_summary[target] = acc\n",
        "\n",
        "    print(f\"Accuracy (%): {acc}\")\n",
        "    print(f\"Selected features ({len(selected)}):\")\n",
        "    print(selected[:10])  # show first 10 only\n",
        "\n",
        "# =========================================================\n",
        "# 4. SAVE RESULTS\n",
        "# =========================================================\n",
        "\n",
        "# Save selected rules per variable\n",
        "pd.Series(selected_features_dict).to_json(\n",
        "    \"selected_features_per_variable.json\",\n",
        "    indent=2\n",
        ")\n",
        "\n",
        "# Save accuracy summary\n",
        "pd.Series(accuracy_summary).sort_values(ascending=False).to_csv(\n",
        "    \"accuracy_per_variable.csv\"\n",
        ")\n",
        "\n",
        "# Optional: save full importance table\n",
        "importance_df = pd.DataFrame(all_results).fillna(0)\n",
        "importance_df.to_csv(\"feature_importance_per_variable.csv\")\n",
        "\n",
        "print(\"\\n=== DONE ===\")\n",
        "print(\"Files generated:\")\n",
        "print(\"- selected_features_per_variable.json\")\n",
        "print(\"- accuracy_per_variable.csv\")\n",
        "print(\"- feature_importance_per_variable.csv\")"
      ],
      "metadata": {
        "id": "8NiLe0plPzs0"
      },
      "id": "8NiLe0plPzs0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Methode rules detection 1 jour"
      ],
      "metadata": {
        "id": "JwIh3kCqQqDl"
      },
      "id": "JwIh3kCqQqDl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donnÃ©es\n",
        "# =========================\n",
        "\n",
        "# Adapter le chemin si nÃ©cessaire\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. DiscrÃ©tisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. DÃ©tection des rÃ¨gles\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules(\n",
        "    disc,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        future = disc[target].shift(-1)\n",
        "        base_rate = (future == \"UP\").mean()\n",
        "\n",
        "        # On ignore les cibles trop rares\n",
        "        if base_rate < 0.05:\n",
        "            continue\n",
        "\n",
        "        leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "        for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "            # Condition Ã  t\n",
        "            mask = (\n",
        "                (disc[l1] == \"UP\") &\n",
        "                (disc[l2] == \"DOWN\")\n",
        "            )\n",
        "\n",
        "            support = mask.mean()\n",
        "            if support < min_support:\n",
        "                continue\n",
        "\n",
        "            confidence = (future[mask] == \"UP\").mean()\n",
        "            if confidence < min_confidence:\n",
        "                continue\n",
        "\n",
        "            lift = confidence / base_rate\n",
        "            if lift < min_lift:\n",
        "                continue\n",
        "\n",
        "            rules.append({\n",
        "                \"target\": target,\n",
        "                \"leaders\": f\"{l1} â†‘ AND {l2} â†“\",\n",
        "                \"support\": round(support, 3),\n",
        "                \"confidence\": round(confidence, 3),\n",
        "                \"lift\": round(lift, 2)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. ExÃ©cution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules(disc)\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. RÃ©sultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 rÃ¨gles dÃ©tectÃ©es :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "# Optionnel : sauvegarde\n",
        "rules.to_csv(\"lead_lag_rules_results1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "H0RhEw3vQqDm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "H0RhEw3vQqDm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Methode cross correlation Leadâ€“Lag multivariÃ© (Lasso)"
      ],
      "metadata": {
        "id": "BdSnwSI0IB7W"
      },
      "id": "BdSnwSI0IB7W"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LEADâ€“LAG MULTIVARIÃ‰ PAR LASSO (SCRIPT COMPLET)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# ----------------------------\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# gestion de la date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2. NETTOYAGE & SÃ‰CURITÃ‰S\n",
        "# ----------------------------\n",
        "\n",
        "# garder uniquement les colonnes numÃ©riques\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "if df.shape[1] == 0:\n",
        "    raise ValueError(\"Aucune colonne numÃ©rique exploitable\")\n",
        "\n",
        "# conserver les sÃ©ries suffisamment complÃ¨tes (au moins 80% des lignes prÃ©sentes)\n",
        "threshold = int(len(df) * 0.8)\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# supprimer les lignes avec NaN restantes\n",
        "df = df.dropna()\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset vide aprÃ¨s nettoyage. Essayez de rÃ©duire le threshold ou de vÃ©rifier la source des donnÃ©es.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. STANDARDISATION\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df.values),\n",
        "    index=df.index,\n",
        "    columns=df.columns\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4. CONSTRUCTION DES LAGS\n",
        "# ----------------------------\n",
        "def build_lagged_matrix(df, max_lag):\n",
        "    X = []\n",
        "    feature_names = []\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        shifted = df.shift(lag)\n",
        "        X.append(shifted.values)\n",
        "\n",
        "        for col in df.columns:\n",
        "            feature_names.append(f\"{col}_lag{lag}\")\n",
        "\n",
        "    X = np.hstack(X)\n",
        "    return X, feature_names\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5. LASSO MULTIVARIÃ‰ LEADâ€“LAG\n",
        "# ----------------------------\n",
        "def multivariate_lead_lag_lasso(df, target, max_lag=5, min_coef=1e-4):\n",
        "    X, feature_names = build_lagged_matrix(df, max_lag)\n",
        "    y = df[target].values\n",
        "\n",
        "    # alignement temporel\n",
        "    X = X[max_lag:]\n",
        "    y = y[max_lag:]\n",
        "\n",
        "    model = LassoCV(\n",
        "        cv=5,\n",
        "        max_iter=5000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for coef, fname in zip(model.coef_, feature_names):\n",
        "        if abs(coef) >= min_coef:\n",
        "            col, lag = fname.rsplit(\"_lag\", 1)\n",
        "            results.append({\n",
        "                \"target\": target,\n",
        "                \"leader\": col,\n",
        "                \"lag_days\": int(lag),\n",
        "                \"coefficient\": coef\n",
        "            })\n",
        "\n",
        "    # Gestion du cas oÃ¹ aucun coefficient n'est retenu (Ã©vite KeyError)\n",
        "    if not results:\n",
        "        return pd.DataFrame(columns=[\"target\", \"leader\", \"lag_days\", \"coefficient\"])\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6. LANCEMENT GLOBAL\n",
        "# ----------------------------\n",
        "MAX_LAG = 5\n",
        "all_results = []\n",
        "\n",
        "for target in df_scaled.columns:\n",
        "    res = multivariate_lead_lag_lasso(\n",
        "        df_scaled,\n",
        "        target=target,\n",
        "        max_lag=MAX_LAG\n",
        "    )\n",
        "\n",
        "    if not res.empty:\n",
        "        all_results.append(res)\n",
        "\n",
        "if not all_results:\n",
        "    print(\"Aucun lead-lag significatif trouvÃ©.\")\n",
        "    best_results = pd.DataFrame()\n",
        "else:\n",
        "    final_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 7. RÃ‰SULTATS FINAUX\n",
        "    # ----------------------------\n",
        "    best_results = final_results.sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    print(best_results.head(20))\n",
        "\n",
        "    # ----------------------------\n",
        "    # 8. EXPORT OPTIONNEL\n",
        "    # ----------------------------\n",
        "    best_results.to_csv(\n",
        "        \"multivariate_lead_lag_lasso_results.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# FIN DU SCRIPT\n",
        "# ============================================================"
      ],
      "metadata": {
        "id": "j6SeFQr3H8PV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j6SeFQr3H8PV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Lead Lag search"
      ],
      "metadata": {
        "id": "Z1jerT1XM-pP"
      },
      "id": "Z1jerT1XM-pP"
    },
    {
      "cell_type": "code",
      "source": [
        "# The following line caused a ModuleNotFoundError because 'toto' is not a valid or installed module.\n",
        "# import toto\n",
        "\n",
        "# To fix this, replace 'toto' with the actual library you intended to use.\n",
        "# If you need to install a package, use: !pip install <package_name>"
      ],
      "metadata": {
        "id": "owfOt5FAWCgN"
      },
      "id": "owfOt5FAWCgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7",
      "metadata": {
        "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7"
      },
      "outputs": [],
      "source": [
        "### Recherche des leads/lags sur les actions du CAC40\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger le CSV\n",
        "dataset_ts = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]   # trÃ¨s important\n",
        ")\n",
        "\n",
        "# Trier par date (sÃ©curitÃ©)\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "# Optionnel : remettre la date en index\n",
        "dataset_ts = dataset_ts.set_index(\"date\")\n",
        "\n",
        "def scan_lead_lag_cac40(\n",
        "    df,\n",
        "    cac40_cols,\n",
        "    other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40,\n",
        "    method=\"pearson\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Scan exhaustif lead/lag entre :\n",
        "    - other_cols (X candidates)\n",
        "    - cac40_cols (Y targets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for y in cac40_cols:\n",
        "        for x in other_cols:\n",
        "\n",
        "            if x == y:\n",
        "                continue\n",
        "\n",
        "            best_row = None\n",
        "\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "\n",
        "                if lag < 0:\n",
        "                    x_shifted = df[x].shift(-lag)\n",
        "                    y_shifted = df[y]\n",
        "                else:\n",
        "                    x_shifted = df[x]\n",
        "                    y_shifted = df[y].shift(lag)\n",
        "\n",
        "                pair = pd.concat([x_shifted, y_shifted], axis=1).dropna()\n",
        "\n",
        "                if len(pair) < min_obs:\n",
        "                    continue\n",
        "\n",
        "                corr = pair.iloc[:, 0].corr(pair.iloc[:, 1], method=method)\n",
        "\n",
        "                if pd.isna(corr):\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"x\": x,\n",
        "                    \"y\": y,\n",
        "                    \"lag\": lag,\n",
        "                    \"correlation\": corr,\n",
        "                    \"abs_corr\": abs(corr),\n",
        "                    \"direction\": \"inverse\" if corr < 0 else \"same\",\n",
        "                    \"n_obs\": len(pair)\n",
        "                }\n",
        "\n",
        "                if best_row is None or row[\"abs_corr\"] > best_row[\"abs_corr\"]:\n",
        "                    best_row = row\n",
        "\n",
        "            if best_row is not None:\n",
        "                results.append(best_row)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df = dataset_ts\n",
        "cac40_cols = pd.read_csv(\"CAC40_TICKERS.csv\")['Ticker'].tolist()\n",
        "other_cols = [c for c in df.columns if c not in cac40_cols]\n",
        "\n",
        "results = scan_lead_lag_cac40(\n",
        "    df=df,\n",
        "    cac40_cols=cac40_cols,\n",
        "    other_cols=other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40\n",
        ")\n",
        "\n",
        "results.sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "results[\n",
        "    results[\"direction\"] == \"inverse\"\n",
        "].sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "\n",
        "results[results[\"y\"] == \"MC.PA\"] \\\n",
        "    .sort_values(\"abs_corr\", ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(\"Shape:\", results.shape)\n",
        "results.to_csv(\"LEAD_LAG_SCAN_CAC40.csv\", index=False)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des leads/lags sur les actions du CAC40 terminÃ©e: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Candidates selection"
      ],
      "metadata": {
        "id": "5rIc4-G4NG_T"
      },
      "id": "5rIc4-G4NG_T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
      },
      "outputs": [],
      "source": [
        "### Recherche des variables candidates qui ont des leads/lags significatifs sur le CAC40\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU CSV\n",
        "# =========================\n",
        "\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "print(\"Dataset initial :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 2. FILTRE : LAG â‰  0\n",
        "# =========================\n",
        "\n",
        "leadlag = leadlag[leadlag[\"lag\"] != 0]\n",
        "\n",
        "print(\"AprÃ¨s suppression lag = 0 :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 3. CLASSIFICATION DU RISQUE\n",
        "# =========================\n",
        "\n",
        "def classify_risk(abs_corr, n_obs):\n",
        "    if abs_corr >= 0.55 and n_obs >= 80:\n",
        "        return \"very_low_risk\"\n",
        "    elif abs_corr >= 0.45 and n_obs >= 60:\n",
        "        return \"low_risk\"\n",
        "    elif abs_corr >= 0.40 and n_obs >= 50:\n",
        "        return \"medium_risk\"\n",
        "    elif abs_corr >= 0.30 and n_obs >= 40:\n",
        "        return \"high_risk\"\n",
        "    else:\n",
        "        return \"very_high_risk\"\n",
        "\n",
        "leadlag[\"risk_level\"] = leadlag.apply(\n",
        "    lambda r: classify_risk(r[\"abs_corr\"], r[\"n_obs\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. TRI FINAL (DU + SÃ›R AU + RISQUÃ‰)\n",
        "# =========================\n",
        "\n",
        "risk_order = [\n",
        "    \"very_low_risk\",\n",
        "    \"low_risk\",\n",
        "    \"medium_risk\",\n",
        "    \"high_risk\",\n",
        "    \"very_high_risk\"\n",
        "]\n",
        "\n",
        "leadlag[\"risk_level\"] = pd.Categorical(\n",
        "    leadlag[\"risk_level\"],\n",
        "    categories=risk_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "leadlag_sorted = leadlag.sort_values(\n",
        "    [\"risk_level\", \"abs_corr\"],\n",
        "    ascending=[True, False]\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. CONTRÃ”LE RAPIDE\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 relations les plus sÃ»res (lag â‰  0) :\")\n",
        "print(leadlag_sorted.head(20))\n",
        "\n",
        "# =========================\n",
        "# 6. SAUVEGARDE\n",
        "# =========================\n",
        "\n",
        "leadlag_sorted.to_csv(\n",
        "    \"LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nCSV sauvegardÃ© : LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\")\n",
        "\n",
        "# Charger le scan lead/lag\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "# Filtre SAFE\n",
        "filtered = leadlag[\n",
        "    (leadlag[\"lag\"] != 0) &\n",
        "    (leadlag[\"abs_corr\"] >= 0.25) &\n",
        "    (leadlag[\"n_obs\"] >= 40)\n",
        "]\n",
        "\n",
        "# Extraire les candidats (variables X)\n",
        "candidatesX = (\n",
        "    filtered[\"x\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesX.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate variables\")\n",
        "print(candidatesX)\n",
        "pd.DataFrame(candidatesX, columns=[\"variable\"]).to_csv(\"CANDIDATES_VARIABLES.csv\", index=False)\n",
        "\n",
        "# Extraire les cibles (variables y)\n",
        "candidatesY = (\n",
        "    filtered[\"y\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesY.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate taget\")\n",
        "print(candidatesY)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des variables candidates CANDIDATES_VARIABLES.csv terminÃ©e: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ],
      "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___CS.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "Uij5aRPaK47h"
      },
      "id": "Uij5aRPaK47h"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"^NDX\", \"LIN\", \"ABBV\", \"CS.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"^NDX\", \"LIN\", \"ABBV\"]]\n",
        "y = df[\"CS.PA\"].shift(-1)  # URW.PA Ã  J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"^NDX\", \"LIN\", \"ABBV\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraÃ®ner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"^NDX\", \"LIN\", \"ABBV\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"^NDX\", \"LIN\", \"ABBV\"]]],\n",
        "        columns=[\"^NDX\", \"LIN\", \"ABBV\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÃ‰DICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"CS.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"CS.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÃ‰SULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# CrÃ©ation du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"CS.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"CS.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# VÃ©rification\n",
        "print(final_dataset[[\n",
        "    \"CS.PA\",\n",
        "    \"CS.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"CS_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "HqrQ5wv0WwYg"
      },
      "id": "HqrQ5wv0WwYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___URW.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "zyY4sc6rMDr0"
      },
      "id": "zyY4sc6rMDr0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"GC=F\", \"^NDX\", \"AAPL\", \"URW.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"GC=F\", \"^NDX\", \"AAPL\"]]\n",
        "y = df[\"URW.PA\"].shift(-1)  # URW.PA Ã  J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"GC=F\", \"^NDX\", \"AAPL\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraÃ®ner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"GC=F\", \"^NDX\", \"AAPL\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"GC=F\", \"^NDX\", \"AAPL\"]]],\n",
        "        columns=[\"GC=F\", \"^NDX\", \"AAPL\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÃ‰DICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"URW.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"URW.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÃ‰SULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# CrÃ©ation du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"URW.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"URW.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# VÃ©rification\n",
        "print(final_dataset[[\n",
        "    \"URW.PA\",\n",
        "    \"URW.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"URW_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "khd_HR51LwOG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "khd_HR51LwOG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect saxo bank"
      ],
      "metadata": {
        "id": "R_gn7zKPNkJh"
      },
      "id": "R_gn7zKPNkJh"
    },
    {
      "cell_type": "code",
      "source": [
        "# test conncetion Saxo bank simulation\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "\n",
        "ACCESS_TOKEN = \"eyJhbGciOiJFUzI1NiIsIng1dCI6IjY3NEM0MjFEMzZEMUE1OUNFNjFBRTIzMjMyOTVFRTAyRTc3MDMzNTkifQ.eyJvYWEiOiI3Nzc3NSIsImlzcyI6Im9hIiwiYWlkIjoiMTA5IiwidWlkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiY2lkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiaXNhIjoiRmFsc2UiLCJ0aWQiOiIyMDAyIiwic2lkIjoiMzYwNGFlY2NmYTc3NGNlNmEzNGZmNDIyNGUyZDBmNWEiLCJkZ2kiOiI4NCIsImV4cCI6IjE3Njg4MTEzMjgiLCJvYWwiOiIxRiIsImlpZCI6IjFkNzAzNjg4NzM4MTQzNjMwNGE1MDhkZTRjNTUxOTUwIn0.NuSPihbRVQ_YHoWnW-sQHESQ9QNBExUgGpEM-GaD7gWOQfhr2OU7D4qmgGjhLEW7FNDKsdkL0bRfwf5xuOzftg\"\n",
        "BASE_URL = \"https://gateway.saxobank.com/sim/openapi\"  # ou /sim/openapi\n",
        "SYMBOL = {\"URW.PA\" : \"19099381\"}\n",
        "EXCHANGE = \"Euronext Paris\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "    \"Accept\" : \"*/*\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def get_AccountKeys():\n",
        "    url = f\"{BASE_URL}/port/v1/accounts/me\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"Data\"]\n",
        "\n",
        "def get_Balances(AccountKey):\n",
        "    url = f\"{BASE_URL}/port/v1/balances?AccountKey=\"+AccountKey+\"&ClientKey=\"+AccountKey\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def get_TickerPosition(Uic):\n",
        "    url = f\"{BASE_URL}/trade/v1/infoprices/list?AccountKey=\"+AccountKey+\"&Uics=\"+Uic+\"&AssetType=Stock&Amount=100000&FieldGroups=DisplayAndFormat,Quote\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "AccountKeys = get_AccountKeys()\n",
        "AccountKey = AccountKeys[0][\"AccountKey\"]\n",
        "Balances = get_Balances(AccountKey)\n",
        "URW = get_TickerPosition(SYMBOL[\"URW.PA\"])\n",
        "print(URW)\n",
        "\n",
        "print(AccountKey)\n",
        "print(Balances)"
      ],
      "metadata": {
        "id": "SWGNJbYASMC1"
      },
      "id": "SWGNJbYASMC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today prediction"
      ],
      "metadata": {
        "id": "HDaUtak-Lj_h"
      },
      "id": "HDaUtak-Lj_h"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CS.PA â€“ SIGNAL QUANT +\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# PARAMÃˆTRES GÃ‰NÃ‰RAUX\n",
        "# =========================\n",
        "\n",
        "CSV_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "FEATURES = [\"^NDX\", \"LIN\",\"ABBV\"]\n",
        "TARGET   = \"CS.PA\"\n",
        "\n",
        "BUY_THRESHOLD  = 0.5\n",
        "SELL_THRESHOLD = -0.5\n",
        "\n",
        "MIN_TRAIN_SIZE = 60\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGER DATASET HISTORIQUE\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "df = df[FEATURES + [TARGET]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. ENTRAÃŽNEMENT DU MODÃˆLE (lag = 1)\n",
        "# =========================\n",
        "\n",
        "X = df[FEATURES]\n",
        "y = df[TARGET].shift(-1)\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = FEATURES + [\"CS_TARGET\"]\n",
        "\n",
        "X_train = sm.add_constant(data[FEATURES], has_constant=\"add\")\n",
        "y_train = data[\"CS_TARGET\"]\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# =========================\n",
        "# 3. DONNÃ‰ES LIVE (clÃ´ture US)\n",
        "# =========================\n",
        "\n",
        "tickers = FEATURES + [TARGET]\n",
        "\n",
        "prices = yf.download(\n",
        "    tickers,\n",
        "    period=\"2d\",\n",
        "    interval=\"1d\",\n",
        "    auto_adjust=True,\n",
        "    progress=False\n",
        ")[\"Close\"]\n",
        "\n",
        "returns_today = prices.pct_change(fill_method=None).iloc[-1] * 100\n",
        "\n",
        "missing = returns_today[FEATURES].isna()\n",
        "\n",
        "if missing.any():\n",
        "    print(\"âš ï¸ MarchÃ©s non clÃ´turÃ©s :\", missing[missing].index.tolist())\n",
        "    predicted_return = 0.0\n",
        "    signal = \"KEEP\"\n",
        "\n",
        "else:\n",
        "    X_today = pd.DataFrame(\n",
        "        [[\n",
        "            returns_today[\"^NDX\"],\n",
        "            returns_today[\"LIN\"],\n",
        "            returns_today[\"ABBV\"]\n",
        "        ]],\n",
        "        columns=FEATURES\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    predicted_return = model.predict(X_today).iloc[0]\n",
        "\n",
        "    if predicted_return > BUY_THRESHOLD:\n",
        "        signal = \"BUY\"\n",
        "    elif predicted_return < SELL_THRESHOLD:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"KEEP\"\n",
        "\n",
        "# =========================\n",
        "# 6. SORTIE\n",
        "# =========================\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"   SIGNAL URW.PA (J+1)\")\n",
        "print(\"==============================\")\n",
        "print(\"Date :\", datetime.now().strftime(\"%Y-%m-%d %H:%M CET\"))\n",
        "print(\"\\nVariations aujourd'hui (%)\")\n",
        "print(returns_today[FEATURES])\n",
        "print(f\"\\nPrÃ©vision URW.PA J+1 : {predicted_return:.3f} %\")\n",
        "print(\"==============================\\n\")"
      ],
      "metadata": {
        "id": "9RikQhCXSRQb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9RikQhCXSRQb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}