{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
        "outputId": "8f8e3e73-5310-4135-ada5-481b2440b236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  131 of 131 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (1000, 132)\n",
            "Creation terminee : 2026-02-07 19:58:00.270649\n"
          ]
        }
      ],
      "source": [
        "### Creation du dataset de variation des tickers des 1000 derniers jours\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# D√©finir la p√©riode de 1000 jours\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# Liste des tickers du CAC 40 (exemple : 'OR.PA' pour L'Or√©al)\n",
        "cac40_tickers = cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # Cr√©dit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault Syst√®mes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # Herm√®s\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'Or√©al\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # Soci√©t√© G√©n√©rale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# Liste des tickers du NASDAQ (exemple : 'AAPL' pour Apple)\n",
        "nasdaq_tickers = nasdaq_top_40 = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices (AMD)\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # Mondelƒìz\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\", # ASML Holding\n",
        "    \"MRNA\", # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "nyse_tickers = nyse_top_40 = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"CSCO\",  # Cisco (aussi cot√© sur NYSE)\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\",    # Altria\n",
        "]\n",
        "\n",
        "# Inclure √©galement les indices macro (VIX, Dollar Index, Brent, Or)\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold Futures COMEX)\n",
        "    \"BZ=F\",        # P√©trole Brent (Brent Crude Oil Futures)\n",
        "    \"^NDX\",        # NASDAQ 100 (technologie US, croissance)\n",
        "    \"^DJI\",        # Dow Jones Industrial Average (blue chips US)\n",
        "    \"^SP500-20\",   # S&P 500 Industrials (secteur industriel)\n",
        "    \"^SP500-15\",   # S&P 500 Materials (mati√®res premi√®res)\n",
        "    \"^STOXX50E\",   # Euro STOXX 50 (grandes capitalisations zone euro)\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY ‚Äì force du dollar US)\n",
        "    \"EUR=X\",       # Taux de change EUR/USD\n",
        "    \"CHF=X\",       # Taux de change USD/CHF (valeur refuge)\n",
        "    \"^VIX\",        # Indice de volatilit√© (peur / stress de march√©)\n",
        "    \"^IRX\",        # Taux US 13 semaines (T-Bills court terme)\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans (benchmark macro mondial)\n",
        "    \"^TYX\",        # Taux US 30 ans (long terme)\n",
        "    \"^CIISCSEP\",   # Indice de surprises √©conomiques Citi (US)\n",
        "    \"CDX\"          # Indice de cr√©dit (Credit Default Swaps ‚Äì stress cr√©dit)\n",
        "    \"BTC-USD\"      # Bitcoin\n",
        "]\n",
        "\n",
        "\n",
        "# Combiner toutes les listes de tickers\n",
        "all_tickers = cac40_tickers + nasdaq_tickers + nyse_tickers + macro_tickers\n",
        "\n",
        "# T√©l√©charger les donn√©es boursi√®res\n",
        "data = yf.download(all_tickers, start=start_date, end=end_date,auto_adjust=True)\n",
        "\n",
        "# Calculer la variation en pourcentage par rapport √† la cl√¥ture pr√©c√©dente\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "\n",
        "dataset_ts = variation.reset_index()\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "print(\"Shape:\", dataset_ts.shape)\n",
        "print(dataset_ts.head())\n",
        "print(dataset_ts.tail())\n",
        "print(\"NaN par colonne :\")\n",
        "print(variation.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "pd.DataFrame(cac40_tickers, columns=[\"Ticker\"]).to_csv(\"CAC40_TICKERS.csv\", index=False)\n",
        "dataset_ts.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv termin√©e: \")\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag 1-3 jours"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donn√©es\n",
        "# =========================\n",
        "\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discr√©tisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Lift conditionnel\n",
        "# =========================\n",
        "\n",
        "def conditional_lift(disc, target, l1, l2, lag, target_state):\n",
        "    shifted = disc.shift(lag)\n",
        "    valid = shifted[[l1, l2, target]].dropna()\n",
        "\n",
        "    base_rate = (valid[target] == target_state).mean()\n",
        "    if base_rate == 0:\n",
        "        return 0\n",
        "\n",
        "    # Lift avec l1 seul\n",
        "    mask_l1 = valid[l1] == \"UP\"\n",
        "    if mask_l1.sum() == 0:\n",
        "        return 0\n",
        "\n",
        "    lift_l1 = (\n",
        "        (valid.loc[mask_l1, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    # Lift avec l1 et l2\n",
        "    mask_l1_l2 = (\n",
        "        (valid[l1] == \"UP\") &\n",
        "        (valid[l2] == \"DOWN\")\n",
        "    )\n",
        "    if mask_l1_l2.sum() == 0:\n",
        "        return 0\n",
        "\n",
        "    lift_l1_l2 = (\n",
        "        (valid.loc[mask_l1_l2, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    return lift_l1_l2 - lift_l1\n",
        "\n",
        "# =========================\n",
        "# 4. Mining des r√®gles (UP + DOWN)\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05,\n",
        "    min_base_rate=0.05,\n",
        "    max_corr=0.85\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            future = disc[target]\n",
        "            leaders_disc = disc.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_disc.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_disc = leaders_disc.loc[valid_idx]\n",
        "\n",
        "            for target_state in [\"UP\", \"DOWN\"]:\n",
        "\n",
        "                base_rate = (future == target_state).mean()\n",
        "                if base_rate < min_base_rate:\n",
        "                    continue\n",
        "\n",
        "                leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "                for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                    # ---- filtre corr√©lation brute\n",
        "                    corr = abs(df[l1].corr(df[l2]))\n",
        "                    if corr > max_corr:\n",
        "                        continue\n",
        "\n",
        "                    # ---- condition logique (leaders)\n",
        "                    mask = (\n",
        "                        (leaders_disc[l1] == \"UP\") &\n",
        "                        (leaders_disc[l2] == \"DOWN\")\n",
        "                    )\n",
        "\n",
        "                    support = mask.mean()\n",
        "                    if support < min_support:\n",
        "                        continue\n",
        "\n",
        "                    confidence = (future[mask] == target_state).mean()\n",
        "                    if confidence < min_confidence:\n",
        "                        continue\n",
        "\n",
        "                    lift = confidence / base_rate\n",
        "                    if lift < min_lift:\n",
        "                        continue\n",
        "\n",
        "                    # ---- lift conditionnel\n",
        "                    delta_lift = conditional_lift(\n",
        "                        disc, target, l1, l2, lag, target_state\n",
        "                    )\n",
        "                    if delta_lift < min_conditional_lift:\n",
        "                        continue\n",
        "\n",
        "                    rules.append({\n",
        "                        \"target\": target,\n",
        "                        \"target_direction\": target_state,\n",
        "                        \"leaders\": f\"{l1} UP AND {l2} DOWN\",\n",
        "                        \"lag_days\": lag,\n",
        "                        \"support\": round(support, 3),\n",
        "                        \"confidence\": round(confidence, 3),\n",
        "                        \"lift\": round(lift, 2),\n",
        "                        \"delta_lift\": round(delta_lift, 2),\n",
        "                        \"corr_leaders\": round(corr, 2),\n",
        "                        \"occurrences\": int(mask.sum())\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 5. Ex√©cution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"target_direction\", \"delta_lift\", \"lift\", \"confidence\"],\n",
        "    ascending=[True, False, False, False]\n",
        ")\n",
        "\n",
        "print(\"\\nTop 20 r√®gles (UP + DOWN) :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "rules.to_csv(\"lead_lag_rules_results_up_down.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "vf5ybat9VqvF"
      },
      "id": "vf5ybat9VqvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QWrMSlWqF-tl"
      },
      "id": "QWrMSlWqF-tl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R√®gles vers features ML Section"
      ],
      "metadata": {
        "id": "5AcBjr6xB4TW"
      },
      "id": "5AcBjr6xB4TW"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donn√©es\n",
        "# =========================\n",
        "\n",
        "DATA_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "RULES_PATH = \"lead_lag_rules_results.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "dates = df[\"date\"] if \"date\" in df.columns else None\n",
        "df = df.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "rules_df = pd.read_csv(RULES_PATH)\n",
        "\n",
        "# =========================\n",
        "# 2. Discr√©tisation (identique √† la d√©tection)\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Parsing des r√®gles\n",
        "# =========================\n",
        "\n",
        "def parse_leaders(leaders_str):\n",
        "    \"\"\"\n",
        "    Ex: 'TEP.PAS ‚Üë AND ^NDX ‚Üì'\n",
        "    \"\"\"\n",
        "    l1, l2 = leaders_str.split(\" AND \")\n",
        "\n",
        "    def parse_one(s):\n",
        "        name, arrow = s.split(\" \")\n",
        "        direction = \"UP\" if \"‚Üë\" in arrow else \"DOWN\"\n",
        "        return name, direction\n",
        "\n",
        "    return parse_one(l1), parse_one(l2)\n",
        "\n",
        "# =========================\n",
        "# 4. G√©n√©ration des features binaires\n",
        "# =========================\n",
        "\n",
        "feature_cols = {}\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "\n",
        "    (l1_name, l1_dir), (l2_name, l2_dir) = parse_leaders(rule[\"leaders\"])\n",
        "    lag = int(rule[\"lag_days\"])\n",
        "\n",
        "    cond = (\n",
        "        (disc[l1_name].shift(lag) == l1_dir) &\n",
        "        (disc[l2_name].shift(lag) == l2_dir)\n",
        "    )\n",
        "\n",
        "    feature_cols[f\"rule_{idx}\"] = cond.astype(int)\n",
        "\n",
        "# Construction finale en UNE fois\n",
        "features = pd.DataFrame(feature_cols, index=disc.index)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. Pond√©ration des r√®gles\n",
        "# =========================\n",
        "\n",
        "weighted_features = features.copy()\n",
        "\n",
        "for idx, rule in rules_df.iterrows():\n",
        "    weight = rule[\"lift\"] * rule[\"confidence\"]\n",
        "    weighted_features[f\"rule_{idx}\"] *= weight\n",
        "\n",
        "# =========================\n",
        "# 6. Agr√©gation (features robustes)\n",
        "# =========================\n",
        "\n",
        "agg_features = pd.DataFrame(index=features.index)\n",
        "\n",
        "agg_features[\"rules_count\"] = (features > 0).sum(axis=1)\n",
        "agg_features[\"rules_strength\"] = weighted_features.sum(axis=1)\n",
        "\n",
        "# =========================\n",
        "# 7. Dataset ML final\n",
        "# =========================\n",
        "\n",
        "X = pd.concat(\n",
        "    [\n",
        "        agg_features,\n",
        "        weighted_features\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Nettoyage des NaN dus aux lags\n",
        "X = X.dropna()\n",
        "\n",
        "# =========================\n",
        "# 8. Exemple de target ML\n",
        "# =========================\n",
        "\n",
        "TARGET = \"NVDA\"  # √† adapter\n",
        "\n",
        "y = (df[TARGET] > 1.0).astype(int)\n",
        "y = y.loc[X.index]\n",
        "\n",
        "# =========================\n",
        "# 9. Export final\n",
        "# =========================\n",
        "\n",
        "if dates is not None:\n",
        "    X.insert(0, \"date\", dates.loc[X.index])\n",
        "\n",
        "X.to_csv(\"ml_features_from_rules.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Dataset ML g√©n√©r√©\")\n",
        "print(\"Shape X :\", X.shape)\n",
        "print(\"Target positives :\", y.mean())\n"
      ],
      "metadata": {
        "id": "lwy4zRekB20J"
      },
      "id": "lwy4zRekB20J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature selection par variable"
      ],
      "metadata": {
        "id": "OHG0YnIhPrtO"
      },
      "id": "OHG0YnIhPrtO"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# =========================================================\n",
        "# 1. LOAD DATA\n",
        "# =========================================================\n",
        "\n",
        "DATA_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "RULES_PATH = \"ml_features_from_rules.csv\"\n",
        "\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "rules = pd.read_csv(RULES_PATH)\n",
        "\n",
        "# Remove overlapping columns (date, etc.)\n",
        "overlap_cols = set(data.columns).intersection(set(rules.columns))\n",
        "rules = rules.drop(columns=list(overlap_cols))\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.concat([data, rules], axis=1)\n",
        "\n",
        "# Identify raw variables (targets)\n",
        "raw_variables = [c for c in data.columns if c.lower() != \"date\"]\n",
        "\n",
        "# =========================================================\n",
        "# 2. FEATURE SELECTION FUNCTION (PER VARIABLE)\n",
        "# =========================================================\n",
        "\n",
        "def feature_selection_for_variable(\n",
        "    df,\n",
        "    target,\n",
        "    raw_variables,\n",
        "    n_splits=3,\n",
        "    n_repeats=10,\n",
        "    min_importance=0.002\n",
        "):\n",
        "\n",
        "    # Target direction at T+1\n",
        "    y = (df[target].shift(-1) > 0).astype(int).dropna()\n",
        "\n",
        "    # üö® Guard: only one class ‚Üí no model possible\n",
        "    if y.nunique() < 2:\n",
        "        print(f\"‚ö†Ô∏è Skipped {target}: only one class in target\")\n",
        "        return pd.Series(dtype=float), [], np.nan\n",
        "\n",
        "    # Features at T (rules only, numeric only)\n",
        "    X = df.loc[y.index].drop(columns=raw_variables, errors=\"ignore\")\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "    # ‚úÖ CRITICAL FIX: NaN = rule inactive\n",
        "    X = X.fillna(0)\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    importances_all = []\n",
        "    acc_scores = []\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Accuracy\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc_scores.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "        # Permutation importance\n",
        "        perm = permutation_importance(\n",
        "            model,\n",
        "            X_test,\n",
        "            y_test,\n",
        "            n_repeats=n_repeats,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        importances_all.append(perm.importances_mean)\n",
        "\n",
        "    mean_importance = np.mean(importances_all, axis=0)\n",
        "\n",
        "    importance_series = pd.Series(\n",
        "        mean_importance,\n",
        "        index=X.columns\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "    selected_features = importance_series[\n",
        "        importance_series > min_importance\n",
        "    ].index.tolist()\n",
        "\n",
        "    return importance_series, selected_features, round(np.mean(acc_scores) * 100, 2)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. RUN FEATURE SELECTION FOR ALL VARIABLES\n",
        "# =========================================================\n",
        "\n",
        "all_results = {}\n",
        "selected_features_dict = {}\n",
        "accuracy_summary = {}\n",
        "\n",
        "# Limit for demonstration to avoid long execution, or run for all\n",
        "for target in raw_variables:\n",
        "    print(f\"\\n===== Feature selection for: {target} =====\")\n",
        "\n",
        "    importance, selected, acc = feature_selection_for_variable(\n",
        "        df,\n",
        "        target,\n",
        "        raw_variables\n",
        "    )\n",
        "\n",
        "    all_results[target] = importance\n",
        "    selected_features_dict[target] = selected\n",
        "    accuracy_summary[target] = acc\n",
        "\n",
        "    print(f\"Accuracy (%): {acc}\")\n",
        "    print(f\"Selected features ({len(selected)}):\")\n",
        "    print(selected[:10])  # show first 10 only\n",
        "\n",
        "# =========================================================\n",
        "# 4. SAVE RESULTS\n",
        "# =========================================================\n",
        "\n",
        "# Save selected rules per variable\n",
        "pd.Series(selected_features_dict).to_json(\n",
        "    \"selected_features_per_variable.json\",\n",
        "    indent=2\n",
        ")\n",
        "\n",
        "# Save accuracy summary\n",
        "pd.Series(accuracy_summary).sort_values(ascending=False).to_csv(\n",
        "    \"accuracy_per_variable.csv\"\n",
        ")\n",
        "\n",
        "# Optional: save full importance table\n",
        "importance_df = pd.DataFrame(all_results).fillna(0)\n",
        "importance_df.to_csv(\"feature_importance_per_variable.csv\")\n",
        "\n",
        "print(\"\\n=== DONE ===\")\n",
        "print(\"Files generated:\")\n",
        "print(\"- selected_features_per_variable.json\")\n",
        "print(\"- accuracy_per_variable.csv\")\n",
        "print(\"- feature_importance_per_variable.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NiLe0plPzs0",
        "outputId": "ed0e6260-fbf9-4e62-d46c-58a085915083"
      },
      "id": "8NiLe0plPzs0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Feature selection for: AAPL =====\n",
            "Accuracy (%): 51.6\n",
            "Selected features (96):\n",
            "['rule_126', 'rule_163', 'rule_449', 'rule_595', 'rule_480', 'rule_388', 'rule_563', 'rule_560', 'rule_389', 'rule_215']\n",
            "\n",
            "===== Feature selection for: ABBV =====\n",
            "Accuracy (%): 49.91\n",
            "Selected features (173):\n",
            "['rules_strength', 'rule_53', 'rule_560', 'rule_587', 'rule_299', 'rule_427', 'rule_302', 'rule_316', 'rule_173', 'rule_287']\n",
            "\n",
            "===== Feature selection for: ABT =====\n",
            "Accuracy (%): 47.83\n",
            "Selected features (73):\n",
            "['rule_287', 'rule_325', 'rule_239', 'rule_311', 'rule_328', 'rule_249', 'rule_418', 'rule_80', 'rule_202', 'rule_129']\n",
            "\n",
            "===== Feature selection for: ACA.PA =====\n",
            "Accuracy (%): 51.6\n",
            "Selected features (107):\n",
            "['rules_strength', 'rules_count', 'rule_526', 'rule_568', 'rule_563', 'rule_235', 'rule_208', 'rule_174', 'rule_389', 'rule_570']\n",
            "\n",
            "===== Feature selection for: ACN =====\n",
            "Accuracy (%): 47.65\n",
            "Selected features (76):\n",
            "['rule_550', 'rule_418', 'rule_51', 'rule_402', 'rule_425', 'rule_198', 'rule_432', 'rule_38', 'rule_588', 'rule_423']\n",
            "\n",
            "===== Feature selection for: ADBE =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (179):\n",
            "['rules_count', 'rule_610', 'rule_557', 'rule_665', 'rule_540', 'rule_22', 'rule_384', 'rule_591', 'rule_514', 'rule_61']\n",
            "\n",
            "===== Feature selection for: ADI =====\n",
            "Accuracy (%): 47.27\n",
            "Selected features (73):\n",
            "['rule_595', 'rule_605', 'rule_209', 'rule_160', 'rule_257', 'rule_563', 'rule_28', 'rule_251', 'rule_506', 'rule_587']\n",
            "\n",
            "===== Feature selection for: AI.PA =====\n",
            "Accuracy (%): 46.52\n",
            "Selected features (75):\n",
            "['rules_strength', 'rule_209', 'rule_287', 'rule_337', 'rule_64', 'rule_645', 'rule_357', 'rule_144', 'rule_584', 'rule_581']\n",
            "\n",
            "===== Feature selection for: AIR.PA =====\n",
            "Accuracy (%): 50.09\n",
            "Selected features (112):\n",
            "['rules_count', 'rule_486', 'rule_666', 'rule_61', 'rule_15', 'rule_145', 'rule_13', 'rule_167', 'rule_207', 'rule_196']\n",
            "\n",
            "===== Feature selection for: ALO.PA =====\n",
            "Accuracy (%): 49.91\n",
            "Selected features (94):\n",
            "['rules_strength', 'rule_361', 'rule_336', 'rule_602', 'rule_101', 'rule_333', 'rule_20', 'rule_515', 'rule_376', 'rule_596']\n",
            "\n",
            "===== Feature selection for: AMAT =====\n",
            "Accuracy (%): 48.4\n",
            "Selected features (34):\n",
            "['rule_262', 'rule_615', 'rule_149', 'rule_670', 'rule_206', 'rule_595', 'rule_230', 'rule_446', 'rule_120', 'rule_272']\n",
            "\n",
            "===== Feature selection for: AMD =====\n",
            "Accuracy (%): 52.73\n",
            "Selected features (141):\n",
            "['rules_strength', 'rule_664', 'rule_368', 'rule_238', 'rule_210', 'rule_455', 'rule_13', 'rule_500', 'rule_228', 'rule_76']\n",
            "\n",
            "===== Feature selection for: AMGN =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (45):\n",
            "['rule_290', 'rules_strength', 'rule_58', 'rule_41', 'rule_444', 'rule_198', 'rule_472', 'rule_90', 'rule_230', 'rule_596']\n",
            "\n",
            "===== Feature selection for: AMZN =====\n",
            "Accuracy (%): 51.79\n",
            "Selected features (151):\n",
            "['rules_strength', 'rule_220', 'rule_18', 'rule_592', 'rule_411', 'rule_542', 'rule_374', 'rule_538', 'rule_623', 'rule_469']\n",
            "\n",
            "===== Feature selection for: ASML =====\n",
            "Accuracy (%): 54.24\n",
            "Selected features (135):\n",
            "['rules_strength', 'rules_count', 'rule_542', 'rule_29', 'rule_163', 'rule_120', 'rule_371', 'rule_40', 'rule_118', 'rule_189']\n",
            "\n",
            "===== Feature selection for: AVGO =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (64):\n",
            "['rule_563', 'rule_102', 'rule_53', 'rule_367', 'rule_40', 'rule_196', 'rules_count', 'rule_151', 'rule_83', 'rule_544']\n",
            "\n",
            "===== Feature selection for: BAC =====\n",
            "Accuracy (%): 50.09\n",
            "Selected features (66):\n",
            "['rules_strength', 'rule_655', 'rule_264', 'rule_16', 'rule_228', 'rule_302', 'rule_553', 'rule_435', 'rule_75', 'rule_560']\n",
            "\n",
            "===== Feature selection for: BKNG =====\n",
            "Accuracy (%): 51.04\n",
            "Selected features (133):\n",
            "['rules_strength', 'rule_489', 'rule_557', 'rule_658', 'rule_196', 'rule_336', 'rule_665', 'rule_576', 'rule_364', 'rule_610']\n",
            "\n",
            "===== Feature selection for: BN.PA =====\n",
            "Accuracy (%): 48.21\n",
            "Selected features (61):\n",
            "['rules_strength', 'rule_305', 'rule_489', 'rule_458', 'rule_553', 'rule_264', 'rule_526', 'rule_253', 'rule_308', 'rule_557']\n",
            "\n",
            "===== Feature selection for: BNP.PA =====\n",
            "Accuracy (%): 51.79\n",
            "Selected features (113):\n",
            "['rules_count', 'rules_strength', 'rule_477', 'rule_598', 'rule_179', 'rule_5', 'rule_256', 'rule_636', 'rule_522', 'rule_184']\n",
            "\n",
            "===== Feature selection for: BZ=F =====\n",
            "Accuracy (%): 54.99\n",
            "Selected features (116):\n",
            "['rules_strength', 'rules_count', 'rule_75', 'rule_438', 'rule_18', 'rule_569', 'rule_336', 'rule_357', 'rule_454', 'rule_198']\n",
            "\n",
            "===== Feature selection for: C =====\n",
            "Accuracy (%): 52.73\n",
            "Selected features (100):\n",
            "['rules_strength', 'rule_305', 'rule_560', 'rules_count', 'rule_232', 'rule_302', 'rule_22', 'rule_499', 'rule_402', 'rule_123']\n",
            "\n",
            "===== Feature selection for: CA.PA =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (55):\n",
            "['rule_3', 'rule_31', 'rule_399', 'rule_601', 'rule_268', 'rule_634', 'rule_561', 'rule_575', 'rule_416', 'rule_459']\n",
            "\n",
            "===== Feature selection for: CAP.PA =====\n",
            "Accuracy (%): 51.79\n",
            "Selected features (150):\n",
            "['rule_334', 'rule_633', 'rules_count', 'rule_215', 'rule_287', 'rule_78', 'rule_67', 'rule_617', 'rule_316', 'rule_357']\n",
            "\n",
            "===== Feature selection for: CAT =====\n",
            "Accuracy (%): 53.11\n",
            "Selected features (96):\n",
            "['rules_count', 'rules_strength', 'rule_21', 'rule_249', 'rule_23', 'rule_282', 'rule_145', 'rule_541', 'rule_38', 'rule_302']\n",
            "\n",
            "===== Feature selection for: CDXBTC-USD =====\n",
            "‚ö†Ô∏è Skipped CDXBTC-USD: only one class in target\n",
            "Accuracy (%): nan\n",
            "Selected features (0):\n",
            "[]\n",
            "\n",
            "===== Feature selection for: CHF=X =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (84):\n",
            "['rule_292', 'rule_41', 'rule_276', 'rule_411', 'rule_155', 'rule_279', 'rule_145', 'rule_463', 'rule_642', 'rule_42']\n",
            "\n",
            "===== Feature selection for: CMCSA =====\n",
            "Accuracy (%): 52.54\n",
            "Selected features (67):\n",
            "['rules_strength', 'rules_count', 'rule_23', 'rule_44', 'rule_13', 'rule_297', 'rule_332', 'rule_174', 'rule_468', 'rule_1']\n",
            "\n",
            "===== Feature selection for: COST =====\n",
            "Accuracy (%): 51.79\n",
            "Selected features (154):\n",
            "['rules_strength', 'rule_160', 'rule_163', 'rule_371', 'rule_176', 'rule_47', 'rule_671', 'rule_30', 'rule_473', 'rule_214']\n",
            "\n",
            "===== Feature selection for: CRM =====\n",
            "Accuracy (%): 49.91\n",
            "Selected features (57):\n",
            "['rule_413', 'rule_175', 'rule_42', 'rule_6', 'rule_457', 'rule_211', 'rule_250', 'rule_36', 'rule_206', 'rule_196']\n",
            "\n",
            "===== Feature selection for: CS.PA =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (58):\n",
            "['rules_strength', 'rule_652', 'rule_467', 'rule_515', 'rule_421', 'rule_270', 'rule_218', 'rule_313', 'rule_316', 'rule_336']\n",
            "\n",
            "===== Feature selection for: CSCO =====\n",
            "Accuracy (%): 53.3\n",
            "Selected features (162):\n",
            "['rules_strength', 'rules_count', 'rule_158', 'rule_400', 'rule_126', 'rule_506', 'rule_408', 'rule_1', 'rule_19', 'rule_577']\n",
            "\n",
            "===== Feature selection for: CVS =====\n",
            "Accuracy (%): 51.22\n",
            "Selected features (58):\n",
            "['rule_15', 'rule_505', 'rule_236', 'rule_207', 'rule_70', 'rule_83', 'rule_94', 'rule_135', 'rule_269', 'rule_60']\n",
            "\n",
            "===== Feature selection for: CVX =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (86):\n",
            "['rules_strength', 'rule_590', 'rule_500', 'rule_119', 'rule_151', 'rule_156', 'rule_197', 'rule_41', 'rule_515', 'rule_53']\n",
            "\n",
            "===== Feature selection for: DG.PA =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (110):\n",
            "['rules_strength', 'rule_362', 'rule_305', 'rule_665', 'rule_481', 'rule_184', 'rule_145', 'rule_436', 'rule_282', 'rule_228']\n",
            "\n",
            "===== Feature selection for: DHR =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (83):\n",
            "['rules_count', 'rule_503', 'rule_104', 'rule_90', 'rule_431', 'rule_594', 'rule_592', 'rule_246', 'rule_57', 'rule_499']\n",
            "\n",
            "===== Feature selection for: DIS =====\n",
            "Accuracy (%): 49.91\n",
            "Selected features (44):\n",
            "['rule_562', 'rule_513', 'rule_476', 'rule_175', 'rule_301', 'rule_587', 'rule_319', 'rule_169', 'rule_15', 'rule_162']\n",
            "\n",
            "===== Feature selection for: DSY.PA =====\n",
            "Accuracy (%): 53.86\n",
            "Selected features (88):\n",
            "['rules_count', 'rule_86', 'rule_525', 'rule_28', 'rule_348', 'rule_559', 'rule_560', 'rule_5', 'rule_418', 'rule_472']\n",
            "\n",
            "===== Feature selection for: DX-Y.NYB =====\n",
            "Accuracy (%): 45.76\n",
            "Selected features (64):\n",
            "['rule_23', 'rule_175', 'rule_151', 'rule_378', 'rule_6', 'rule_405', 'rule_546', 'rule_250', 'rule_320', 'rule_98']\n",
            "\n",
            "===== Feature selection for: EDEN.PA =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (128):\n",
            "['rules_strength', 'rules_count', 'rule_144', 'rule_149', 'rule_400', 'rule_467', 'rule_375', 'rule_633', 'rule_18', 'rule_16']\n",
            "\n",
            "===== Feature selection for: EL.PA =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (90):\n",
            "['rules_count', 'rules_strength', 'rule_290', 'rule_179', 'rule_663', 'rule_563', 'rule_376', 'rule_632', 'rule_658', 'rule_144']\n",
            "\n",
            "===== Feature selection for: EN.PA =====\n",
            "Accuracy (%): 53.11\n",
            "Selected features (98):\n",
            "['rules_strength', 'rule_214', 'rule_296', 'rule_145', 'rule_453', 'rule_400', 'rule_333', 'rule_591', 'rule_588', 'rule_258']\n",
            "\n",
            "===== Feature selection for: ENGI.PA =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (111):\n",
            "['rules_strength', 'rule_598', 'rule_24', 'rule_562', 'rule_505', 'rule_52', 'rule_412', 'rule_411', 'rule_25', 'rule_657']\n",
            "\n",
            "===== Feature selection for: ERF.PA =====\n",
            "Accuracy (%): 49.53\n",
            "Selected features (57):\n",
            "['rule_1', 'rule_457', 'rules_strength', 'rule_306', 'rule_500', 'rule_592', 'rule_110', 'rule_487', 'rule_276', 'rule_637']\n",
            "\n",
            "===== Feature selection for: EUR=X =====\n",
            "Accuracy (%): 51.22\n",
            "Selected features (87):\n",
            "['rules_count', 'rules_strength', 'rule_30', 'rule_368', 'rule_429', 'rule_160', 'rule_53', 'rule_595', 'rule_421', 'rule_83']\n",
            "\n",
            "===== Feature selection for: FISV =====\n",
            "Accuracy (%): 48.21\n",
            "Selected features (48):\n",
            "['rule_302', 'rule_478', 'rule_493', 'rule_351', 'rule_541', 'rule_88', 'rule_489', 'rule_3', 'rule_597', 'rule_457']\n",
            "\n",
            "===== Feature selection for: GC=F =====\n",
            "Accuracy (%): 51.41\n",
            "Selected features (88):\n",
            "['rules_strength', 'rule_24', 'rule_226', 'rule_425', 'rule_325', 'rule_75', 'rule_158', 'rule_371', 'rule_417', 'rule_239']\n",
            "\n",
            "===== Feature selection for: GE =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (78):\n",
            "['rule_414', 'rule_40', 'rule_610', 'rule_142', 'rule_356', 'rule_570', 'rule_602', 'rule_16', 'rule_219', 'rule_225']\n",
            "\n",
            "===== Feature selection for: GLE.PA =====\n",
            "Accuracy (%): 49.15\n",
            "Selected features (50):\n",
            "['rule_228', 'rule_643', 'rule_572', 'rules_strength', 'rule_196', 'rule_637', 'rule_5', 'rule_1', 'rule_531', 'rule_218']\n",
            "\n",
            "===== Feature selection for: GOOGL =====\n",
            "Accuracy (%): 53.11\n",
            "Selected features (89):\n",
            "['rules_strength', 'rule_610', 'rule_357', 'rule_538', 'rule_411', 'rule_596', 'rule_490', 'rule_311', 'rule_88', 'rule_19']\n",
            "\n",
            "===== Feature selection for: GS =====\n",
            "Accuracy (%): 54.43\n",
            "Selected features (102):\n",
            "['rules_strength', 'rules_count', 'rule_12', 'rule_25', 'rule_515', 'rule_212', 'rule_313', 'rule_163', 'rule_418', 'rule_317']\n",
            "\n",
            "===== Feature selection for: HD =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (114):\n",
            "['rules_count', 'rule_76', 'rule_186', 'rule_461', 'rule_603', 'rule_266', 'rule_531', 'rule_199', 'rule_295', 'rule_382']\n",
            "\n",
            "===== Feature selection for: HO.PA =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (79):\n",
            "['rules_strength', 'rule_597', 'rule_412', 'rule_143', 'rule_292', 'rule_556', 'rule_307', 'rule_4', 'rule_40', 'rule_478']\n",
            "\n",
            "===== Feature selection for: HON =====\n",
            "Accuracy (%): 50.09\n",
            "Selected features (64):\n",
            "['rules_strength', 'rule_25', 'rule_195', 'rule_151', 'rule_167', 'rule_105', 'rule_253', 'rule_511', 'rule_93', 'rule_24']\n",
            "\n",
            "===== Feature selection for: IBM =====\n",
            "Accuracy (%): 54.8\n",
            "Selected features (181):\n",
            "['rules_count', 'rules_strength', 'rule_371', 'rule_259', 'rule_176', 'rule_47', 'rule_529', 'rule_145', 'rule_13', 'rule_473']\n",
            "\n",
            "===== Feature selection for: ILMN =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (97):\n",
            "['rule_178', 'rule_50', 'rule_163', 'rule_558', 'rule_62', 'rule_478', 'rule_51', 'rule_20', 'rule_653', 'rule_282']\n",
            "\n",
            "===== Feature selection for: INTC =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (82):\n",
            "['rule_456', 'rules_strength', 'rule_615', 'rule_645', 'rule_335', 'rule_262', 'rule_97', 'rule_200', 'rule_195', 'rule_251']\n",
            "\n",
            "===== Feature selection for: ISRG =====\n",
            "Accuracy (%): 51.41\n",
            "Selected features (146):\n",
            "['rules_strength', 'rule_405', 'rule_588', 'rule_282', 'rule_624', 'rule_48', 'rule_514', 'rule_120', 'rule_544', 'rule_604']\n",
            "\n",
            "===== Feature selection for: JNJ =====\n",
            "Accuracy (%): 46.52\n",
            "Selected features (82):\n",
            "['rule_3', 'rule_253', 'rule_68', 'rule_88', 'rule_454', 'rule_360', 'rule_478', 'rule_174', 'rule_543', 'rule_101']\n",
            "\n",
            "===== Feature selection for: KDP =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (127):\n",
            "['rules_strength', 'rule_1', 'rule_608', 'rule_202', 'rule_465', 'rule_410', 'rule_419', 'rule_83', 'rule_22', 'rule_208']\n",
            "\n",
            "===== Feature selection for: KER.PA =====\n",
            "Accuracy (%): 51.22\n",
            "Selected features (96):\n",
            "['rules_strength', 'rules_count', 'rule_575', 'rule_79', 'rule_7', 'rule_568', 'rule_39', 'rule_350', 'rule_10', 'rule_501']\n",
            "\n",
            "===== Feature selection for: KO =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (202):\n",
            "['rules_strength', 'rule_667', 'rule_363', 'rule_378', 'rule_1', 'rule_545', 'rule_386', 'rule_455', 'rule_264', 'rule_243']\n",
            "\n",
            "===== Feature selection for: LIN =====\n",
            "Accuracy (%): 54.61\n",
            "Selected features (170):\n",
            "['rules_strength', 'rules_count', 'rule_416', 'rule_499', 'rule_602', 'rule_179', 'rule_313', 'rule_577', 'rule_40', 'rule_584']\n",
            "\n",
            "===== Feature selection for: LLY =====\n",
            "Accuracy (%): 52.17\n",
            "Selected features (168):\n",
            "['rules_count', 'rule_28', 'rule_151', 'rule_307', 'rule_290', 'rule_605', 'rules_strength', 'rule_257', 'rule_501', 'rule_107']\n",
            "\n",
            "===== Feature selection for: LR.PA =====\n",
            "Accuracy (%): 48.4\n",
            "Selected features (77):\n",
            "['rule_468', 'rule_29', 'rule_595', 'rule_391', 'rule_199', 'rule_197', 'rule_604', 'rule_138', 'rule_28', 'rule_16']\n",
            "\n",
            "===== Feature selection for: LRCX =====\n",
            "Accuracy (%): 52.54\n",
            "Selected features (158):\n",
            "['rules_strength', 'rule_378', 'rules_count', 'rule_238', 'rule_163', 'rule_284', 'rule_65', 'rule_118', 'rule_315', 'rule_211']\n",
            "\n",
            "===== Feature selection for: MA =====\n",
            "Accuracy (%): 45.01\n",
            "Selected features (82):\n",
            "['rule_449', 'rule_6', 'rule_169', 'rule_451', 'rule_231', 'rule_621', 'rule_544', 'rule_437', 'rule_316', 'rule_497']\n",
            "\n",
            "===== Feature selection for: MC.PA =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (35):\n",
            "['rule_287', 'rule_513', 'rule_405', 'rule_575', 'rule_17', 'rule_190', 'rule_655', 'rule_55', 'rule_361', 'rule_468']\n",
            "\n",
            "===== Feature selection for: MCD =====\n",
            "Accuracy (%): 50.28\n",
            "Selected features (85):\n",
            "['rule_490', 'rule_117', 'rule_71', 'rule_85', 'rule_547', 'rule_250', 'rule_279', 'rule_411', 'rule_65', 'rule_340']\n",
            "\n",
            "===== Feature selection for: MDLZ =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (86):\n",
            "['rules_strength', 'rule_151', 'rule_236', 'rule_596', 'rule_562', 'rule_511', 'rule_421', 'rule_568', 'rule_175', 'rule_332']\n",
            "\n",
            "===== Feature selection for: MDT =====\n",
            "Accuracy (%): 51.04\n",
            "Selected features (98):\n",
            "['rule_225', 'rule_18', 'rule_588', 'rule_35', 'rule_371', 'rule_280', 'rule_360', 'rule_79', 'rule_307', 'rule_75']\n",
            "\n",
            "===== Feature selection for: META =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (60):\n",
            "['rule_163', 'rule_31', 'rule_367', 'rule_108', 'rule_374', 'rule_385', 'rule_168', 'rule_243', 'rule_364', 'rule_604']\n",
            "\n",
            "===== Feature selection for: ML.PA =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (32):\n",
            "['rule_44', 'rule_418', 'rule_315', 'rule_42', 'rule_28', 'rule_254', 'rule_214', 'rule_481', 'rule_449', 'rule_575']\n",
            "\n",
            "===== Feature selection for: MMM =====\n",
            "Accuracy (%): 50.28\n",
            "Selected features (146):\n",
            "['rule_408', 'rule_40', 'rule_23', 'rule_215', 'rule_583', 'rule_17', 'rule_637', 'rule_30', 'rule_513', 'rule_405']\n",
            "\n",
            "===== Feature selection for: MNST =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (108):\n",
            "['rule_664', 'rule_1', 'rule_72', 'rule_66', 'rule_478', 'rule_362', 'rule_200', 'rule_40', 'rule_8', 'rule_206']\n",
            "\n",
            "===== Feature selection for: MO =====\n",
            "Accuracy (%): 47.83\n",
            "Selected features (78):\n",
            "['rule_418', 'rule_29', 'rules_strength', 'rule_587', 'rule_485', 'rule_52', 'rule_231', 'rule_246', 'rule_632', 'rule_575']\n",
            "\n",
            "===== Feature selection for: MRK =====\n",
            "Accuracy (%): 50.85\n",
            "Selected features (97):\n",
            "['rules_strength', 'rule_418', 'rule_228', 'rule_650', 'rule_67', 'rule_175', 'rule_658', 'rule_262', 'rule_362', 'rule_149']\n",
            "\n",
            "===== Feature selection for: MRNA =====\n",
            "Accuracy (%): 54.61\n",
            "Selected features (154):\n",
            "['rules_count', 'rule_42', 'rule_331', 'rule_174', 'rules_strength', 'rule_226', 'rule_17', 'rule_23', 'rule_41', 'rule_200']\n",
            "\n",
            "===== Feature selection for: MSFT =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (45):\n",
            "['rule_196', 'rule_8', 'rule_457', 'rule_74', 'rule_138', 'rule_532', 'rule_473', 'rule_458', 'rule_1', 'rule_526']\n",
            "\n",
            "===== Feature selection for: MU =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (32):\n",
            "['rule_164', 'rule_335', 'rule_257', 'rule_245', 'rule_58', 'rule_321', 'rule_428', 'rule_360', 'rule_305', 'rule_454']\n",
            "\n",
            "===== Feature selection for: NFLX =====\n",
            "Accuracy (%): 49.15\n",
            "Selected features (54):\n",
            "['rule_39', 'rule_529', 'rule_588', 'rule_455', 'rule_74', 'rule_376', 'rule_557', 'rule_302', 'rule_559', 'rule_99']\n",
            "\n",
            "===== Feature selection for: NKE =====\n",
            "Accuracy (%): 50.66\n",
            "Selected features (97):\n",
            "['rules_strength', 'rule_28', 'rule_101', 'rule_60', 'rule_17', 'rule_577', 'rule_13', 'rule_4', 'rule_388', 'rule_422']\n",
            "\n",
            "===== Feature selection for: NVDA =====\n",
            "Accuracy (%): 50.28\n",
            "Selected features (103):\n",
            "['rules_strength', 'rule_400', 'rule_633', 'rule_3', 'rule_86', 'rule_602', 'rule_135', 'rule_279', 'rule_389', 'rule_283']\n",
            "\n",
            "===== Feature selection for: OR.PA =====\n",
            "Accuracy (%): 47.27\n",
            "Selected features (119):\n",
            "['rules_strength', 'rule_87', 'rule_226', 'rule_670', 'rule_10', 'rule_400', 'rule_251', 'rule_616', 'rule_302', 'rule_224']\n",
            "\n",
            "===== Feature selection for: ORA.PA =====\n",
            "Accuracy (%): 46.7\n",
            "Selected features (23):\n",
            "['rule_332', 'rule_585', 'rule_198', 'rule_559', 'rule_53', 'rule_1', 'rule_427', 'rule_299', 'rule_69', 'rule_90']\n",
            "\n",
            "===== Feature selection for: ORCL =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (48):\n",
            "['rule_141', 'rule_196', 'rule_587', 'rule_388', 'rule_603', 'rule_110', 'rule_579', 'rule_39', 'rule_483', 'rule_209']\n",
            "\n",
            "===== Feature selection for: PEP =====\n",
            "Accuracy (%): 49.53\n",
            "Selected features (69):\n",
            "['rules_strength', 'rule_413', 'rule_473', 'rule_467', 'rule_594', 'rule_173', 'rule_124', 'rule_310', 'rule_22', 'rule_129']\n",
            "\n",
            "===== Feature selection for: PFE =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (60):\n",
            "['rule_12', 'rule_510', 'rule_581', 'rule_348', 'rules_count', 'rule_292', 'rule_623', 'rule_596', 'rule_446', 'rule_559']\n",
            "\n",
            "===== Feature selection for: PG =====\n",
            "Accuracy (%): 50.66\n",
            "Selected features (145):\n",
            "['rules_strength', 'rules_count', 'rule_168', 'rule_649', 'rule_80', 'rule_151', 'rule_511', 'rule_558', 'rule_58', 'rule_316']\n",
            "\n",
            "===== Feature selection for: PM =====\n",
            "Accuracy (%): 47.27\n",
            "Selected features (54):\n",
            "['rule_371', 'rule_52', 'rule_74', 'rule_442', 'rule_6', 'rule_562', 'rule_36', 'rule_80', 'rule_223', 'rule_307']\n",
            "\n",
            "===== Feature selection for: PUB.PA =====\n",
            "Accuracy (%): 45.76\n",
            "Selected features (75):\n",
            "['rule_540', 'rule_523', 'rule_387', 'rule_41', 'rule_264', 'rule_326', 'rule_480', 'rule_451', 'rule_231', 'rule_42']\n",
            "\n",
            "===== Feature selection for: PYPL =====\n",
            "Accuracy (%): 55.56\n",
            "Selected features (110):\n",
            "['rules_strength', 'rules_count', 'rule_192', 'rule_30', 'rule_531', 'rule_40', 'rule_633', 'rule_146', 'rule_376', 'rule_304']\n",
            "\n",
            "===== Feature selection for: QCOM =====\n",
            "Accuracy (%): 48.96\n",
            "Selected features (67):\n",
            "['rules_strength', 'rule_464', 'rule_455', 'rule_43', 'rule_175', 'rule_212', 'rule_560', 'rule_541', 'rule_472', 'rule_403']\n",
            "\n",
            "===== Feature selection for: REGN =====\n",
            "Accuracy (%): 52.92\n",
            "Selected features (74):\n",
            "['rule_370', 'rule_173', 'rule_650', 'rule_270', 'rule_127', 'rule_586', 'rule_109', 'rule_603', 'rule_479', 'rule_489']\n",
            "\n",
            "===== Feature selection for: RI.PA =====\n",
            "Accuracy (%): 51.6\n",
            "Selected features (74):\n",
            "['rules_strength', 'rule_6', 'rule_209', 'rule_589', 'rule_37', 'rule_616', 'rule_279', 'rule_645', 'rule_418', 'rule_270']\n",
            "\n",
            "===== Feature selection for: RMS.PA =====\n",
            "Accuracy (%): 48.59\n",
            "Selected features (78):\n",
            "['rules_count', 'rule_558', 'rule_468', 'rule_564', 'rule_179', 'rule_456', 'rule_490', 'rule_563', 'rule_103', 'rule_223']\n",
            "\n",
            "===== Feature selection for: RNO.PA =====\n",
            "Accuracy (%): 48.4\n",
            "Selected features (33):\n",
            "['rule_5', 'rule_650', 'rule_538', 'rule_337', 'rule_198', 'rule_219', 'rule_411', 'rule_156', 'rule_207', 'rule_250']\n",
            "\n",
            "===== Feature selection for: RTX =====\n",
            "Accuracy (%): 53.48\n",
            "Selected features (199):\n",
            "['rules_strength', 'rules_count', 'rule_586', 'rule_23', 'rule_53', 'rule_149', 'rule_422', 'rule_512', 'rule_527', 'rule_511']\n",
            "\n",
            "===== Feature selection for: SAF.PA =====\n",
            "Accuracy (%): 48.4\n",
            "Selected features (55):\n",
            "['rule_145', 'rule_284', 'rule_23', 'rule_34', 'rule_97', 'rule_127', 'rule_383', 'rule_241', 'rule_111', 'rule_378']\n",
            "\n",
            "===== Feature selection for: SAN.PA =====\n",
            "Accuracy (%): 49.91\n",
            "Selected features (50):\n",
            "['rule_208', 'rule_524', 'rule_101', 'rule_559', 'rule_632', 'rule_5', 'rule_311', 'rule_515', 'rule_666', 'rule_97']\n",
            "\n",
            "===== Feature selection for: SBUX =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (62):\n",
            "['rules_count', 'rule_196', 'rules_strength', 'rule_26', 'rule_668', 'rule_284', 'rule_636', 'rule_101', 'rule_500', 'rule_307']\n",
            "\n",
            "===== Feature selection for: SGO.PA =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (75):\n",
            "['rules_strength', 'rule_563', 'rule_42', 'rule_23', 'rule_5', 'rule_156', 'rule_195', 'rule_24', 'rule_279', 'rule_75']\n",
            "\n",
            "===== Feature selection for: SU.PA =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (85):\n",
            "['rule_378', 'rule_211', 'rule_417', 'rule_425', 'rule_464', 'rule_603', 'rule_426', 'rule_302', 'rule_330', 'rule_435']\n",
            "\n",
            "===== Feature selection for: TEAM =====\n",
            "Accuracy (%): 51.41\n",
            "Selected features (48):\n",
            "['rules_count', 'rule_238', 'rule_197', 'rule_163', 'rule_0', 'rule_31', 'rule_423', 'rule_626', 'rule_618', 'rule_254']\n",
            "\n",
            "===== Feature selection for: TEP.PA =====\n",
            "Accuracy (%): 51.98\n",
            "Selected features (76):\n",
            "['rules_count', 'rule_270', 'rule_25', 'rule_279', 'rule_145', 'rule_377', 'rule_119', 'rule_262', 'rule_541', 'rule_33']\n",
            "\n",
            "===== Feature selection for: TMO =====\n",
            "Accuracy (%): 49.53\n",
            "Selected features (72):\n",
            "['rules_count', 'rule_157', 'rule_186', 'rule_478', 'rule_284', 'rule_254', 'rule_119', 'rule_179', 'rule_488', 'rule_312']\n",
            "\n",
            "===== Feature selection for: TSLA =====\n",
            "Accuracy (%): 55.56\n",
            "Selected features (91):\n",
            "['rules_count', 'rules_strength', 'rule_158', 'rule_383', 'rule_588', 'rule_189', 'rule_145', 'rule_144', 'rule_179', 'rule_355']\n",
            "\n",
            "===== Feature selection for: TTE.PA =====\n",
            "Accuracy (%): 52.54\n",
            "Selected features (86):\n",
            "['rule_144', 'rule_213', 'rule_156', 'rule_37', 'rule_368', 'rule_93', 'rule_247', 'rule_12', 'rule_570', 'rule_601']\n",
            "\n",
            "===== Feature selection for: TXN =====\n",
            "Accuracy (%): 49.15\n",
            "Selected features (72):\n",
            "['rule_24', 'rule_19', 'rule_151', 'rule_2', 'rule_590', 'rule_641', 'rule_201', 'rule_63', 'rule_359', 'rule_633']\n",
            "\n",
            "===== Feature selection for: UNH =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (85):\n",
            "['rule_543', 'rule_280', 'rule_644', 'rule_58', 'rule_309', 'rule_500', 'rule_143', 'rule_189', 'rule_665', 'rule_453']\n",
            "\n",
            "===== Feature selection for: UPS =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (108):\n",
            "['rule_357', 'rule_83', 'rule_127', 'rule_107', 'rule_399', 'rule_576', 'rule_10', 'rule_62', 'rule_20', 'rule_390']\n",
            "\n",
            "===== Feature selection for: URW.PA =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (59):\n",
            "['rules_strength', 'rule_505', 'rule_82', 'rule_499', 'rule_14', 'rule_627', 'rule_235', 'rule_326', 'rule_597', 'rule_618']\n",
            "\n",
            "===== Feature selection for: V =====\n",
            "Accuracy (%): 43.88\n",
            "Selected features (123):\n",
            "['rule_243', 'rule_188', 'rule_279', 'rule_594', 'rule_28', 'rule_374', 'rule_229', 'rule_385', 'rule_165', 'rule_121']\n",
            "\n",
            "===== Feature selection for: VIE.PA =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (101):\n",
            "['rules_strength', 'rule_14', 'rule_16', 'rule_296', 'rule_257', 'rule_33', 'rule_400', 'rule_211', 'rule_44', 'rule_357']\n",
            "\n",
            "===== Feature selection for: VIV.PA =====\n",
            "Accuracy (%): 48.78\n",
            "Selected features (115):\n",
            "['rules_strength', 'rule_34', 'rule_42', 'rule_80', 'rule_425', 'rule_64', 'rule_567', 'rule_193', 'rule_386', 'rule_1']\n",
            "\n",
            "===== Feature selection for: VZ =====\n",
            "Accuracy (%): 47.08\n",
            "Selected features (51):\n",
            "['rule_288', 'rule_343', 'rule_202', 'rule_153', 'rule_112', 'rule_539', 'rule_29', 'rule_640', 'rule_449', 'rule_190']\n",
            "\n",
            "===== Feature selection for: WDAY =====\n",
            "Accuracy (%): 47.65\n",
            "Selected features (64):\n",
            "['rule_156', 'rule_472', 'rule_637', 'rule_1', 'rule_28', 'rule_184', 'rule_147', 'rule_15', 'rule_22', 'rule_457']\n",
            "\n",
            "===== Feature selection for: WLN.PA =====\n",
            "Accuracy (%): 50.28\n",
            "Selected features (73):\n",
            "['rule_44', 'rule_371', 'rule_460', 'rule_449', 'rule_570', 'rule_184', 'rule_287', 'rule_75', 'rule_82', 'rule_247']\n",
            "\n",
            "===== Feature selection for: WMT =====\n",
            "Accuracy (%): 47.83\n",
            "Selected features (76):\n",
            "['rule_1', 'rules_strength', 'rule_126', 'rule_311', 'rule_105', 'rule_550', 'rule_165', 'rule_371', 'rule_559', 'rule_611']\n",
            "\n",
            "===== Feature selection for: XOM =====\n",
            "Accuracy (%): 49.72\n",
            "Selected features (73):\n",
            "['rule_199', 'rule_416', 'rule_419', 'rule_192', 'rule_83', 'rule_592', 'rule_560', 'rule_658', 'rule_414', 'rule_206']\n",
            "\n",
            "===== Feature selection for: ^CIISCSEP =====\n",
            "‚ö†Ô∏è Skipped ^CIISCSEP: only one class in target\n",
            "Accuracy (%): nan\n",
            "Selected features (0):\n",
            "[]\n",
            "\n",
            "===== Feature selection for: ^DJI =====\n",
            "Accuracy (%): 51.98\n",
            "Selected features (67):\n",
            "['rule_571', 'rule_40', 'rule_129', 'rule_538', 'rule_254', 'rule_487', 'rule_76', 'rule_118', 'rule_529', 'rule_324']\n",
            "\n",
            "===== Feature selection for: ^FVX =====\n",
            "Accuracy (%): 52.35\n",
            "Selected features (130):\n",
            "['rules_count', 'rule_3', 'rule_466', 'rule_74', 'rule_384', 'rule_665', 'rule_424', 'rule_191', 'rule_39', 'rule_405']\n",
            "\n",
            "===== Feature selection for: ^IRX =====\n",
            "Accuracy (%): 55.18\n",
            "Selected features (118):\n",
            "['rules_strength', 'rule_419', 'rule_139', 'rule_224', 'rule_112', 'rule_324', 'rule_223', 'rule_151', 'rule_369', 'rule_304']\n",
            "\n",
            "===== Feature selection for: ^NDX =====\n",
            "Accuracy (%): 51.6\n",
            "Selected features (63):\n",
            "['rules_strength', 'rule_259', 'rule_235', 'rule_213', 'rule_365', 'rule_425', 'rule_417', 'rule_664', 'rule_120', 'rule_566']\n",
            "\n",
            "===== Feature selection for: ^SP500-15 =====\n",
            "Accuracy (%): 51.22\n",
            "Selected features (68):\n",
            "['rules_strength', 'rule_211', 'rule_254', 'rule_127', 'rule_486', 'rule_55', 'rule_14', 'rule_22', 'rule_107', 'rule_610']\n",
            "\n",
            "===== Feature selection for: ^SP500-20 =====\n",
            "Accuracy (%): 54.8\n",
            "Selected features (145):\n",
            "['rules_count', 'rules_strength', 'rule_550', 'rule_376', 'rule_364', 'rule_369', 'rule_154', 'rule_435', 'rule_158', 'rule_446']\n",
            "\n",
            "===== Feature selection for: ^STOXX50E =====\n",
            "Accuracy (%): 50.47\n",
            "Selected features (135):\n",
            "['rules_strength', 'rule_363', 'rule_423', 'rule_61', 'rule_86', 'rule_279', 'rule_375', 'rule_66', 'rule_425', 'rule_417']\n",
            "\n",
            "===== Feature selection for: ^TNX =====\n",
            "Accuracy (%): 49.34\n",
            "Selected features (59):\n",
            "['rule_22', 'rule_365', 'rule_39', 'rule_405', 'rule_485', 'rule_74', 'rule_529', 'rule_605', 'rule_316', 'rule_248']\n",
            "\n",
            "===== Feature selection for: ^TYX =====\n",
            "Accuracy (%): 48.96\n",
            "Selected features (111):\n",
            "['rule_74', 'rule_35', 'rule_67', 'rule_280', 'rule_108', 'rule_554', 'rule_422', 'rule_103', 'rule_5', 'rule_316']\n",
            "\n",
            "===== Feature selection for: ^VIX =====\n",
            "Accuracy (%): 55.93\n",
            "Selected features (103):\n",
            "['rule_124', 'rule_292', 'rule_65', 'rules_strength', 'rule_230', 'rule_201', 'rule_208', 'rule_563', 'rule_602', 'rule_359']\n",
            "\n",
            "=== DONE ===\n",
            "Files generated:\n",
            "- selected_features_per_variable.json\n",
            "- accuracy_per_variable.csv\n",
            "- feature_importance_per_variable.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Annex*"
      ],
      "metadata": {
        "id": "AGqlLzokPs_r"
      },
      "id": "AGqlLzokPs_r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Methode rules detection 1 jour"
      ],
      "metadata": {
        "id": "JwIh3kCqQqDl"
      },
      "id": "JwIh3kCqQqDl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des donn√©es\n",
        "# =========================\n",
        "\n",
        "# Adapter le chemin si n√©cessaire\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discr√©tisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. D√©tection des r√®gles\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules(\n",
        "    disc,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        future = disc[target].shift(-1)\n",
        "        base_rate = (future == \"UP\").mean()\n",
        "\n",
        "        # On ignore les cibles trop rares\n",
        "        if base_rate < 0.05:\n",
        "            continue\n",
        "\n",
        "        leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "        for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "            # Condition √† t\n",
        "            mask = (\n",
        "                (disc[l1] == \"UP\") &\n",
        "                (disc[l2] == \"DOWN\")\n",
        "            )\n",
        "\n",
        "            support = mask.mean()\n",
        "            if support < min_support:\n",
        "                continue\n",
        "\n",
        "            confidence = (future[mask] == \"UP\").mean()\n",
        "            if confidence < min_confidence:\n",
        "                continue\n",
        "\n",
        "            lift = confidence / base_rate\n",
        "            if lift < min_lift:\n",
        "                continue\n",
        "\n",
        "            rules.append({\n",
        "                \"target\": target,\n",
        "                \"leaders\": f\"{l1} ‚Üë AND {l2} ‚Üì\",\n",
        "                \"support\": round(support, 3),\n",
        "                \"confidence\": round(confidence, 3),\n",
        "                \"lift\": round(lift, 2)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. Ex√©cution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules(disc)\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. R√©sultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 r√®gles d√©tect√©es :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "# Optionnel : sauvegarde\n",
        "rules.to_csv(\"lead_lag_rules_results1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "H0RhEw3vQqDm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "H0RhEw3vQqDm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Methode cross correlation Lead‚ÄìLag multivari√© (Lasso)"
      ],
      "metadata": {
        "id": "BdSnwSI0IB7W"
      },
      "id": "BdSnwSI0IB7W"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LEAD‚ÄìLAG MULTIVARI√â PAR LASSO (SCRIPT COMPLET)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# ----------------------------\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# gestion de la date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2. NETTOYAGE & S√âCURIT√âS\n",
        "# ----------------------------\n",
        "\n",
        "# garder uniquement les colonnes num√©riques\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "if df.shape[1] == 0:\n",
        "    raise ValueError(\"Aucune colonne num√©rique exploitable\")\n",
        "\n",
        "# conserver les s√©ries suffisamment compl√®tes (au moins 80% des lignes pr√©sentes)\n",
        "threshold = int(len(df) * 0.8)\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# supprimer les lignes avec NaN restantes\n",
        "df = df.dropna()\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset vide apr√®s nettoyage. Essayez de r√©duire le threshold ou de v√©rifier la source des donn√©es.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. STANDARDISATION\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df.values),\n",
        "    index=df.index,\n",
        "    columns=df.columns\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4. CONSTRUCTION DES LAGS\n",
        "# ----------------------------\n",
        "def build_lagged_matrix(df, max_lag):\n",
        "    X = []\n",
        "    feature_names = []\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        shifted = df.shift(lag)\n",
        "        X.append(shifted.values)\n",
        "\n",
        "        for col in df.columns:\n",
        "            feature_names.append(f\"{col}_lag{lag}\")\n",
        "\n",
        "    X = np.hstack(X)\n",
        "    return X, feature_names\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5. LASSO MULTIVARI√â LEAD‚ÄìLAG\n",
        "# ----------------------------\n",
        "def multivariate_lead_lag_lasso(df, target, max_lag=5, min_coef=1e-4):\n",
        "    X, feature_names = build_lagged_matrix(df, max_lag)\n",
        "    y = df[target].values\n",
        "\n",
        "    # alignement temporel\n",
        "    X = X[max_lag:]\n",
        "    y = y[max_lag:]\n",
        "\n",
        "    model = LassoCV(\n",
        "        cv=5,\n",
        "        max_iter=5000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for coef, fname in zip(model.coef_, feature_names):\n",
        "        if abs(coef) >= min_coef:\n",
        "            col, lag = fname.rsplit(\"_lag\", 1)\n",
        "            results.append({\n",
        "                \"target\": target,\n",
        "                \"leader\": col,\n",
        "                \"lag_days\": int(lag),\n",
        "                \"coefficient\": coef\n",
        "            })\n",
        "\n",
        "    # Gestion du cas o√π aucun coefficient n'est retenu (√©vite KeyError)\n",
        "    if not results:\n",
        "        return pd.DataFrame(columns=[\"target\", \"leader\", \"lag_days\", \"coefficient\"])\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6. LANCEMENT GLOBAL\n",
        "# ----------------------------\n",
        "MAX_LAG = 5\n",
        "all_results = []\n",
        "\n",
        "for target in df_scaled.columns:\n",
        "    res = multivariate_lead_lag_lasso(\n",
        "        df_scaled,\n",
        "        target=target,\n",
        "        max_lag=MAX_LAG\n",
        "    )\n",
        "\n",
        "    if not res.empty:\n",
        "        all_results.append(res)\n",
        "\n",
        "if not all_results:\n",
        "    print(\"Aucun lead-lag significatif trouv√©.\")\n",
        "    best_results = pd.DataFrame()\n",
        "else:\n",
        "    final_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 7. R√âSULTATS FINAUX\n",
        "    # ----------------------------\n",
        "    best_results = final_results.sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    print(best_results.head(20))\n",
        "\n",
        "    # ----------------------------\n",
        "    # 8. EXPORT OPTIONNEL\n",
        "    # ----------------------------\n",
        "    best_results.to_csv(\n",
        "        \"multivariate_lead_lag_lasso_results.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# FIN DU SCRIPT\n",
        "# ============================================================"
      ],
      "metadata": {
        "id": "j6SeFQr3H8PV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "6aba8ede-2a96-4900-ef5e-13e06ccc8efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'DATASET_1000DAYS_VARIATION.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2077941601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mFILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DATASET_1000DAYS_VARIATION.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# gestion de la date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DATASET_1000DAYS_VARIATION.csv'"
          ]
        }
      ],
      "id": "j6SeFQr3H8PV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Lead Lag search"
      ],
      "metadata": {
        "id": "Z1jerT1XM-pP"
      },
      "id": "Z1jerT1XM-pP"
    },
    {
      "cell_type": "code",
      "source": [
        "# The following line caused a ModuleNotFoundError because 'toto' is not a valid or installed module.\n",
        "# import toto\n",
        "\n",
        "# To fix this, replace 'toto' with the actual library you intended to use.\n",
        "# If you need to install a package, use: !pip install <package_name>"
      ],
      "metadata": {
        "id": "owfOt5FAWCgN"
      },
      "id": "owfOt5FAWCgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7",
      "metadata": {
        "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7"
      },
      "outputs": [],
      "source": [
        "### Recherche des leads/lags sur les actions du CAC40\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger le CSV\n",
        "dataset_ts = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]   # tr√®s important\n",
        ")\n",
        "\n",
        "# Trier par date (s√©curit√©)\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "# Optionnel : remettre la date en index\n",
        "dataset_ts = dataset_ts.set_index(\"date\")\n",
        "\n",
        "def scan_lead_lag_cac40(\n",
        "    df,\n",
        "    cac40_cols,\n",
        "    other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40,\n",
        "    method=\"pearson\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Scan exhaustif lead/lag entre :\n",
        "    - other_cols (X candidates)\n",
        "    - cac40_cols (Y targets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for y in cac40_cols:\n",
        "        for x in other_cols:\n",
        "\n",
        "            if x == y:\n",
        "                continue\n",
        "\n",
        "            best_row = None\n",
        "\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "\n",
        "                if lag < 0:\n",
        "                    x_shifted = df[x].shift(-lag)\n",
        "                    y_shifted = df[y]\n",
        "                else:\n",
        "                    x_shifted = df[x]\n",
        "                    y_shifted = df[y].shift(lag)\n",
        "\n",
        "                pair = pd.concat([x_shifted, y_shifted], axis=1).dropna()\n",
        "\n",
        "                if len(pair) < min_obs:\n",
        "                    continue\n",
        "\n",
        "                corr = pair.iloc[:, 0].corr(pair.iloc[:, 1], method=method)\n",
        "\n",
        "                if pd.isna(corr):\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"x\": x,\n",
        "                    \"y\": y,\n",
        "                    \"lag\": lag,\n",
        "                    \"correlation\": corr,\n",
        "                    \"abs_corr\": abs(corr),\n",
        "                    \"direction\": \"inverse\" if corr < 0 else \"same\",\n",
        "                    \"n_obs\": len(pair)\n",
        "                }\n",
        "\n",
        "                if best_row is None or row[\"abs_corr\"] > best_row[\"abs_corr\"]:\n",
        "                    best_row = row\n",
        "\n",
        "            if best_row is not None:\n",
        "                results.append(best_row)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df = dataset_ts\n",
        "cac40_cols = pd.read_csv(\"CAC40_TICKERS.csv\")['Ticker'].tolist()\n",
        "other_cols = [c for c in df.columns if c not in cac40_cols]\n",
        "\n",
        "results = scan_lead_lag_cac40(\n",
        "    df=df,\n",
        "    cac40_cols=cac40_cols,\n",
        "    other_cols=other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40\n",
        ")\n",
        "\n",
        "results.sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "results[\n",
        "    results[\"direction\"] == \"inverse\"\n",
        "].sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "\n",
        "results[results[\"y\"] == \"MC.PA\"] \\\n",
        "    .sort_values(\"abs_corr\", ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(\"Shape:\", results.shape)\n",
        "results.to_csv(\"LEAD_LAG_SCAN_CAC40.csv\", index=False)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des leads/lags sur les actions du CAC40 termin√©e: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___Candidates selection"
      ],
      "metadata": {
        "id": "5rIc4-G4NG_T"
      },
      "id": "5rIc4-G4NG_T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
      },
      "outputs": [],
      "source": [
        "### Recherche des variables candidates qui ont des leads/lags significatifs sur le CAC40\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU CSV\n",
        "# =========================\n",
        "\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "print(\"Dataset initial :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 2. FILTRE : LAG ‚â† 0\n",
        "# =========================\n",
        "\n",
        "leadlag = leadlag[leadlag[\"lag\"] != 0]\n",
        "\n",
        "print(\"Apr√®s suppression lag = 0 :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 3. CLASSIFICATION DU RISQUE\n",
        "# =========================\n",
        "\n",
        "def classify_risk(abs_corr, n_obs):\n",
        "    if abs_corr >= 0.55 and n_obs >= 80:\n",
        "        return \"very_low_risk\"\n",
        "    elif abs_corr >= 0.45 and n_obs >= 60:\n",
        "        return \"low_risk\"\n",
        "    elif abs_corr >= 0.40 and n_obs >= 50:\n",
        "        return \"medium_risk\"\n",
        "    elif abs_corr >= 0.30 and n_obs >= 40:\n",
        "        return \"high_risk\"\n",
        "    else:\n",
        "        return \"very_high_risk\"\n",
        "\n",
        "leadlag[\"risk_level\"] = leadlag.apply(\n",
        "    lambda r: classify_risk(r[\"abs_corr\"], r[\"n_obs\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. TRI FINAL (DU + S√õR AU + RISQU√â)\n",
        "# =========================\n",
        "\n",
        "risk_order = [\n",
        "    \"very_low_risk\",\n",
        "    \"low_risk\",\n",
        "    \"medium_risk\",\n",
        "    \"high_risk\",\n",
        "    \"very_high_risk\"\n",
        "]\n",
        "\n",
        "leadlag[\"risk_level\"] = pd.Categorical(\n",
        "    leadlag[\"risk_level\"],\n",
        "    categories=risk_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "leadlag_sorted = leadlag.sort_values(\n",
        "    [\"risk_level\", \"abs_corr\"],\n",
        "    ascending=[True, False]\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. CONTR√îLE RAPIDE\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 relations les plus s√ªres (lag ‚â† 0) :\")\n",
        "print(leadlag_sorted.head(20))\n",
        "\n",
        "# =========================\n",
        "# 6. SAUVEGARDE\n",
        "# =========================\n",
        "\n",
        "leadlag_sorted.to_csv(\n",
        "    \"LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nCSV sauvegard√© : LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\")\n",
        "\n",
        "# Charger le scan lead/lag\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "# Filtre SAFE\n",
        "filtered = leadlag[\n",
        "    (leadlag[\"lag\"] != 0) &\n",
        "    (leadlag[\"abs_corr\"] >= 0.25) &\n",
        "    (leadlag[\"n_obs\"] >= 40)\n",
        "]\n",
        "\n",
        "# Extraire les candidats (variables X)\n",
        "candidatesX = (\n",
        "    filtered[\"x\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesX.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate variables\")\n",
        "print(candidatesX)\n",
        "pd.DataFrame(candidatesX, columns=[\"variable\"]).to_csv(\"CANDIDATES_VARIABLES.csv\", index=False)\n",
        "\n",
        "# Extraire les cibles (variables y)\n",
        "candidatesY = (\n",
        "    filtered[\"y\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesY.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate taget\")\n",
        "print(candidatesY)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des variables candidates CANDIDATES_VARIABLES.csv termin√©e: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ],
      "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___CS.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "Uij5aRPaK47h"
      },
      "id": "Uij5aRPaK47h"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"^NDX\", \"LIN\", \"ABBV\", \"CS.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"^NDX\", \"LIN\", \"ABBV\"]]\n",
        "y = df[\"CS.PA\"].shift(-1)  # URW.PA √† J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"^NDX\", \"LIN\", \"ABBV\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entra√Æner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"^NDX\", \"LIN\", \"ABBV\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"^NDX\", \"LIN\", \"ABBV\"]]],\n",
        "        columns=[\"^NDX\", \"LIN\", \"ABBV\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PR√âDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"CS.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"CS.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. R√âSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Cr√©ation du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"CS.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"CS.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# V√©rification\n",
        "print(final_dataset[[\n",
        "    \"CS.PA\",\n",
        "    \"CS.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"CS_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "HqrQ5wv0WwYg"
      },
      "id": "HqrQ5wv0WwYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___URW.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "zyY4sc6rMDr0"
      },
      "id": "zyY4sc6rMDr0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"GC=F\", \"^NDX\", \"AAPL\", \"URW.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"GC=F\", \"^NDX\", \"AAPL\"]]\n",
        "y = df[\"URW.PA\"].shift(-1)  # URW.PA √† J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"GC=F\", \"^NDX\", \"AAPL\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entra√Æner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"GC=F\", \"^NDX\", \"AAPL\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"GC=F\", \"^NDX\", \"AAPL\"]]],\n",
        "        columns=[\"GC=F\", \"^NDX\", \"AAPL\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PR√âDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"URW.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"URW.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. R√âSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Cr√©ation du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"URW.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"URW.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# V√©rification\n",
        "print(final_dataset[[\n",
        "    \"URW.PA\",\n",
        "    \"URW.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"URW_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "khd_HR51LwOG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "khd_HR51LwOG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect saxo bank"
      ],
      "metadata": {
        "id": "R_gn7zKPNkJh"
      },
      "id": "R_gn7zKPNkJh"
    },
    {
      "cell_type": "code",
      "source": [
        "# test conncetion Saxo bank simulation\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "\n",
        "ACCESS_TOKEN = \"eyJhbGciOiJFUzI1NiIsIng1dCI6IjY3NEM0MjFEMzZEMUE1OUNFNjFBRTIzMjMyOTVFRTAyRTc3MDMzNTkifQ.eyJvYWEiOiI3Nzc3NSIsImlzcyI6Im9hIiwiYWlkIjoiMTA5IiwidWlkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiY2lkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiaXNhIjoiRmFsc2UiLCJ0aWQiOiIyMDAyIiwic2lkIjoiMzYwNGFlY2NmYTc3NGNlNmEzNGZmNDIyNGUyZDBmNWEiLCJkZ2kiOiI4NCIsImV4cCI6IjE3Njg4MTEzMjgiLCJvYWwiOiIxRiIsImlpZCI6IjFkNzAzNjg4NzM4MTQzNjMwNGE1MDhkZTRjNTUxOTUwIn0.NuSPihbRVQ_YHoWnW-sQHESQ9QNBExUgGpEM-GaD7gWOQfhr2OU7D4qmgGjhLEW7FNDKsdkL0bRfwf5xuOzftg\"\n",
        "BASE_URL = \"https://gateway.saxobank.com/sim/openapi\"  # ou /sim/openapi\n",
        "SYMBOL = {\"URW.PA\" : \"19099381\"}\n",
        "EXCHANGE = \"Euronext Paris\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "    \"Accept\" : \"*/*\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def get_AccountKeys():\n",
        "    url = f\"{BASE_URL}/port/v1/accounts/me\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"Data\"]\n",
        "\n",
        "def get_Balances(AccountKey):\n",
        "    url = f\"{BASE_URL}/port/v1/balances?AccountKey=\"+AccountKey+\"&ClientKey=\"+AccountKey\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def get_TickerPosition(Uic):\n",
        "    url = f\"{BASE_URL}/trade/v1/infoprices/list?AccountKey=\"+AccountKey+\"&Uics=\"+Uic+\"&AssetType=Stock&Amount=100000&FieldGroups=DisplayAndFormat,Quote\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "AccountKeys = get_AccountKeys()\n",
        "AccountKey = AccountKeys[0][\"AccountKey\"]\n",
        "Balances = get_Balances(AccountKey)\n",
        "URW = get_TickerPosition(SYMBOL[\"URW.PA\"])\n",
        "print(URW)\n",
        "\n",
        "print(AccountKey)\n",
        "print(Balances)"
      ],
      "metadata": {
        "id": "SWGNJbYASMC1"
      },
      "id": "SWGNJbYASMC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today prediction"
      ],
      "metadata": {
        "id": "HDaUtak-Lj_h"
      },
      "id": "HDaUtak-Lj_h"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CS.PA ‚Äì SIGNAL QUANT +\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# PARAM√àTRES G√âN√âRAUX\n",
        "# =========================\n",
        "\n",
        "CSV_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "FEATURES = [\"^NDX\", \"LIN\",\"ABBV\"]\n",
        "TARGET   = \"CS.PA\"\n",
        "\n",
        "BUY_THRESHOLD  = 0.5\n",
        "SELL_THRESHOLD = -0.5\n",
        "\n",
        "MIN_TRAIN_SIZE = 60\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGER DATASET HISTORIQUE\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "df = df[FEATURES + [TARGET]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. ENTRA√éNEMENT DU MOD√àLE (lag = 1)\n",
        "# =========================\n",
        "\n",
        "X = df[FEATURES]\n",
        "y = df[TARGET].shift(-1)\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = FEATURES + [\"CS_TARGET\"]\n",
        "\n",
        "X_train = sm.add_constant(data[FEATURES], has_constant=\"add\")\n",
        "y_train = data[\"CS_TARGET\"]\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# =========================\n",
        "# 3. DONN√âES LIVE (cl√¥ture US)\n",
        "# =========================\n",
        "\n",
        "tickers = FEATURES + [TARGET]\n",
        "\n",
        "prices = yf.download(\n",
        "    tickers,\n",
        "    period=\"2d\",\n",
        "    interval=\"1d\",\n",
        "    auto_adjust=True,\n",
        "    progress=False\n",
        ")[\"Close\"]\n",
        "\n",
        "returns_today = prices.pct_change(fill_method=None).iloc[-1] * 100\n",
        "\n",
        "missing = returns_today[FEATURES].isna()\n",
        "\n",
        "if missing.any():\n",
        "    print(\"‚ö†Ô∏è March√©s non cl√¥tur√©s :\", missing[missing].index.tolist())\n",
        "    predicted_return = 0.0\n",
        "    signal = \"KEEP\"\n",
        "\n",
        "else:\n",
        "    X_today = pd.DataFrame(\n",
        "        [[\n",
        "            returns_today[\"^NDX\"],\n",
        "            returns_today[\"LIN\"],\n",
        "            returns_today[\"ABBV\"]\n",
        "        ]],\n",
        "        columns=FEATURES\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    predicted_return = model.predict(X_today).iloc[0]\n",
        "\n",
        "    if predicted_return > BUY_THRESHOLD:\n",
        "        signal = \"BUY\"\n",
        "    elif predicted_return < SELL_THRESHOLD:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"KEEP\"\n",
        "\n",
        "# =========================\n",
        "# 6. SORTIE\n",
        "# =========================\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"   SIGNAL URW.PA (J+1)\")\n",
        "print(\"==============================\")\n",
        "print(\"Date :\", datetime.now().strftime(\"%Y-%m-%d %H:%M CET\"))\n",
        "print(\"\\nVariations aujourd'hui (%)\")\n",
        "print(returns_today[FEATURES])\n",
        "print(f\"\\nPr√©vision URW.PA J+1 : {predicted_return:.3f} %\")\n",
        "print(\"==============================\\n\")"
      ],
      "metadata": {
        "id": "9RikQhCXSRQb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9RikQhCXSRQb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}