{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e"
      },
      "outputs": [],
      "source": [
        "### Creation du dataset de variation des tickers des 1000 derniers jours\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Définir la période de 1000 jours\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# Liste des tickers du CAC 40 (exemple : 'OR.PA' pour L'Oréal)\n",
        "cac40_tickers = cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # Crédit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault Systèmes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # Hermès\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'Oréal\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # Société Générale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# Liste des tickers du NASDAQ (exemple : 'AAPL' pour Apple)\n",
        "nasdaq_tickers = nasdaq_top_40 = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices (AMD)\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # Mondelēz\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\", # ASML Holding\n",
        "    \"MRNA\", # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "nyse_tickers = nyse_top_40 = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"CSCO\",  # Cisco (aussi coté sur NYSE)\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\",    # Altria\n",
        "]\n",
        "\n",
        "# Inclure également les indices macro (VIX, Dollar Index, Brent, Or)\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold Futures COMEX)\n",
        "    \"BZ=F\",        # Pétrole Brent (Brent Crude Oil Futures)\n",
        "    \"^NDX\",        # NASDAQ 100 (technologie US, croissance)\n",
        "    \"^DJI\",        # Dow Jones Industrial Average (blue chips US)\n",
        "    \"^SP500-20\",   # S&P 500 Industrials (secteur industriel)\n",
        "    \"^SP500-15\",   # S&P 500 Materials (matières premières)\n",
        "    \"^STOXX50E\",   # Euro STOXX 50 (grandes capitalisations zone euro)\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY – force du dollar US)\n",
        "    \"EUR=X\",       # Taux de change EUR/USD\n",
        "    \"CHF=X\",       # Taux de change USD/CHF (valeur refuge)\n",
        "    \"^VIX\",        # Indice de volatilité (peur / stress de marché)\n",
        "    \"^IRX\",        # Taux US 13 semaines (T-Bills court terme)\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans (benchmark macro mondial)\n",
        "    \"^TYX\",        # Taux US 30 ans (long terme)\n",
        "    \"^CIISCSEP\",   # Indice de surprises économiques Citi (US)\n",
        "    \"^MOVE\",       # Indice de crédit (Credit Default Swaps – stress crédit)\n",
        "    \"BTC-USD\",     # Bitcoin\n",
        "    \"^T5YIE\",      # Breakeven inflation 5 ans\n",
        "    \"HYG\",         # Crédit high yield (risk-on/off)\n",
        "    \"LQD\",         # Crédit investment grade\n",
        "    \"000001.SS\",   # Shanghai Composite\n",
        "    \"FXI\",         # ETF China large caps\n",
        "    \"HG=F\"         # Cuivre (Dr Copper)\n",
        "]\n",
        "\n",
        "\n",
        "# Combiner toutes les listes de tickers\n",
        "all_tickers = cac40_tickers + nasdaq_tickers + nyse_tickers + macro_tickers\n",
        "\n",
        "# Télécharger les données boursières\n",
        "data = yf.download(all_tickers, start=start_date, end=end_date,auto_adjust=True)\n",
        "\n",
        "# Calculer la variation en pourcentage par rapport à la clôture précédente\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "\n",
        "dataset_ts = variation.reset_index()\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "print(\"Shape:\", dataset_ts.shape)\n",
        "print(dataset_ts.head())\n",
        "print(dataset_ts.tail())\n",
        "print(\"NaN par colonne :\")\n",
        "print(variation.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "pd.DataFrame(cac40_tickers, columns=[\"Ticker\"]).to_csv(\"CAC40_TICKERS.csv\", index=False)\n",
        "dataset_ts.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminée: \")\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methode rules detection avec lag 1-3 jours"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x, thr=1.0):\n",
        "    if x > thr:\n",
        "        return \"UP\"\n",
        "    elif x < -thr:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Définition des patterns\n",
        "# =========================\n",
        "\n",
        "LEADER_PATTERNS = [\n",
        "    (\"UP\", \"UP\"),\n",
        "    (\"UP\", \"DOWN\"),\n",
        "    (\"DOWN\", \"UP\"),\n",
        "    (\"DOWN\", \"DOWN\"),\n",
        "]\n",
        "\n",
        "TARGET_STATES = [\"UP\", \"DOWN\"]\n",
        "\n",
        "# =========================\n",
        "# 4. Lift conditionnel (pattern-aware)\n",
        "# =========================\n",
        "\n",
        "def conditional_lift(disc, target, l1, l2, s1, s2, lag, target_state):\n",
        "    shifted = disc.shift(lag)\n",
        "    valid = shifted[[l1, l2, target]].dropna()\n",
        "\n",
        "    base_rate = (valid[target] == target_state).mean()\n",
        "    if base_rate == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Lift avec l1 seul\n",
        "    mask_l1 = valid[l1] == s1\n",
        "    if mask_l1.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    lift_l1 = (\n",
        "        (valid.loc[mask_l1, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    # Lift avec l1 + l2\n",
        "    mask_l1_l2 = (valid[l1] == s1) & (valid[l2] == s2)\n",
        "    if mask_l1_l2.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    lift_l1_l2 = (\n",
        "        (valid.loc[mask_l1_l2, target] == target_state).mean()\n",
        "    ) / base_rate\n",
        "\n",
        "    return lift_l1_l2 - lift_l1\n",
        "\n",
        "# =========================\n",
        "# 5. Mining des règles exhaustives\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules_exhaustive(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05,\n",
        "    min_base_rate=0.05,\n",
        "    max_corr=0.85\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            print(f\"\\nTarget: {target} | lag: {lag}\")\n",
        "\n",
        "            future = disc[target]\n",
        "            leaders_disc = disc.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_disc.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_disc = leaders_disc.loc[valid_idx]\n",
        "\n",
        "            leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                # ---- filtre corrélation brute\n",
        "                corr = abs(df[l1].corr(df[l2]))\n",
        "                if corr > max_corr:\n",
        "                    continue\n",
        "\n",
        "                for s1, s2 in LEADER_PATTERNS:\n",
        "\n",
        "                    mask = (\n",
        "                        (leaders_disc[l1] == s1) &\n",
        "                        (leaders_disc[l2] == s2)\n",
        "                    )\n",
        "\n",
        "                    support = mask.mean()\n",
        "                    if support < min_support:\n",
        "                        continue\n",
        "\n",
        "                    for target_state in TARGET_STATES:\n",
        "\n",
        "                        base_rate = (future == target_state).mean()\n",
        "                        if base_rate < min_base_rate:\n",
        "                            continue\n",
        "\n",
        "                        confidence = (future[mask] == target_state).mean()\n",
        "                        if confidence < min_confidence:\n",
        "                            continue\n",
        "\n",
        "                        lift = confidence / base_rate\n",
        "                        if lift < min_lift:\n",
        "                            continue\n",
        "\n",
        "                        delta_lift = conditional_lift(\n",
        "                            disc, target, l1, l2, s1, s2, lag, target_state\n",
        "                        )\n",
        "                        if delta_lift < min_conditional_lift:\n",
        "                            continue\n",
        "\n",
        "                        rules.append({\n",
        "                            \"target\": target,\n",
        "                            \"target_direction\": target_state,\n",
        "                            \"leader_1\": l1,\n",
        "                            \"leader_2\": l2,\n",
        "                            \"leader_1_state\": s1,\n",
        "                            \"leader_2_state\": s2,\n",
        "                            \"lag_days\": lag,\n",
        "                            \"support\": round(support, 3),\n",
        "                            \"confidence\": round(confidence, 3),\n",
        "                            \"lift\": round(lift, 2),\n",
        "                            \"delta_lift\": round(delta_lift, 2),\n",
        "                            \"corr_leaders\": round(corr, 2),\n",
        "                            \"occurrences\": int(mask.sum())\n",
        "                        })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 6. Exécution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules_exhaustive(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_conditional_lift=0.05\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"delta_lift\", \"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "print(\"\\nTop 20 règles exhaustives (2 leaders, tous les sens) :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "rules.to_csv(\"lead_lag_rules_exhaustive_2leaders.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ulxQ3yK6AiaM"
      },
      "id": "ulxQ3yK6AiaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature selection par variable"
      ],
      "metadata": {
        "id": "OHG0YnIhPrtO"
      },
      "id": "OHG0YnIhPrtO"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==============================\n",
        "# PATHS\n",
        "# ==============================\n",
        "RULES_PATH = \"lead_lag_rules_exhaustive_2leaders.csv\"\n",
        "DATA_PATH  = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "# ==============================\n",
        "# LOAD DATA\n",
        "# ==============================\n",
        "rules = pd.read_csv(RULES_PATH)\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])   # first column = date\n",
        "df = df.set_index(df.columns[0]).sort_index()   # index = date\n",
        "\n",
        "# ==============================\n",
        "# HELPERS\n",
        "# ==============================\n",
        "def state_condition(series: pd.Series, state: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Convert UP / DOWN state into boolean condition\n",
        "    \"\"\"\n",
        "    if state == \"UP\":\n",
        "        return series > 0\n",
        "    elif state == \"DOWN\":\n",
        "        return series < 0\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown state: {state}\")\n",
        "\n",
        "# ==============================\n",
        "# FEATURE GENERATION\n",
        "# ==============================\n",
        "features = pd.DataFrame(index=df.index)\n",
        "feature_cols = []\n",
        "\n",
        "for idx, rule in rules.iterrows():\n",
        "\n",
        "    try:\n",
        "        target        = rule[\"target\"]\n",
        "        lag           = int(rule[\"lag_days\"])\n",
        "        direction     = 1 if rule[\"target_direction\"] == \"UP\" else -1\n",
        "\n",
        "        leaders = [\n",
        "            (rule[\"leader_1\"], rule[\"leader_1_state\"]),\n",
        "            (rule[\"leader_2\"], rule[\"leader_2_state\"]),\n",
        "        ]\n",
        "\n",
        "        support    = rule[\"support\"]\n",
        "        confidence = rule[\"confidence\"]\n",
        "        lift       = rule[\"lift\"]\n",
        "\n",
        "        fname = f\"rule_{idx}_{leaders[0][0]}_{leaders[1][0]}_lag{lag}\"\n",
        "\n",
        "        # Leader conditions\n",
        "        conds = []\n",
        "        for leader, state in leaders:\n",
        "            series = df[leader]\n",
        "            conds.append(state_condition(series, state))\n",
        "\n",
        "        # Logical AND between leaders\n",
        "        rule_active = conds[0] & conds[1]\n",
        "\n",
        "        # Apply lag (NO LOOKAHEAD)\n",
        "        rule_active = rule_active.shift(lag)\n",
        "\n",
        "        # Binary feature\n",
        "        features[fname] = rule_active.astype(int)\n",
        "\n",
        "        # Weighted feature (rule quality)\n",
        "        features[f\"{fname}_w\"] = (\n",
        "            features[fname]\n",
        "            * confidence\n",
        "            * np.log1p(support)\n",
        "            * lift\n",
        "        )\n",
        "\n",
        "        # Directional feature\n",
        "        features[f\"{fname}_dir\"] = features[fname] * direction\n",
        "\n",
        "        feature_cols.extend([\n",
        "            fname,\n",
        "            f\"{fname}_w\",\n",
        "            f\"{fname}_dir\"\n",
        "        ])\n",
        "\n",
        "    except KeyError:\n",
        "        # missing ticker in dataset\n",
        "        continue\n",
        "\n",
        "# ==============================\n",
        "# TARGET (example: J+1 return)\n",
        "# ==============================\n",
        "HORIZON = 1\n",
        "TARGET_TICKER = rules.iloc[0][\"target\"]   # mono-target example\n",
        "\n",
        "features[\"target\"] = df[TARGET_TICKER].shift(-HORIZON)\n",
        "\n",
        "# ==============================\n",
        "# RULE-BASED SIGNAL\n",
        "# ==============================\n",
        "features[\"signal_score\"] = features.filter(like=\"_dir\").sum(axis=1)\n",
        "\n",
        "features[\"signal\"] = np.where(\n",
        "    features[\"signal_score\"] > 0,  1,\n",
        "    np.where(features[\"signal_score\"] < 0, -1, 0)\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# FINAL DATASETS\n",
        "# ==============================\n",
        "features = features.fillna(0)\n",
        "\n",
        "X = features.drop(columns=[\"target\", \"signal\"])\n",
        "y = features[\"target\"]\n",
        "signal = features[\"signal\"]\n",
        "\n",
        "print(\"Features shape :\", X.shape)\n",
        "print(\"Target shape   :\", y.shape)\n",
        "print(\"Signal counts  :\")\n",
        "print(signal.value_counts())\n"
      ],
      "metadata": {
        "id": "YBefcEpe3EhY"
      },
      "id": "YBefcEpe3EhY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}