{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phicaillol-cyber/repository/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "c-sdParCMe8G"
      },
      "id": "c-sdParCMe8G"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
      "metadata": {
        "id": "a59ae70c-0532-4cf6-ac10-38d606068a6e",
        "outputId": "60e72f52-f878-4605-99cc-c32ebb8642d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[****                   9%                       ]  12 of 131 completedERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: CDXBTC-USD\"}}}\n",
            "[*********************100%***********************]  131 of 131 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['CDXBTC-USD']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (710, 132)\n",
            "Ticker       date      AAPL      ABBV       ABT    ACA.PA       ACN      ADBE  \\\n",
            "0      2023-05-16  0.000000 -2.251187 -0.409664  0.428143  0.605376 -0.162012   \n",
            "1      2023-05-17  0.360309  0.041890 -0.521075 -0.289895  1.948473  3.338072   \n",
            "2      2023-05-18  1.366610  0.062773 -0.321628  0.991954  1.001335  1.065527   \n",
            "3      2023-05-19  0.062844  1.164257  0.424073  0.508052  0.845276  3.001972   \n",
            "4      2023-05-22 -0.548097 -0.144730 -0.624246  0.454928  0.358712  0.215485   \n",
            "\n",
            "Ticker       ADI     AI.PA    AIR.PA  ...      ^DJI      ^FVX      ^IRX  \\\n",
            "0      -0.856987 -0.635285  2.584107  ... -1.008921  1.527817  1.160002   \n",
            "1       2.828393  0.037619  1.552598  ...  1.237814  1.987505  0.593115   \n",
            "2       2.287729  1.065162  0.468019  ...  0.344518  2.951001  0.727205   \n",
            "3      -0.899826  1.004331  0.341618  ... -0.325863  1.352081 -0.780487   \n",
            "4       0.409371 -0.282354 -0.804708  ... -0.418980  0.506938  0.196652   \n",
            "\n",
            "Ticker      ^NDX  ^SP500-15  ^SP500-20  ^STOXX50E      ^TNX      ^TYX  \\\n",
            "0       0.093190  -1.644172  -1.362889  -0.020860  1.168761  0.754816   \n",
            "1       1.215923   0.666612   1.700486   0.178895  0.901664  0.180831   \n",
            "2       1.805546   0.548096   0.667387        NaN  1.870983  0.593089   \n",
            "3      -0.225014   0.259382  -0.268740        NaN  1.206138  1.204818   \n",
            "4       0.335060  -0.547745   0.003565  -0.220006  0.731316  0.557246   \n",
            "\n",
            "Ticker      ^VIX  \n",
            "0       5.081769  \n",
            "1      -6.225675  \n",
            "2      -4.860709  \n",
            "3       4.735204  \n",
            "4       2.379534  \n",
            "\n",
            "[5 rows x 132 columns]\n",
            "Ticker       date      AAPL      ABBV       ABT    ACA.PA       ACN      ADBE  \\\n",
            "705    2026-02-02  4.058116  1.179322  0.100641  1.149420  1.194809  0.044332   \n",
            "706    2026-02-03 -0.196289  0.008866 -0.356464  1.433990 -9.588066 -7.311341   \n",
            "707    2026-02-04  2.601298 -3.788887 -0.752155 -3.040819  0.182408  2.861030   \n",
            "708    2026-02-05 -0.209768  0.879740  0.813313 -2.503438 -3.339537 -3.689527   \n",
            "709    2026-02-06       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "Ticker       ADI     AI.PA    AIR.PA  ...      ^DJI      ^FVX      ^IRX  \\\n",
            "705     1.923565  0.949728  0.206830  ...  1.053723  1.000793  0.167974   \n",
            "706    -1.757867  0.025084 -1.836944  ... -0.337340  0.052147  0.335379   \n",
            "707     2.939379  5.706049 -1.040796  ...  0.528650 -0.104247  0.139279   \n",
            "708     0.524277 -0.462688  0.669292  ... -1.197104 -2.165404 -0.472878   \n",
            "709          NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
            "\n",
            "Ticker      ^NDX  ^SP500-15  ^SP500-20  ^STOXX50E      ^TNX      ^TYX  \\\n",
            "705     0.728772   0.711470   1.259241   1.003726  0.801696  0.759436   \n",
            "706    -1.554048   2.001594   0.893708  -0.202408 -0.023390 -0.061108   \n",
            "707    -1.765601   1.804768   0.239105  -0.414986  0.023396  0.183445   \n",
            "708    -1.376190  -2.751861  -0.607487  -0.749858 -1.520469 -1.078331   \n",
            "709          NaN        NaN        NaN        NaN       NaN       NaN   \n",
            "\n",
            "Ticker       ^VIX  \n",
            "705     -6.307341  \n",
            "706     10.159118  \n",
            "707      3.555552  \n",
            "708     16.791852  \n",
            "709     -2.113004  \n",
            "\n",
            "[5 rows x 132 columns]\n",
            "NaN par colonne :\n",
            "Ticker\n",
            "CDXBTC-USD    710\n",
            "^CIISCSEP     710\n",
            "FISV           53\n",
            "ABT            51\n",
            "ABBV           51\n",
            "AAPL           51\n",
            "ADI            51\n",
            "ASML           51\n",
            "AMZN           51\n",
            "AMGN           51\n",
            "ACN            51\n",
            "AVGO           51\n",
            "BAC            51\n",
            "BKNG           51\n",
            "CSCO           51\n",
            "dtype: int64\n",
            "Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminée: \n",
            "2026-02-06 08:37:41.293269\n"
          ]
        }
      ],
      "source": [
        "### Creation du dataset de variation des tickers des 1000 derniers jours\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Définir la période de 1000 jours\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=1000)\n",
        "\n",
        "# Liste des tickers du CAC 40 (exemple : 'OR.PA' pour L'Oréal)\n",
        "cac40_tickers = cac40_tickers = [\n",
        "    \"AI.PA\",   # Air Liquide\n",
        "    \"AIR.PA\",  # Airbus\n",
        "    \"ALO.PA\",  # Alstom\n",
        "    \"CS.PA\",   # AXA\n",
        "    \"BNP.PA\",  # BNP Paribas\n",
        "    \"EN.PA\",   # Bouygues\n",
        "    \"CAP.PA\",  # Capgemini\n",
        "    \"CA.PA\",   # Carrefour\n",
        "    \"ACA.PA\",  # Crédit Agricole\n",
        "    \"BN.PA\",   # Danone\n",
        "    \"DSY.PA\",  # Dassault Systèmes\n",
        "    \"EDEN.PA\", # Edenred\n",
        "    \"ENGI.PA\", # Engie\n",
        "    \"EL.PA\",   # EssilorLuxottica\n",
        "    \"ERF.PA\",  # Eurofins Scientific\n",
        "    \"RMS.PA\",  # Hermès\n",
        "    \"KER.PA\",  # Kering\n",
        "    \"LR.PA\",   # Legrand\n",
        "    \"OR.PA\",   # L'Oréal\n",
        "    \"MC.PA\",   # LVMH\n",
        "    \"ML.PA\",   # Michelin\n",
        "    \"ORA.PA\",  # Orange\n",
        "    \"RI.PA\",   # Pernod Ricard\n",
        "    \"PUB.PA\",  # Publicis\n",
        "    \"RNO.PA\",  # Renault\n",
        "    \"SAF.PA\",  # Safran\n",
        "    \"SGO.PA\",  # Saint-Gobain\n",
        "    \"SAN.PA\",  # Sanofi\n",
        "    \"SU.PA\",   # Schneider Electric\n",
        "    \"GLE.PA\",  # Société Générale\n",
        "    \"TEP.PA\",  # Teleperformance\n",
        "    \"HO.PA\",   # Thales\n",
        "    \"TTE.PA\",  # TotalEnergies\n",
        "    \"URW.PA\",  # Unibail-Rodamco-Westfield\n",
        "    \"VIE.PA\",  # Veolia\n",
        "    \"DG.PA\",   # Vinci\n",
        "    \"VIV.PA\",  # Vivendi\n",
        "    \"WLN.PA\"   # Worldline\n",
        "]\n",
        "\n",
        "# Liste des tickers du NASDAQ (exemple : 'AAPL' pour Apple)\n",
        "nasdaq_tickers = nasdaq_top_40 = [\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"GOOGL\", # Alphabet (Google)\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"META\",  # Meta (Facebook)\n",
        "    \"NVDA\",  # NVIDIA\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"COST\",  # Costco\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NFLX\",  # Netflix\n",
        "    \"PYPL\",  # PayPal\n",
        "    \"INTC\",  # Intel\n",
        "    \"CSCO\",  # Cisco\n",
        "    \"CMCSA\", # Comcast\n",
        "    \"AMGN\",  # Amgen\n",
        "    \"TXN\",   # Texas Instruments\n",
        "    \"QCOM\",  # Qualcomm\n",
        "    \"HON\",   # Honeywell\n",
        "    \"AMD\",   # Advanced Micro Devices (AMD)\n",
        "    \"ISRG\",  # Intuitive Surgical\n",
        "    \"SBUX\",  # Starbucks\n",
        "    \"MDLZ\",  # Mondelēz\n",
        "    \"AMAT\",  # Applied Materials\n",
        "    \"LRCX\",  # Lam Research\n",
        "    \"ADI\",   # Analog Devices\n",
        "    \"MU\",    # Micron Technology\n",
        "    \"ASML\", # ASML Holding\n",
        "    \"MRNA\", # Moderna\n",
        "    \"ILMN\",  # Illumina\n",
        "    \"BKNG\",  # Booking Holdings\n",
        "    \"REGN\",  # Regeneron\n",
        "    \"KDP\",   # Keurig Dr Pepper\n",
        "    \"MNST\",  # Monster Beverage\n",
        "    \"FISV\",  # Fiserv\n",
        "    \"WDAY\",  # Workday\n",
        "    \"TEAM\"   # Atlassian\n",
        "]\n",
        "\n",
        "nyse_tickers = nyse_top_40 = [\n",
        "    \"XOM\",   # Exxon Mobil\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"V\",     # Visa\n",
        "    \"PG\",    # Procter & Gamble\n",
        "    \"MA\",    # Mastercard\n",
        "    \"HD\",    # Home Depot\n",
        "    \"DIS\",   # Disney\n",
        "    \"VZ\",    # Verizon\n",
        "    \"MCD\",   # McDonald's\n",
        "    \"CVX\",   # Chevron\n",
        "    \"WMT\",   # Walmart\n",
        "    \"BAC\",   # Bank of America\n",
        "    \"PFE\",   # Pfizer\n",
        "    \"KO\",    # Coca-Cola\n",
        "    \"MRK\",   # Merck\n",
        "    \"ABBV\",  # AbbVie\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"PEP\",   # PepsiCo\n",
        "    \"C\",     # Citigroup\n",
        "    \"TMO\",   # Thermo Fisher Scientific\n",
        "    \"LIN\",   # Linde\n",
        "    \"CSCO\",  # Cisco (aussi coté sur NYSE)\n",
        "    \"ACN\",   # Accenture\n",
        "    \"CVS\",   # CVS Health\n",
        "    \"ORCL\",  # Oracle\n",
        "    \"NKE\",   # Nike\n",
        "    \"LLY\",   # Eli Lilly\n",
        "    \"DHR\",   # Danaher\n",
        "    \"UNH\",   # UnitedHealth\n",
        "    \"PM\",    # Philip Morris\n",
        "    \"IBM\",   # IBM\n",
        "    \"MMM\",   # 3M\n",
        "    \"MDT\",   # Medtronic\n",
        "    \"GE\",    # General Electric\n",
        "    \"GS\",    # Goldman Sachs\n",
        "    \"CAT\",   # Caterpillar\n",
        "    \"RTX\",   # Raytheon Technologies\n",
        "    \"UPS\",   # UPS\n",
        "    \"MO\",    # Altria\n",
        "]\n",
        "\n",
        "# Inclure également les indices macro (VIX, Dollar Index, Brent, Or)\n",
        "macro_tickers = [\n",
        "    \"GC=F\",        # Or (Gold Futures COMEX)\n",
        "    \"BZ=F\",        # Pétrole Brent (Brent Crude Oil Futures)\n",
        "    \"^NDX\",        # NASDAQ 100 (technologie US, croissance)\n",
        "    \"^DJI\",        # Dow Jones Industrial Average (blue chips US)\n",
        "    \"^SP500-20\",   # S&P 500 Industrials (secteur industriel)\n",
        "    \"^SP500-15\",   # S&P 500 Materials (matières premières)\n",
        "    \"^STOXX50E\",   # Euro STOXX 50 (grandes capitalisations zone euro)\n",
        "    \"DX-Y.NYB\",    # Dollar Index (DXY – force du dollar US)\n",
        "    \"EUR=X\",       # Taux de change EUR/USD\n",
        "    \"CHF=X\",       # Taux de change USD/CHF (valeur refuge)\n",
        "    \"^VIX\",        # Indice de volatilité (peur / stress de marché)\n",
        "    \"^IRX\",        # Taux US 13 semaines (T-Bills court terme)\n",
        "    \"^FVX\",        # Taux US 5 ans\n",
        "    \"^TNX\",        # Taux US 10 ans (benchmark macro mondial)\n",
        "    \"^TYX\",        # Taux US 30 ans (long terme)\n",
        "    \"^CIISCSEP\",   # Indice de surprises économiques Citi (US)\n",
        "    \"CDX\"          # Indice de crédit (Credit Default Swaps – stress crédit)\n",
        "    \"BTC-USD\"      # Bitcoin\n",
        "]\n",
        "\n",
        "\n",
        "# Combiner toutes les listes de tickers\n",
        "all_tickers = cac40_tickers + nasdaq_tickers + nyse_tickers + macro_tickers\n",
        "\n",
        "# Télécharger les données boursières\n",
        "data = yf.download(all_tickers, start=start_date, end=end_date,auto_adjust=True)\n",
        "\n",
        "# Calculer la variation en pourcentage par rapport à la clôture précédente\n",
        "variation = data[\"Close\"].pct_change(fill_method=None) * 100\n",
        "variation = variation.dropna(how=\"all\")\n",
        "\n",
        "variation.index = pd.to_datetime(variation.index)\n",
        "variation.index.name = \"date\"\n",
        "\n",
        "dataset_ts = variation.reset_index()\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "print(\"Shape:\", dataset_ts.shape)\n",
        "print(dataset_ts.head())\n",
        "print(dataset_ts.tail())\n",
        "print(\"NaN par colonne :\")\n",
        "print(variation.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "pd.DataFrame(cac40_tickers, columns=[\"Ticker\"]).to_csv(\"CAC40_TICKERS.csv\", index=False)\n",
        "dataset_ts.to_csv(\"DATASET_1000DAYS_VARIATION.csv\", index=False)\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Creation de CAC40_TICKERS.csv et du dataset DATASET_1000DAYS_VARIATION.csv terminée: \")\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rules detection"
      ],
      "metadata": {
        "id": "AAME67UIp6Oc"
      },
      "id": "AAME67UIp6Oc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "# Adapter le chemin si nécessaire\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Détection des règles\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules(\n",
        "    disc,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        "):\n",
        "    rules = []\n",
        "\n",
        "    for target in disc.columns:\n",
        "        future = disc[target].shift(-1)\n",
        "        base_rate = (future == \"UP\").mean()\n",
        "\n",
        "        # On ignore les cibles trop rares\n",
        "        if base_rate < 0.05:\n",
        "            continue\n",
        "\n",
        "        leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "        for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "            # Condition à t\n",
        "            mask = (\n",
        "                (disc[l1] == \"UP\") &\n",
        "                (disc[l2] == \"DOWN\")\n",
        "            )\n",
        "\n",
        "            support = mask.mean()\n",
        "            if support < min_support:\n",
        "                continue\n",
        "\n",
        "            confidence = (future[mask] == \"UP\").mean()\n",
        "            if confidence < min_confidence:\n",
        "                continue\n",
        "\n",
        "            lift = confidence / base_rate\n",
        "            if lift < min_lift:\n",
        "                continue\n",
        "\n",
        "            rules.append({\n",
        "                \"target\": target,\n",
        "                \"leaders\": f\"{l1} ↑ AND {l2} ↓\",\n",
        "                \"support\": round(support, 3),\n",
        "                \"confidence\": round(confidence, 3),\n",
        "                \"lift\": round(lift, 2)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. Exécution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules(disc)\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Résultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 règles détectées :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "# Optionnel : sauvegarde\n",
        "rules.to_csv(\"lead_lag_rules_results1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "tTulpFIuqE4F",
        "outputId": "b6913a77-91cb-426c-ce31-04c4919efc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tTulpFIuqE4F",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 règles détectées :\n",
            "\n",
            "      target               leaders  support  confidence  lift\n",
            "83   EDEN.PA      C ↑ AND SGO.PA ↓    0.032       0.696  2.96\n",
            "82       DIS      ADBE ↑ AND ADI ↓    0.034       0.625  2.94\n",
            "5     AIR.PA   FISV ↑ AND TEP.PA ↓    0.034       0.625  2.90\n",
            "96       IBM   DSY.PA ↑ AND REGN ↓    0.045       0.625  2.88\n",
            "73    BNP.PA    ACN ↑ AND CAP.PA ↓    0.031       0.636  2.72\n",
            "189   SAF.PA       C ↑ AND EL.PA ↓    0.038       0.630  2.71\n",
            "74    CAP.PA    DIS ↑ AND VIV.PA ↓    0.032       0.609  2.70\n",
            "81       CVS     EN.PA ↑ AND NKE ↓    0.032       0.652  2.62\n",
            "191   SAF.PA    CRM ↑ AND SGO.PA ↓    0.039       0.607  2.61\n",
            "0       ADBE    BNP.PA ↑ AND LIN ↓    0.035       0.640  2.60\n",
            "106   KER.PA   PYPL ↑ AND SGO.PA ↓    0.035       0.640  2.60\n",
            "76       CAT      ALO.PA ↑ AND C ↓    0.034       0.708  2.58\n",
            "1       ADBE      DG.PA ↑ AND PG ↓    0.031       0.636  2.58\n",
            "80       CRM       GS ↑ AND MDLZ ↓    0.031       0.636  2.58\n",
            "190   SAF.PA    CAT ↑ AND PUB.PA ↓    0.035       0.600  2.58\n",
            "192   SGO.PA     CRM ↑ AND EL.PA ↓    0.035       0.600  2.58\n",
            "2        ADI  EDEN.PA ↑ AND GC=F ↓    0.031       0.682  2.55\n",
            "79       CRM        CAT ↑ AND PG ↓    0.041       0.621  2.52\n",
            "184   RNO.PA    ACN ↑ AND CAP.PA ↓    0.031       0.682  2.50\n",
            "194    SU.PA    INTC ↑ AND ML.PA ↓    0.031       0.636  2.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules detection with lag"
      ],
      "metadata": {
        "id": "uLFG9Y6xVvR1"
      },
      "id": "uLFG9Y6xVvR1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# =========================\n",
        "# 1. Chargement des données\n",
        "# =========================\n",
        "\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df = df.drop(columns=[\"date\"])\n",
        "\n",
        "# =========================\n",
        "# 2. Discrétisation\n",
        "# =========================\n",
        "\n",
        "def discretize(x):\n",
        "    if x > 1.0:\n",
        "        return \"UP\"\n",
        "    elif x < -1.0:\n",
        "        return \"DOWN\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "disc = df.map(discretize)\n",
        "\n",
        "# =========================\n",
        "# 3. Détection des règles avec lag\n",
        "# =========================\n",
        "\n",
        "def mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40,\n",
        "    min_base_rate=0.05\n",
        "):\n",
        "    rules = []\n",
        "    n = len(disc)\n",
        "\n",
        "    for target in disc.columns:\n",
        "\n",
        "        for lag in range(1, max_lag + 1):\n",
        "\n",
        "            future = disc[target]\n",
        "            leaders_disc = disc.shift(lag)\n",
        "\n",
        "            valid_idx = leaders_disc.index[lag:]\n",
        "            future = future.loc[valid_idx]\n",
        "            leaders_disc = leaders_disc.loc[valid_idx]\n",
        "\n",
        "            base_rate = (future == \"UP\").mean()\n",
        "            if base_rate < min_base_rate:\n",
        "                continue\n",
        "\n",
        "            leaders = [c for c in disc.columns if c != target]\n",
        "\n",
        "            for l1, l2 in combinations(leaders, 2):\n",
        "\n",
        "                mask = (\n",
        "                    (leaders_disc[l1] == \"UP\") &\n",
        "                    (leaders_disc[l2] == \"DOWN\")\n",
        "                )\n",
        "\n",
        "                support = mask.mean()\n",
        "                if support < min_support:\n",
        "                    continue\n",
        "\n",
        "                confidence = (future[mask] == \"UP\").mean()\n",
        "                if confidence < min_confidence:\n",
        "                    continue\n",
        "\n",
        "                lift = confidence / base_rate\n",
        "                if lift < min_lift:\n",
        "                    continue\n",
        "\n",
        "                rules.append({\n",
        "                    \"target\": target,\n",
        "                    \"leaders\": f\"{l1} ↑ AND {l2} ↓\",\n",
        "                    \"lag_days\": lag,\n",
        "                    \"support\": round(support, 3),\n",
        "                    \"confidence\": round(confidence, 3),\n",
        "                    \"lift\": round(lift, 2),\n",
        "                    \"occurrences\": int(mask.sum())\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(rules)\n",
        "\n",
        "# =========================\n",
        "# 4. Exécution\n",
        "# =========================\n",
        "\n",
        "rules = mine_lead_rules_with_lag(\n",
        "    disc,\n",
        "    max_lag=3,\n",
        "    min_support=0.03,\n",
        "    min_confidence=0.60,\n",
        "    min_lift=1.40\n",
        ")\n",
        "\n",
        "rules = rules.sort_values(\n",
        "    by=[\"lift\", \"confidence\", \"support\"],\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Résultats\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 règles détectées :\\n\")\n",
        "print(rules.head(20))\n",
        "\n",
        "rules.to_csv(\"lead_lag_rules_results2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "vf5ybat9VqvF"
      },
      "id": "vf5ybat9VqvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross correlation Lead–Lag multivarié (Lasso)"
      ],
      "metadata": {
        "id": "BdSnwSI0IB7W"
      },
      "id": "BdSnwSI0IB7W"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LEAD–LAG MULTIVARIÉ PAR LASSO (SCRIPT COMPLET)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# ----------------------------\n",
        "FILE_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# gestion de la date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2. NETTOYAGE & SÉCURITÉS\n",
        "# ----------------------------\n",
        "\n",
        "# garder uniquement les colonnes numériques\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "if df.shape[1] == 0:\n",
        "    raise ValueError(\"Aucune colonne numérique exploitable\")\n",
        "\n",
        "# conserver les séries suffisamment complètes (au moins 80% des lignes présentes)\n",
        "threshold = int(len(df) * 0.8)\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# supprimer les lignes avec NaN restantes\n",
        "df = df.dropna()\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset vide après nettoyage. Essayez de réduire le threshold ou de vérifier la source des données.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. STANDARDISATION\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df.values),\n",
        "    index=df.index,\n",
        "    columns=df.columns\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4. CONSTRUCTION DES LAGS\n",
        "# ----------------------------\n",
        "def build_lagged_matrix(df, max_lag):\n",
        "    X = []\n",
        "    feature_names = []\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        shifted = df.shift(lag)\n",
        "        X.append(shifted.values)\n",
        "\n",
        "        for col in df.columns:\n",
        "            feature_names.append(f\"{col}_lag{lag}\")\n",
        "\n",
        "    X = np.hstack(X)\n",
        "    return X, feature_names\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5. LASSO MULTIVARIÉ LEAD–LAG\n",
        "# ----------------------------\n",
        "def multivariate_lead_lag_lasso(df, target, max_lag=5, min_coef=1e-4):\n",
        "    X, feature_names = build_lagged_matrix(df, max_lag)\n",
        "    y = df[target].values\n",
        "\n",
        "    # alignement temporel\n",
        "    X = X[max_lag:]\n",
        "    y = y[max_lag:]\n",
        "\n",
        "    model = LassoCV(\n",
        "        cv=5,\n",
        "        max_iter=5000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for coef, fname in zip(model.coef_, feature_names):\n",
        "        if abs(coef) >= min_coef:\n",
        "            col, lag = fname.rsplit(\"_lag\", 1)\n",
        "            results.append({\n",
        "                \"target\": target,\n",
        "                \"leader\": col,\n",
        "                \"lag_days\": int(lag),\n",
        "                \"coefficient\": coef\n",
        "            })\n",
        "\n",
        "    # Gestion du cas où aucun coefficient n'est retenu (évite KeyError)\n",
        "    if not results:\n",
        "        return pd.DataFrame(columns=[\"target\", \"leader\", \"lag_days\", \"coefficient\"])\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6. LANCEMENT GLOBAL\n",
        "# ----------------------------\n",
        "MAX_LAG = 5\n",
        "all_results = []\n",
        "\n",
        "for target in df_scaled.columns:\n",
        "    res = multivariate_lead_lag_lasso(\n",
        "        df_scaled,\n",
        "        target=target,\n",
        "        max_lag=MAX_LAG\n",
        "    )\n",
        "\n",
        "    if not res.empty:\n",
        "        all_results.append(res)\n",
        "\n",
        "if not all_results:\n",
        "    print(\"Aucun lead-lag significatif trouvé.\")\n",
        "    best_results = pd.DataFrame()\n",
        "else:\n",
        "    final_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 7. RÉSULTATS FINAUX\n",
        "    # ----------------------------\n",
        "    best_results = final_results.sort_values(\n",
        "        by=\"coefficient\",\n",
        "        key=np.abs,\n",
        "        ascending=False\n",
        "    )\n",
        "\n",
        "    print(best_results.head(20))\n",
        "\n",
        "    # ----------------------------\n",
        "    # 8. EXPORT OPTIONNEL\n",
        "    # ----------------------------\n",
        "    best_results.to_csv(\n",
        "        \"multivariate_lead_lag_lasso_results.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# FIN DU SCRIPT\n",
        "# ============================================================"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6SeFQr3H8PV",
        "outputId": "7f9c4592-b6d3-48e5-a5f3-cd62c4ee36cc"
      },
      "id": "j6SeFQr3H8PV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.943e-02, tolerance: 5.369e-02\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e-02, tolerance: 5.062e-02\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lead Lag search"
      ],
      "metadata": {
        "id": "Z1jerT1XM-pP"
      },
      "id": "Z1jerT1XM-pP"
    },
    {
      "cell_type": "code",
      "source": [
        "import toto\n"
      ],
      "metadata": {
        "id": "owfOt5FAWCgN"
      },
      "id": "owfOt5FAWCgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7",
      "metadata": {
        "id": "9d90c73b-74f4-4a70-8588-6a07e48a09a7"
      },
      "outputs": [],
      "source": [
        "### Recherche des leads/lags sur les actions du CAC40\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger le CSV\n",
        "dataset_ts = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]   # très important\n",
        ")\n",
        "\n",
        "# Trier par date (sécurité)\n",
        "dataset_ts = dataset_ts.sort_values(\"date\")\n",
        "\n",
        "# Optionnel : remettre la date en index\n",
        "dataset_ts = dataset_ts.set_index(\"date\")\n",
        "\n",
        "def scan_lead_lag_cac40(\n",
        "    df,\n",
        "    cac40_cols,\n",
        "    other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40,\n",
        "    method=\"pearson\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Scan exhaustif lead/lag entre :\n",
        "    - other_cols (X candidates)\n",
        "    - cac40_cols (Y targets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for y in cac40_cols:\n",
        "        for x in other_cols:\n",
        "\n",
        "            if x == y:\n",
        "                continue\n",
        "\n",
        "            best_row = None\n",
        "\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "\n",
        "                if lag < 0:\n",
        "                    x_shifted = df[x].shift(-lag)\n",
        "                    y_shifted = df[y]\n",
        "                else:\n",
        "                    x_shifted = df[x]\n",
        "                    y_shifted = df[y].shift(lag)\n",
        "\n",
        "                pair = pd.concat([x_shifted, y_shifted], axis=1).dropna()\n",
        "\n",
        "                if len(pair) < min_obs:\n",
        "                    continue\n",
        "\n",
        "                corr = pair.iloc[:, 0].corr(pair.iloc[:, 1], method=method)\n",
        "\n",
        "                if pd.isna(corr):\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"x\": x,\n",
        "                    \"y\": y,\n",
        "                    \"lag\": lag,\n",
        "                    \"correlation\": corr,\n",
        "                    \"abs_corr\": abs(corr),\n",
        "                    \"direction\": \"inverse\" if corr < 0 else \"same\",\n",
        "                    \"n_obs\": len(pair)\n",
        "                }\n",
        "\n",
        "                if best_row is None or row[\"abs_corr\"] > best_row[\"abs_corr\"]:\n",
        "                    best_row = row\n",
        "\n",
        "            if best_row is not None:\n",
        "                results.append(best_row)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df = dataset_ts\n",
        "cac40_cols = pd.read_csv(\"CAC40_TICKERS.csv\")['Ticker'].tolist()\n",
        "other_cols = [c for c in df.columns if c not in cac40_cols]\n",
        "\n",
        "results = scan_lead_lag_cac40(\n",
        "    df=df,\n",
        "    cac40_cols=cac40_cols,\n",
        "    other_cols=other_cols,\n",
        "    max_lag=20,\n",
        "    min_obs=40\n",
        ")\n",
        "\n",
        "results.sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "results[\n",
        "    results[\"direction\"] == \"inverse\"\n",
        "].sort_values(\"abs_corr\", ascending=False).head(20)\n",
        "\n",
        "results[results[\"y\"] == \"MC.PA\"] \\\n",
        "    .sort_values(\"abs_corr\", ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(\"Shape:\", results.shape)\n",
        "results.to_csv(\"LEAD_LAG_SCAN_CAC40.csv\", index=False)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des leads/lags sur les actions du CAC40 terminée: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candidates selection"
      ],
      "metadata": {
        "id": "5rIc4-G4NG_T"
      },
      "id": "5rIc4-G4NG_T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
      },
      "outputs": [],
      "source": [
        "### Recherche des variables candidates qui ont des leads/lags significatifs sur le CAC40\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU CSV\n",
        "# =========================\n",
        "\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "print(\"Dataset initial :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 2. FILTRE : LAG ≠ 0\n",
        "# =========================\n",
        "\n",
        "leadlag = leadlag[leadlag[\"lag\"] != 0]\n",
        "\n",
        "print(\"Après suppression lag = 0 :\", leadlag.shape)\n",
        "\n",
        "# =========================\n",
        "# 3. CLASSIFICATION DU RISQUE\n",
        "# =========================\n",
        "\n",
        "def classify_risk(abs_corr, n_obs):\n",
        "    if abs_corr >= 0.55 and n_obs >= 80:\n",
        "        return \"very_low_risk\"\n",
        "    elif abs_corr >= 0.45 and n_obs >= 60:\n",
        "        return \"low_risk\"\n",
        "    elif abs_corr >= 0.40 and n_obs >= 50:\n",
        "        return \"medium_risk\"\n",
        "    elif abs_corr >= 0.30 and n_obs >= 40:\n",
        "        return \"high_risk\"\n",
        "    else:\n",
        "        return \"very_high_risk\"\n",
        "\n",
        "leadlag[\"risk_level\"] = leadlag.apply(\n",
        "    lambda r: classify_risk(r[\"abs_corr\"], r[\"n_obs\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. TRI FINAL (DU + SÛR AU + RISQUÉ)\n",
        "# =========================\n",
        "\n",
        "risk_order = [\n",
        "    \"very_low_risk\",\n",
        "    \"low_risk\",\n",
        "    \"medium_risk\",\n",
        "    \"high_risk\",\n",
        "    \"very_high_risk\"\n",
        "]\n",
        "\n",
        "leadlag[\"risk_level\"] = pd.Categorical(\n",
        "    leadlag[\"risk_level\"],\n",
        "    categories=risk_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "leadlag_sorted = leadlag.sort_values(\n",
        "    [\"risk_level\", \"abs_corr\"],\n",
        "    ascending=[True, False]\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. CONTRÔLE RAPIDE\n",
        "# =========================\n",
        "\n",
        "print(\"\\nTop 20 relations les plus sûres (lag ≠ 0) :\")\n",
        "print(leadlag_sorted.head(20))\n",
        "\n",
        "# =========================\n",
        "# 6. SAUVEGARDE\n",
        "# =========================\n",
        "\n",
        "leadlag_sorted.to_csv(\n",
        "    \"LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nCSV sauvegardé : LEAD_LAG_SCAN_CAC40_CLASSIFIED_LAG_NOT_ZERO.csv\")\n",
        "\n",
        "# Charger le scan lead/lag\n",
        "leadlag = pd.read_csv(\"LEAD_LAG_SCAN_CAC40.csv\")\n",
        "\n",
        "# Filtre SAFE\n",
        "filtered = leadlag[\n",
        "    (leadlag[\"lag\"] != 0) &\n",
        "    (leadlag[\"abs_corr\"] >= 0.25) &\n",
        "    (leadlag[\"n_obs\"] >= 40)\n",
        "]\n",
        "\n",
        "# Extraire les candidats (variables X)\n",
        "candidatesX = (\n",
        "    filtered[\"x\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesX.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate variables\")\n",
        "print(candidatesX)\n",
        "pd.DataFrame(candidatesX, columns=[\"variable\"]).to_csv(\"CANDIDATES_VARIABLES.csv\", index=False)\n",
        "\n",
        "# Extraire les cibles (variables y)\n",
        "candidatesY = (\n",
        "    filtered[\"y\"]\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "candidatesY.columns = [\"variable\", \"count\"]\n",
        "print(\"\\nCandidate taget\")\n",
        "print(candidatesY)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(\"Recherche des variables candidates CANDIDATES_VARIABLES.csv terminée: \")\n",
        "print(now)\n",
        "\n",
        "\n"
      ],
      "id": "3b366ed8-1943-496d-a0dd-31c35f4ee81a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "Uij5aRPaK47h"
      },
      "id": "Uij5aRPaK47h"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"^NDX\", \"LIN\", \"ABBV\", \"CS.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"^NDX\", \"LIN\", \"ABBV\"]]\n",
        "y = df[\"CS.PA\"].shift(-1)  # URW.PA à J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"^NDX\", \"LIN\", \"ABBV\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraîner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"^NDX\", \"LIN\", \"ABBV\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"^NDX\", \"LIN\", \"ABBV\"]]],\n",
        "        columns=[\"^NDX\", \"LIN\", \"ABBV\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÉDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"CS.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"CS.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÉSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Création du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"CS.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"CS.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Vérification\n",
        "print(final_dataset[[\n",
        "    \"CS.PA\",\n",
        "    \"CS.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"CS_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "HqrQ5wv0WwYg"
      },
      "id": "HqrQ5wv0WwYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# URW.PA prediction historique for validation"
      ],
      "metadata": {
        "id": "zyY4sc6rMDr0"
      },
      "id": "zyY4sc6rMDr0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGEMENT DU DATASET\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"DATASET_1000DAYS_VARIATION.csv\",\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "# On ne garde que les colonnes utiles\n",
        "df = df[[\"GC=F\", \"^NDX\", \"AAPL\", \"URW.PA\"]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. FEATURES & TARGET\n",
        "# =========================\n",
        "\n",
        "X = df[[\"GC=F\", \"^NDX\", \"AAPL\"]]\n",
        "y = df[\"URW.PA\"].shift(-1)  # URW.PA à J+1\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = [\"GC=F\", \"^NDX\", \"AAPL\", \"CS_PRED_TARGET\"]\n",
        "\n",
        "# =========================\n",
        "# 3. WALK-FORWARD PREDICTION\n",
        "# =========================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "MIN_TRAIN_SIZE = 60  # minimum historique pour entraîner\n",
        "\n",
        "for i in range(MIN_TRAIN_SIZE, len(data)):\n",
        "\n",
        "    train = data.iloc[:i]\n",
        "\n",
        "    X_train = sm.add_constant(train[[\"GC=F\", \"^NDX\", \"AAPL\"]])\n",
        "    y_train = train[\"CS_PRED_TARGET\"]\n",
        "\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "    X_today = pd.DataFrame(\n",
        "        [data.iloc[i][[\"GC=F\", \"^NDX\", \"AAPL\"]]],\n",
        "        columns=[\"GC=F\", \"^NDX\", \"AAPL\"]\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    pred = model.predict(X_today).iloc[0]\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "# =========================\n",
        "# 4. AJOUT DE LA COLONNE DE PRÉDICTION\n",
        "# =========================\n",
        "\n",
        "# Aligner avec les dates\n",
        "pred_series = pd.Series(\n",
        "    predictions,\n",
        "    index=data.index[MIN_TRAIN_SIZE:],\n",
        "    name=\"URW.PA_PRED_TOMORROW\"\n",
        ")\n",
        "\n",
        "final_dataset = df.copy()\n",
        "final_dataset[\"URW.PA_PRED_TOMORROW\"] = pred_series\n",
        "\n",
        "# =========================\n",
        "# 5. RÉSULTAT FINAL\n",
        "# =========================\n",
        "\n",
        "# Création du signal d'achat / vente\n",
        "final_dataset[\"signal\"] = np.where(\n",
        "    final_dataset[\"URW.PA_PRED_TOMORROW\"] > 0.15,\n",
        "    \"buy\",\n",
        "    np.where(\n",
        "        final_dataset[\"URW.PA_PRED_TOMORROW\"] < -0.15,\n",
        "        \"sell\",\n",
        "        \"keep\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Vérification\n",
        "print(final_dataset[[\n",
        "    \"URW.PA\",\n",
        "    \"URW.PA_PRED_TOMORROW\",\n",
        "    \"signal\"\n",
        "]].tail(10))\n",
        "\n",
        "\n",
        "print(final_dataset.tail(10))\n",
        "\n",
        "# Sauvegarde\n",
        "final_dataset.to_csv(\n",
        "    \"URW_PREDICTION_DATASET.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "khd_HR51LwOG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "khd_HR51LwOG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect saxo bank"
      ],
      "metadata": {
        "id": "R_gn7zKPNkJh"
      },
      "id": "R_gn7zKPNkJh"
    },
    {
      "cell_type": "code",
      "source": [
        "# test conncetion Saxo bank simulation\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "\n",
        "ACCESS_TOKEN = \"eyJhbGciOiJFUzI1NiIsIng1dCI6IjY3NEM0MjFEMzZEMUE1OUNFNjFBRTIzMjMyOTVFRTAyRTc3MDMzNTkifQ.eyJvYWEiOiI3Nzc3NSIsImlzcyI6Im9hIiwiYWlkIjoiMTA5IiwidWlkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiY2lkIjoiMWN1NWpOclFzanVKMkY2VXlLYTg5dz09IiwiaXNhIjoiRmFsc2UiLCJ0aWQiOiIyMDAyIiwic2lkIjoiMzYwNGFlY2NmYTc3NGNlNmEzNGZmNDIyNGUyZDBmNWEiLCJkZ2kiOiI4NCIsImV4cCI6IjE3Njg4MTEzMjgiLCJvYWwiOiIxRiIsImlpZCI6IjFkNzAzNjg4NzM4MTQzNjMwNGE1MDhkZTRjNTUxOTUwIn0.NuSPihbRVQ_YHoWnW-sQHESQ9QNBExUgGpEM-GaD7gWOQfhr2OU7D4qmgGjhLEW7FNDKsdkL0bRfwf5xuOzftg\"\n",
        "BASE_URL = \"https://gateway.saxobank.com/sim/openapi\"  # ou /sim/openapi\n",
        "SYMBOL = {\"URW.PA\" : \"19099381\"}\n",
        "EXCHANGE = \"Euronext Paris\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "    \"Accept\" : \"*/*\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def get_AccountKeys():\n",
        "    url = f\"{BASE_URL}/port/v1/accounts/me\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"Data\"]\n",
        "\n",
        "def get_Balances(AccountKey):\n",
        "    url = f\"{BASE_URL}/port/v1/balances?AccountKey=\"+AccountKey+\"&ClientKey=\"+AccountKey\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def get_TickerPosition(Uic):\n",
        "    url = f\"{BASE_URL}/trade/v1/infoprices/list?AccountKey=\"+AccountKey+\"&Uics=\"+Uic+\"&AssetType=Stock&Amount=100000&FieldGroups=DisplayAndFormat,Quote\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "AccountKeys = get_AccountKeys()\n",
        "AccountKey = AccountKeys[0][\"AccountKey\"]\n",
        "Balances = get_Balances(AccountKey)\n",
        "URW = get_TickerPosition(SYMBOL[\"URW.PA\"])\n",
        "print(URW)\n",
        "\n",
        "print(AccountKey)\n",
        "print(Balances)"
      ],
      "metadata": {
        "id": "SWGNJbYASMC1"
      },
      "id": "SWGNJbYASMC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today prediction"
      ],
      "metadata": {
        "id": "HDaUtak-Lj_h"
      },
      "id": "HDaUtak-Lj_h"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CS.PA – SIGNAL QUANT +\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# PARAMÈTRES GÉNÉRAUX\n",
        "# =========================\n",
        "\n",
        "CSV_PATH = \"DATASET_1000DAYS_VARIATION.csv\"\n",
        "\n",
        "FEATURES = [\"^NDX\", \"LIN\",\"ABBV\"]\n",
        "TARGET   = \"CS.PA\"\n",
        "\n",
        "BUY_THRESHOLD  = 0.5\n",
        "SELL_THRESHOLD = -0.5\n",
        "\n",
        "MIN_TRAIN_SIZE = 60\n",
        "\n",
        "# =========================\n",
        "# 1. CHARGER DATASET HISTORIQUE\n",
        "# =========================\n",
        "\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    parse_dates=[\"date\"]\n",
        ").set_index(\"date\")\n",
        "\n",
        "df = df[FEATURES + [TARGET]].dropna()\n",
        "\n",
        "# =========================\n",
        "# 2. ENTRAÎNEMENT DU MODÈLE (lag = 1)\n",
        "# =========================\n",
        "\n",
        "X = df[FEATURES]\n",
        "y = df[TARGET].shift(-1)\n",
        "\n",
        "data = pd.concat([X, y], axis=1).dropna()\n",
        "data.columns = FEATURES + [\"CS_TARGET\"]\n",
        "\n",
        "X_train = sm.add_constant(data[FEATURES], has_constant=\"add\")\n",
        "y_train = data[\"CS_TARGET\"]\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# =========================\n",
        "# 3. DONNÉES LIVE (clôture US)\n",
        "# =========================\n",
        "\n",
        "tickers = FEATURES + [TARGET]\n",
        "\n",
        "prices = yf.download(\n",
        "    tickers,\n",
        "    period=\"2d\",\n",
        "    interval=\"1d\",\n",
        "    auto_adjust=True,\n",
        "    progress=False\n",
        ")[\"Close\"]\n",
        "\n",
        "returns_today = prices.pct_change(fill_method=None).iloc[-1] * 100\n",
        "\n",
        "missing = returns_today[FEATURES].isna()\n",
        "\n",
        "if missing.any():\n",
        "    print(\"⚠️ Marchés non clôturés :\", missing[missing].index.tolist())\n",
        "    predicted_return = 0.0\n",
        "    signal = \"KEEP\"\n",
        "\n",
        "else:\n",
        "    X_today = pd.DataFrame(\n",
        "        [[\n",
        "            returns_today[\"^NDX\"],\n",
        "            returns_today[\"LIN\"],\n",
        "            returns_today[\"ABBV\"]\n",
        "        ]],\n",
        "        columns=FEATURES\n",
        "    )\n",
        "\n",
        "    X_today = sm.add_constant(X_today, has_constant=\"add\")\n",
        "\n",
        "    predicted_return = model.predict(X_today).iloc[0]\n",
        "\n",
        "    if predicted_return > BUY_THRESHOLD:\n",
        "        signal = \"BUY\"\n",
        "    elif predicted_return < SELL_THRESHOLD:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"KEEP\"\n",
        "\n",
        "# =========================\n",
        "# 6. SORTIE\n",
        "# =========================\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"   SIGNAL URW.PA (J+1)\")\n",
        "print(\"==============================\")\n",
        "print(\"Date :\", datetime.now().strftime(\"%Y-%m-%d %H:%M CET\"))\n",
        "print(\"\\nVariations aujourd'hui (%)\")\n",
        "print(returns_today[FEATURES])\n",
        "print(f\"\\nPrévision URW.PA J+1 : {predicted_return:.3f} %\")\n",
        "print(\"==============================\\n\")"
      ],
      "metadata": {
        "id": "9RikQhCXSRQb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9RikQhCXSRQb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}